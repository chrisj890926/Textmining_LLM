{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Rl_oZp3G8QVrxlRdXZea6Sm2HQgPTJhY",
      "authorship_tag": "ABX9TyPsCwlUmBzGzQk2nQe+5rmS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LEE-I-HUA/antitrust/blob/CHRIS/HN%26FN%26Opinion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# footnote TXT\n",
        "!pip install PyMuPDF\n",
        "!pip install --upgrade PyMuPDF\n",
        "\n",
        "import re # 正則表達式\n",
        "import os # 提供和作業系統互動的方式\n",
        "import fitz # prat of MyPDF Library 處理PDF文件\n",
        "\n",
        "# target_font_size 目標字體大小 , minimum_font_size 最小字體大小\n",
        "# extract 提取目標文本到目標txt檔案\n",
        "def extract_text_by_font_size(pdf_path, txt_path, target_font_size=9.0, min_font_size=10.0):\n",
        "    doc = fitz.open(pdf_path)\n",
        "\n",
        "    with open(txt_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "            extracted_text = f\"--- 頁碼 {page_num + 1} ---\\n\"\n",
        "\n",
        "            for b in blocks:\n",
        "                if \"lines\" in b:\n",
        "                    for line in b[\"lines\"]:\n",
        "                        for span in line[\"spans\"]:\n",
        "                            size = span[\"size\"]\n",
        "                            text = span[\"text\"]\n",
        "\n",
        "                            # 找出9號字的内容\n",
        "                            if size == target_font_size:\n",
        "                                extracted_text += f\"{text} \"\n",
        "\n",
        "                            # 找出大於10號字的内容\n",
        "                            if size > min_font_size:\n",
        "                                extracted_text += f\"{text} \"\n",
        "\n",
        "            file.write(extracted_text + \"\\n\\n\")\n",
        "\n",
        "    doc.close()\n",
        "\n",
        "# 以不區分大小寫的方式在每個 \"關鍵字\" 和 \"Reference\" 中提取文本\n",
        "def extract_between_keywords(full_text, keywords, output_path):\n",
        "    patterns = [re.compile(rf\"{re.escape(keyword)}.*?Reference\", re.DOTALL | re.IGNORECASE) for keyword in keywords]  #Core Terms 或 Reference\n",
        "\n",
        "    for pattern in patterns:\n",
        "            matches = pattern.findall(full_text)\n",
        "            for match in matches:\n",
        "                file.write(match + \"\\n\\n\")\n",
        "\n",
        "# re. = function from regular expression, complies re pattern into re object for matching operations\n",
        "# rf = raw format string , creat strings without without print / as escape characters\n",
        "# escape character為轉義符號 or 跳脫字元 , like /n\n",
        "# re.escape(keyword) = ensure keyword as literal word form as re\n",
        "# .*?Reference = match sequence followed by  \"Reference\"\n",
        "# re.DOTALL | re.IGNORECASE : flag, DOTALL for matching any character('.' & '/n'). IGNORECASE for match both uppercase & lowercase letter( a & A)\n",
        "# for keyword in keywords : every keyword work these operation\n",
        "\n",
        "pdf_file_path = \"/content/drive/MyDrive/textmining/StateSupremeCourt.PDF\" #無法讀取檔案可以換個路徑就可以了\n",
        "txt_file_path = \"/content/drive/MyDrive/textmining/footnote0109-supreme.txt\"\n",
        "\n",
        "extract_text_by_font_size(pdf_file_path, txt_file_path)\n",
        "\n",
        "# 關鍵字（大寫）\n",
        "keywords = ['Syllabus', 'Opinion']\n",
        "\n",
        "# 讀取\n",
        "with open(txt_file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    full_text = file.read()\n",
        "\n"
      ],
      "metadata": {
        "id": "OUpmMY9yxkMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c55b35e-0b23-436d-c03f-73ea0975590a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.23.17-cp310-none-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.9 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.23.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.23.17 PyMuPDFb-1.23.9\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.23.17)\n",
            "Requirement already satisfied: PyMuPDFb==1.23.9 in /usr/local/lib/python3.10/dist-packages (from PyMuPDF) (1.23.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FN CSV\n",
        "import re\n",
        "import csv\n",
        "import fitz\n",
        "\n",
        "def extract_text_by_font_size(pdf_path, csv_path, target_font_size=9.0, min_font_size=10.0):\n",
        "    doc = fitz.open(pdf_path)\n",
        "\n",
        "    with open(csv_path, \"w\", encoding=\"utf-8\", newline='') as csv_file:\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "        csv_writer.writerow(['頁碼', '內容'])\n",
        "\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "            extracted_text = \"\"\n",
        "\n",
        "            for b in blocks:\n",
        "                if \"lines\" in b:\n",
        "                    for line in b[\"lines\"]:\n",
        "                        for span in line[\"spans\"]:\n",
        "                            size = span[\"size\"]\n",
        "                            text = span[\"text\"]\n",
        "\n",
        "                            # 找出9號字的内容\n",
        "                            if size == target_font_size:\n",
        "                                extracted_text += f\"{text} \"\n",
        "\n",
        "                            # 找出大於10號字的内容\n",
        "                            if size > min_font_size:\n",
        "                                extracted_text += f\"{text} \"\n",
        "\n",
        "            if extracted_text.strip():  # 只有在提取到內容時才寫入 CSV\n",
        "                csv_writer.writerow([page_num + 1, extracted_text.strip()])\n",
        "\n",
        "    doc.close()\n",
        "\n",
        "def extract_between_keywords(full_text, keywords, output_path):\n",
        "    patterns = [re.compile(rf\"{re.escape(keyword)}.*?Reference\", re.DOTALL | re.IGNORECASE) for keyword in keywords]  #Core Terms 或 Reference\n",
        "\n",
        "    with open(output_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        for pattern in patterns:\n",
        "            matches = pattern.findall(full_text)\n",
        "            for match in matches:\n",
        "                file.write(match + \"\\n\\n\")\n",
        "\n",
        "# 檔案路徑\n",
        "pdf_file_path = \"/content/drive/MyDrive/textmining/State Surperior Court (CA) (30).PDF\"\n",
        "csv_file_path = \"/content/drive/MyDrive/textmining/FN/footnote_Surperior.csv\"\n",
        "\n",
        "# 提取到 CSV 檔案\n",
        "extract_text_by_font_size(pdf_file_path, csv_file_path)\n",
        "\n",
        "# 關鍵字（大寫）\n",
        "keywords = ['Syllabus', 'Opinion']\n",
        "\n",
        "# 讀取\n",
        "with open(csv_file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    full_text = file.read()\n",
        "\n",
        "\n",
        "print(f\"完成：{csv_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCvsGyKCKC1s",
        "outputId": "027ba6c7-7111-4374-8163-48b5209b0dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "完成：/content/drive/MyDrive/textmining/FN/footnote_Surperior.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 分界線"
      ],
      "metadata": {
        "id": "CIWlcZ-euw9G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12848998-ab12-4a7f-a6bf-7dadd30120c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted segments are saved in /content/drive/MyDrive/textmining/HN/HN_Circuit.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HN CSV 整合 (註解)\n",
        "#Headnotes 及 HN1 之間 會出現抓錯內容的HN編號(手動刪除) 99%抓取正確\n",
        "# 若只有 Core Terms及Opinion 會抓到Opinion & opinion by的內容(手動刪除)\n",
        "# !pip install PyMuPDF\n",
        "# !pip install --upgrade PyMuPDF\n",
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import csv\n",
        "\n",
        "# initialize variables to store page content & ongoing Headnotes\n",
        "def extract_and_process_segments_with_page_number(pdf_file_path, csv_file_path):\n",
        "    with fitz.open(pdf_file_path) as doc:\n",
        "        page_content = []  # 儲存頁面和對應內容\n",
        "        ongoing_hn_label = None # track ongoing HN label of processing HN sections\n",
        "        ongoing_hn_content = \"\" # accumulate content of ongoing HN sections\n",
        "\n",
        "# append \"page\", \"HN label\", \"content\" to page_content list\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num) # iterate through each page of PDF\n",
        "            text = page.get_text()\n",
        "\n",
        "            # 提取 Core Terms 和 Headnotes\n",
        "\n",
        "            core_terms_match = re.search(r'Core Terms\\s+(.+?)(?=LexisNexis® Headnotes|Counsel|$)', text, re.S)\n",
        "            # match \"Core Terms\", /s \"match >=1 whitespace characters\", (.+?) \"capture any character(except newline) times and store.\n",
        "            # (?=LexisNexis® Headnotes|Counsel|$) : ensure the ending is \"Headnotes or Counsel\" . $ 結尾為Core Terms字串\n",
        "            # re.S : flag, makes \".\" match all character(newline), store in core_terms_match\n",
        "            if core_terms_match:\n",
        "                core_terms_content = core_terms_match.group(1).strip() # extracted content between first captured group\n",
        "                page_content.append([page_num + 1, \"Core Terms\", core_terms_content])\n",
        "\n",
        "            headnotes_match = re.search(r'LexisNexis® Headnotes\\s+(.+?)(?=HN\\d+|$)', text, re.S)\n",
        "            if headnotes_match:\n",
        "                headnotes_content = headnotes_match.group(1).strip()\n",
        "                page_content.append([page_num + 1, \"Headnotes\", headnotes_content])\n",
        "\n",
        "            # 提取 HN 部分，包含 “>” 的内容，並處理跨頁問題\n",
        "            hn_matches = re.findall(r'HN\\d+\\[|[^.]+>.*?(?=HN\\d+|$)|(?<=\\>).*?(?=Lawyers\\' Counsel or Judges)', text, re.DOTALL)\n",
        "            # re.findall: find all match of re, above re = find patterns relate to \"HN\", content with \">\", handle spans(跨頁)\n",
        "\n",
        "            for match in hn_matches: # examine the uncatched \"HN content\"\n",
        "                if 'HN' in match:\n",
        "                    if ongoing_hn_label and ongoing_hn_content:\n",
        "                        page_content.append([page_num + 1, ongoing_hn_label, ongoing_hn_content.strip()])\n",
        "                    ongoing_hn_label = match\n",
        "                    ongoing_hn_content = \"\"\n",
        "                elif \">\" in match and ongoing_hn_label: # examine whether \">\" exist in HN content or not\n",
        "                    ongoing_hn_content += match.strip() + \" \"\n",
        "\n",
        "            # 檢查是否有未完成的HN部分\n",
        "            if ongoing_hn_label and ongoing_hn_content:\n",
        "                page_content.append([page_num + 1, ongoing_hn_label, ongoing_hn_content.strip()])\n",
        "                ongoing_hn_label = None\n",
        "                ongoing_hn_content = \"\"\n",
        "\n",
        "        # 寫入CSV文件\n",
        "        with open(csv_file_path, 'w', encoding='utf-8', newline='') as csv_file: # \"with\" ensure closed after writed ,'w' = write mode, newline = '' ensure currect ending\n",
        "            csv_writer = csv.writer(csv_file)\n",
        "            csv_writer.writerow(['Page', 'Section', 'Content'])\n",
        "            csv_writer.writerows(page_content)\n",
        "\n",
        "# 指定的文件路径\n",
        "pdf_file_path = '/content/drive/MyDrive/textmining/Federal District (30).PDF'\n",
        "csv_file_path = '/content/drive/MyDrive/textmining/HN/HN_District_wrong.csv'\n",
        "\n",
        "# 調用函数\n",
        "extract_and_process_segments_with_page_number(pdf_file_path, csv_file_path)\n"
      ],
      "metadata": {
        "id": "XcHLQxMoH4lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HN CSV 整合\n",
        "# !pip install PyMuPDF\n",
        "# !pip install --upgrade PyMuPDF\n",
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import csv\n",
        "\n",
        "def extract_and_process_segments_with_page_number(pdf_file_path, csv_file_path):\n",
        "    with fitz.open(pdf_file_path) as doc:\n",
        "        page_content = []\n",
        "        ongoing_hn_label = None\n",
        "        ongoing_hn_content = \"\"\n",
        "\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num) # iterate through each page of PDF\n",
        "            text = page.get_text()\n",
        "\n",
        "            core_terms_match = re.search(r'Core Terms\\s+(.+?)(?=LexisNexis® Headnotes|Counsel|$)', text, re.S)\n",
        "\n",
        "            if core_terms_match:\n",
        "                core_terms_content = core_terms_match.group(1).strip()\n",
        "                page_content.append([page_num + 1, \"Core Terms\", core_terms_content])\n",
        "\n",
        "            headnotes_match = re.search(r'LexisNexis® Headnotes\\s+(.+?)(?=HN\\d+|$)', text, re.S)\n",
        "            if headnotes_match:\n",
        "                headnotes_content = headnotes_match.group(1).strip()\n",
        "                page_content.append([page_num + 1, \"Headnotes\", headnotes_content])\n",
        "\n",
        "\n",
        "            hn_matches = re.findall(r'HN\\d+\\[|[^.]+>.*?(?=HN\\d+|$)|(?<=\\>).*?(?=Lawyers\\' Counsel or Judges)', text, re.DOTALL)\n",
        "\n",
        "\n",
        "            for match in hn_matches:\n",
        "                if 'HN' in match:\n",
        "                    if ongoing_hn_label and ongoing_hn_content:\n",
        "                        page_content.append([page_num + 1, ongoing_hn_label, ongoing_hn_content.strip()])\n",
        "                    ongoing_hn_label = match\n",
        "                    ongoing_hn_content = \"\"\n",
        "                elif \">\" in match and ongoing_hn_label:\n",
        "                    ongoing_hn_content += match.strip() + \" \"\n",
        "\n",
        "\n",
        "            if ongoing_hn_label and ongoing_hn_content:\n",
        "                page_content.append([page_num + 1, ongoing_hn_label, ongoing_hn_content.strip()])\n",
        "                ongoing_hn_label = None\n",
        "                ongoing_hn_content = \"\"\n",
        "\n",
        "        with open(csv_file_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
        "            csv_writer = csv.writer(csv_file)\n",
        "            csv_writer.writerow(['Page', 'Section', 'Content'])\n",
        "            csv_writer.writerows(page_content)\n",
        "\n",
        "pdf_file_path = '/content/drive/MyDrive/textmining/Federal District (30).PDF'\n",
        "csv_file_path = '/content/drive/MyDrive/textmining/HN/HN_District_wrong.csv'\n",
        "\n",
        "extract_and_process_segments_with_page_number(pdf_file_path, csv_file_path)\n"
      ],
      "metadata": {
        "id": "Pn4oPdrUwUdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OyyUqxiQKUsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HN CSV for Surperior\n",
        "# Surperior 只有 \"Core Terms\" 的內容 以 Judges or Counsel or Opinion 作為結尾切割\n",
        "\n",
        "# !pip install PyMuPDF\n",
        "# !pip install --upgrade PyMuPDF\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import csv\n",
        "\n",
        "def extract_core_terms(text):\n",
        "    # 使用非貪婪匹配提取 Core Terms 內容\n",
        "    core_terms_match = re.search(r'Core Terms(.*?)(Opinion|Counsel|Judges|$)', text, re.DOTALL)\n",
        "    if core_terms_match:\n",
        "        core_terms_content = core_terms_match.group(1).strip()\n",
        "        return core_terms_content\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def extract_and_process_segments_with_page_number(pdf_file_path, csv_file_path):\n",
        "    with fitz.open(pdf_file_path) as doc:\n",
        "        page_content = []  # 用於存儲頁面和對應的內容\n",
        "\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            text = page.get_text()\n",
        "\n",
        "            core_terms_content = extract_core_terms(text)\n",
        "            if core_terms_content:\n",
        "                page_content.append([page_num + 1, \"Core Terms\", core_terms_content])\n",
        "\n",
        "        # 寫入CSV文件\n",
        "        with open(csv_file_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
        "            csv_writer = csv.writer(csv_file)\n",
        "            csv_writer.writerow(['Page', 'Section', 'Content'])\n",
        "            csv_writer.writerows(page_content)\n",
        "\n",
        "# 指定的文件路徑\n",
        "pdf_file_path = '/content/drive/MyDrive/textmining/State Surperior Court (CA) (30).PDF'\n",
        "csv_file_path = '/content/drive/MyDrive/textmining/Core_Terms_output_with_page.csv'\n",
        "\n",
        "# 調用函數\n",
        "extract_and_process_segments_with_page_number(pdf_file_path, csv_file_path)\n"
      ],
      "metadata": {
        "id": "YPBeCWRZKnE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# opinion by , councel,judges,\n",
        "import fitz\n",
        "import re\n",
        "import csv\n",
        "\n",
        "pdf_file_path = '/content/drive/MyDrive/textmining/StateSupremeCourt.PDF'\n",
        "csv_file_path = '/content/drive/MyDrive/textmining/opinion_Surpreme.csv'\n",
        "\n",
        "pattern = r'(Counsel:.*?|Judges:.*?|Opinion by:.*?)(?=Counsel:|Judges:|Opinion by:|Opinion|$)'\n",
        "\n",
        "with fitz.open(pdf_file_path) as doc:\n",
        "    with open(csv_file_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "        csv_writer.writerow(['頁碼', '內容'])\n",
        "\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            text = page.get_text(\"text\")\n",
        "\n",
        "            matches = re.findall(pattern, text, re.DOTALL)\n",
        "\n",
        "            for match in matches:\n",
        "                csv_writer.writerow([page_num + 1, match.strip()])\n",
        "\n",
        "print(f\"完成：{csv_file_path}\")\n",
        "\n",
        "# Appeal p.211後面的judges 包含 opinion by 所以摘出來的judges為空白"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce50d1e6-f72c-42a7-badd-6cba64889b2d",
        "id": "HgmHgQGfd-qd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "完成：/content/drive/MyDrive/textmining/opinion_Surpreme.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 解決Surpreme 層級的 Judges空白問題，（p.809會因為Counsel內容跨頁 而無法抓到跨頁的部分）\n",
        "import fitz\n",
        "import re\n",
        "import csv\n",
        "\n",
        "pdf_file_path = '/content/drive/MyDrive/textmining/State Appeal Court (CA).PDF'\n",
        "csv_file_path = '/content/drive/MyDrive/textmining/opinion_Appeal.csv'\n",
        "\n",
        "pattern = r'(Counsel:.*?|Judges:.*?(?=Opinion by:)|Opinion by:.*?)((?=Counsel:)|(?=Judges:)|(?=Opinion by:)|(?=Opinion)|$)'\n",
        "# \" (Counsel:.*?|Judges:.*?(?=Opinion by:)|Opinion by:.*?) \"  :\n",
        "# capture content of \" Counsel: \", \" Judges: \" 到 \"Opinion by:\" or \"Opinion\"\n",
        "\n",
        "# \" ((?=Counsel:)|(?=Judges:)|(?=Opinion by:)|(?=Opinion)|$) \" :\n",
        "# capture the type of the match (\"Counsel:\", \"Judges:\", \"Opinion by:\", \"Opinion\", or the end of the string).\n",
        "\n",
        "with fitz.open(pdf_file_path) as doc:\n",
        "    with open(csv_file_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "        csv_writer.writerow(['頁碼', '內容類型', '內容'])\n",
        "\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            text = page.get_text(\"text\")\n",
        "\n",
        "            matches = re.finditer(pattern, text, re.DOTALL) # find all occourance of pattern in 'text'\n",
        "            last_match_type = None # keep track last match type\n",
        "\n",
        "            for match in matches: #loop initiate matches from re.finditer\n",
        "                match_content = match.group(1).strip() # retrive re captured content from group1\n",
        "                match_type = match.group(2).strip()\n",
        "\n",
        "                if 'Counsel:' in match_content:\n",
        "                    last_match_type = 'Counsel'\n",
        "                    csv_writer.writerow([page_num + 1, 'Counsel', match_content])\n",
        "                elif 'Judges:' in match_content:\n",
        "                    last_match_type = 'Judges'\n",
        "                    csv_writer.writerow([page_num + 1, 'Judges', match_content])\n",
        "                elif 'Opinion by:' in match_content:\n",
        "                    last_match_type = 'Opinion by'\n",
        "                    csv_writer.writerow([page_num + 1, 'Opinion by', match_content])\n",
        "                else:\n",
        "                    # 若上一個匹配的標記是 Opinion by 且 current type not Judges 則  write Judges' content into csv\n",
        "                    if last_match_type == 'Opinion by' and 'Judges:' not in match_type:\n",
        "                        csv_writer.writerow([page_num + 1, 'Judges', match_content])\n",
        "\n",
        "                    # 檢查下一頁的開頭是否屬於這個匹配的結尾, 看matches後面是否有更多content, 將next 20 character start with Counsel to be \"None\"\n",
        "                    if match.end() < len(text) and text[match.end():match.end()+20].strip().startswith('Counsel:'):\n",
        "                        last_match_type = None  # 現在的內容跨頁了\n",
        "\n",
        "print(f\"完成：{csv_file_path}\")\n"
      ],
      "metadata": {
        "id": "m0AJ_JMIntxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HN TXT 提取"
      ],
      "metadata": {
        "id": "rsggOBvm2XlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HN TXT STEP1\n",
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "\n",
        "def extract_segments(pdf_file_path, txt_file_path):\n",
        "    # 打開PDF檔案\n",
        "    with fitz.open(pdf_file_path) as doc:\n",
        "        text = \"\"\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "\n",
        "    start_marker = 'Core Terms'\n",
        "    end_marker = 'Counsel' or 'Judges'\n",
        "    pattern = re.compile(re.escape(start_marker) + '.*?' + re.escape(end_marker), re.DOTALL)\n",
        "\n",
        "    matches = pattern.findall(text)\n",
        "\n",
        "    with open(txt_file_path, 'w') as file:\n",
        "        for match in matches:\n",
        "            file.write(match + '\\n\\n-----\\n\\n')\n",
        "\n",
        "    return txt_file_path\n",
        "extracted_file = extract_segments('/content/drive/MyDrive/textmining/Federal Circuit (30).PDF', '/content/drive/MyDrive/textmining/HN/HN_Circuit.txt')\n",
        "\n",
        "print(f\"Extracted segments are saved in {extracted_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg9Pke3pR2lo",
        "outputId": "f990312a-d4fd-4dc0-a5bf-9de86bc1cf15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted segments are saved in /content/drive/MyDrive/textmining/HN/HN_Circuit.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HN TXT STEP 2\n",
        "import re\n",
        "\n",
        "txt_input_path = '/content/drive/MyDrive/textmining/HN/HN_Circuit.txt'\n",
        "txt_output_path = '/content/drive/MyDrive/textmining/HN/HN_Circuit_output.txt'\n",
        "\n",
        "with open(txt_input_path, 'r') as file:\n",
        "    full_text = file.read()\n",
        "\n",
        "matches1 = re.findall(r'HN\\d+\\[|[^.]+>.*?(?=HN\\d+)|(?<=\\>).*?(?=Lawyers\\' Counsel or Judges)', full_text, re.DOTALL)\n",
        "cleaned_matches = [re.sub(r'Page \\d+ of \\d+', '', match) for match in matches1]\n",
        "\n",
        "with open(txt_output_path, 'w') as txt_file:\n",
        "    for match in cleaned_matches:\n",
        "        txt_file.write(match.strip() + \"\\n\\n\")\n",
        "\n",
        "print(f\"完成：{txt_output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrwKZH35R40j",
        "outputId": "830ea66d-eff7-4f36-9367-750c7e4bc90e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "完成：/content/drive/MyDrive/textmining/HN/HN_Circuit_output.txt\n"
          ]
        }
      ]
    }
  ]
}