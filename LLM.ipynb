{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "\n",
    "# 加載預訓練的 BERT 模型和分詞器\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from datasets import Dataset\n",
    "\n",
    "# # 假設您已經有了知識圖譜數據\n",
    "# data = [\n",
    "#     {\"Name\": \"Entry Barriers\", \"Definition\": \"Factors that deter or prevent new competitors from entering a market.\"},\n",
    "#     {\"Name\": \"Natural Barriers\", \"Definition\": \"Barriers due to market characteristics or geographical constraints.\"},\n",
    "#     {\"Name\": \"Artificial Barriers\", \"Definition\": \"Barriers created by firms or legal structures to restrict entry.\"},\n",
    "#     {\"Name\": \"Strategic Barriers\", \"Definition\": \"Barriers created intentionally by existing firms to deter competition.\"},\n",
    "#     {\"Name\": \"Economies of Scale\", \"Definition\": \"Cost advantages due to increased production, making it harder for new entrants to compete.\"},\n",
    "#     {\"Name\": \"High Initial Capital Requirements\", \"Definition\": \"Significant upfront investments required to enter a market.\"},\n",
    "#     {\"Name\": \"Geographic Constraints\", \"Definition\": \"Barriers arising from location-specific factors such as access to resources.\"},\n",
    "#     {\"Name\": \"Patent Protection\", \"Definition\": \"Legal barriers that prevent competitors from using patented technologies or processes.\"},\n",
    "#     {\"Name\": \"Brand Loyalty\", \"Definition\": \"Consumer preference for existing brands, reducing new entrants' market share.\"},\n",
    "#     {\"Name\": \"Government Regulations\", \"Definition\": \"Legal or administrative restrictions that limit market entry.\"},\n",
    "#     {\"Name\": \"Predatory Pricing\", \"Definition\": \"Deliberate underpricing to drive out competitors or prevent new entry.\"},\n",
    "#     {\"Name\": \"Network Effects\", \"Definition\": \"The value of a product increases as more people use it, deterring new competitors.\"},\n",
    "#     {\"Name\": \"Vertical Integration\", \"Definition\": \"Control over supply chain that limits competitors' access to resources or distribution.\"}\n",
    "# ]\n",
    "\n",
    "# # 轉換為訓練格式，並將 'answer' 設置為 'Name'\n",
    "# train_data = [{\"question\": f\"What is {item['Name']}?\", \"context\": item[\"Definition\"], \"answer\": item[\"Name\"]} for item in data]\n",
    "\n",
    "# # 分割數據集：80% 用於訓練，20% 用於驗證\n",
    "# train_data_split, val_data_split = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# # 轉換為 Hugging Face Dataset 格式\n",
    "# train_dataset = Dataset.from_dict({\n",
    "#     \"question\": [entry[\"question\"] for entry in train_data_split],\n",
    "#     \"context\": [entry[\"context\"] for entry in train_data_split],\n",
    "#     \"answer\": [entry[\"answer\"] for entry in train_data_split],  # 新增 answer 字段\n",
    "# })\n",
    "\n",
    "# val_dataset = Dataset.from_dict({\n",
    "#     \"question\": [entry[\"question\"] for entry in val_data_split],\n",
    "#     \"context\": [entry[\"context\"] for entry in val_data_split],\n",
    "#     \"answer\": [entry[\"answer\"] for entry in val_data_split],  # 新增 answer 字段\n",
    "# })\n",
    "\n",
    "# # 查看數據集格式\n",
    "# print(\"Train Dataset:\", train_dataset)\n",
    "# print(\"Validation Dataset:\", val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 合併DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Case Index  Page                                          Paragraph\n",
      "0               1    26  Opinion\\n[*42] ORDER\\nWOLF, D.J.\\nFebruary 3, ...\n",
      "1               1    27                                       Page 7 of 15\n",
      "2               1    27  Beverages, Inc. (\"MWMBI\"), a trade association...\n",
      "3               1    28                                       Page 8 of 15\n",
      "4               1    28  The Magistrate Judge's Report and Recommendati...\n",
      "...           ...   ...                                                ...\n",
      "24740         100  1875  Accordingly, considering the Kansas state cour...\n",
      "24741         100  1876                                      Page 11 of 12\n",
      "24742         100  1876  Plaintiffs' Complaint asserts that defendants'...\n",
      "24743         100  1877                                      Page 12 of 12\n",
      "24744         100  1877  obtaining relief does not trigger K.S.A. 60-51...\n",
      "\n",
      "[24745 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 指定資料夾路徑\n",
    "folder_path = r'C:\\Users\\User\\Dropbox\\textmining1\\Data2_Opinion_low'  # 替換成您的資料夾路徑\n",
    "\n",
    "# 遍歷資料夾中的所有 CSV 文件\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('_opinion.csv')]\n",
    "\n",
    "# 用來儲存所有 CSV 的列表\n",
    "dataframes = []\n",
    "\n",
    "# 讀取每個 CSV 文件並加載到 DataFrame 中\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    df = pd.read_csv(file_path)  # 讀取 CSV\n",
    "    dataframes.append(df)  # 將 DataFrame 添加到列表中\n",
    "\n",
    "# 如果需要，將所有 DataFrame 合併成一個大的 DataFrame\n",
    "final_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "# 顯示合併後的 DataFrame\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 導入必要的庫和設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from tqdm.notebook import tqdm  # 用於顯示進度條\n",
    "\n",
    "# 2. Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # 如果返回 True，則說明您的環境可以使用 GPU\n",
    "print(torch.cuda.current_device())  # 顯示當前的 GPU 設備索引\n",
    "print(torch.cuda.get_device_name(0))  # 顯示當前 GPU 的名稱\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加載模型並將其移動到 GPU (如果有)\n",
    "model = BertForQuestionAnswering.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀取所有 CSV 文件並合併"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b75810422444adad8318d885c1496b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading CSV files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Case Index  Page                                          Paragraph\n",
      "0           1    26  Opinion\\n[*42] ORDER\\nWOLF, D.J.\\nFebruary 3, ...\n",
      "1           1    27                                       Page 7 of 15\n",
      "2           1    27  Beverages, Inc. (\"MWMBI\"), a trade association...\n",
      "3           1    28                                       Page 8 of 15\n",
      "4           1    28  The Magistrate Judge's Report and Recommendati...\n"
     ]
    }
   ],
   "source": [
    "# 3. Set folder path and read CSV files\n",
    "folder_path = r'C:\\Users\\User\\Dropbox\\textmining1\\Data2_Opinion_low'  # 替換成您的資料夾路徑\n",
    "\n",
    "# Get all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('_opinion.csv')]\n",
    "\n",
    "# Initialize list to store dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Read each CSV file and append it to the list\n",
    "for csv_file in tqdm(csv_files, desc=\"Reading CSV files\"):\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    df = pd.read_csv(file_path)  # Read CSV\n",
    "    dataframes.append(df)  # Add DataFrame to the list\n",
    "\n",
    "# Merge all dataframes into one large dataframe\n",
    "final_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Display merged dataframe\n",
    "print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 裁剪文本並創建問答對"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Function to split text into chunks (max_length 512 tokens)\n",
    "def split_text_into_chunks(text, max_length=256):\n",
    "    # Tokenize the text, do not truncate yet\n",
    "    tokens = tokenizer.encode(text, truncation=False)\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    # Split the tokens into chunks of size max_length\n",
    "    while len(tokens) > max_length:\n",
    "        chunk = tokens[:max_length]  # First chunk of max_length tokens\n",
    "        chunks.append(chunk)         # Append the chunk to the list\n",
    "        tokens = tokens[max_length:] # Remaining tokens to be processed\n",
    "    \n",
    "    # If there are leftover tokens, process them\n",
    "    if len(tokens) > 0:\n",
    "        chunks.append(tokens)\n",
    "    \n",
    "    # Ensure that each chunk is strictly max_length or less\n",
    "    chunks = [chunk[:max_length] for chunk in chunks]  # Ensure no chunk exceeds max_length\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00941d72fd34ca788e605f7e86b3e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing text:   0%|          | 0/24745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (854 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# 5. Create Q&A pairs and split the text\n",
    "train_data = []\n",
    "for _, row in tqdm(final_df.iterrows(), total=final_df.shape[0], desc=\"Processing text\"):\n",
    "    case_index = row['Case Index']\n",
    "    paragraph = row['Paragraph']\n",
    "    \n",
    "    # Split paragraph into chunks\n",
    "    paragraph_chunks = split_text_into_chunks(paragraph)\n",
    "    \n",
    "    for chunk in paragraph_chunks:\n",
    "        # Decode tokens back to text\n",
    "        context = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "        \n",
    "        # Create question and answer pair\n",
    "        question = f\"What is the judge's opinion regarding entry barriers in case {case_index}?\"\n",
    "        train_data.append({\"question\": question, \"context\": context, \"answer\": context})\n",
    "        \n",
    "        # If prosecutor's argument is involved\n",
    "        question_prosecutor = f\"What is the prosecutor's argument regarding entry barriers in case {case_index}?\"\n",
    "        train_data.append({\"question\": question_prosecutor, \"context\": context, \"answer\": context})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': 'opinion [ * 42 ] order wolf, d. j. february 3, 1998 the following analysis is based upon the transcript of the decision rendered orally on january 27, 1998, granting plaintiffs\\'motions for summary judgment ( docket nos. 158 & 166 ) and denying defendants\\'motions for summary judgment ( docket nos. 162 & 169 ). this memorandum adds citations, revises and amplifies some [ * * 2 ] of the discussion, and deletes certain non - essential matters. the transcripts of the hearings on january 26 and 27, 1998, are being prepared and may be acquired from the court reporter. * * * * the present plaintiffs in this case are sea shore corporation, which does business as canterbury liquors and pantry, a licensed retailer of alcoholic beverages, 1 and an intervenor as plaintiff, whitehall company limited, a licensed wholesaler of alcoholic [ * 43 ] beverages 2 [ * * 3 ] ( collectively, the \" plaintiffs \" ). the defendants are the chairman and commissioners of the massachusetts alcoholic beverages and control commission ( \" the commission \" ), sued in their official capacities, 3 and a defendant - intervenor, massachusetts wholesalers of malt 16 f. supp. 2d 41, * 41 ; 1998 u', 'answer': 'opinion [ * 42 ] order wolf, d. j. february 3, 1998 the following analysis is based upon the transcript of the decision rendered orally on january 27, 1998, granting plaintiffs\\'motions for summary judgment ( docket nos. 158 & 166 ) and denying defendants\\'motions for summary judgment ( docket nos. 162 & 169 ). this memorandum adds citations, revises and amplifies some [ * * 2 ] of the discussion, and deletes certain non - essential matters. the transcripts of the hearings on january 26 and 27, 1998, are being prepared and may be acquired from the court reporter. * * * * the present plaintiffs in this case are sea shore corporation, which does business as canterbury liquors and pantry, a licensed retailer of alcoholic beverages, 1 and an intervenor as plaintiff, whitehall company limited, a licensed wholesaler of alcoholic [ * 43 ] beverages 2 [ * * 3 ] ( collectively, the \" plaintiffs \" ). the defendants are the chairman and commissioners of the massachusetts alcoholic beverages and control commission ( \" the commission \" ), sued in their official capacities, 3 and a defendant - intervenor, massachusetts wholesalers of malt 16 f. supp. 2d 41, * 41 ; 1998 u'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': 'opinion [ * 42 ] order wolf, d. j. february 3, 1998 the following analysis is based upon the transcript of the decision rendered orally on january 27, 1998, granting plaintiffs\\'motions for summary judgment ( docket nos. 158 & 166 ) and denying defendants\\'motions for summary judgment ( docket nos. 162 & 169 ). this memorandum adds citations, revises and amplifies some [ * * 2 ] of the discussion, and deletes certain non - essential matters. the transcripts of the hearings on january 26 and 27, 1998, are being prepared and may be acquired from the court reporter. * * * * the present plaintiffs in this case are sea shore corporation, which does business as canterbury liquors and pantry, a licensed retailer of alcoholic beverages, 1 and an intervenor as plaintiff, whitehall company limited, a licensed wholesaler of alcoholic [ * 43 ] beverages 2 [ * * 3 ] ( collectively, the \" plaintiffs \" ). the defendants are the chairman and commissioners of the massachusetts alcoholic beverages and control commission ( \" the commission \" ), sued in their official capacities, 3 and a defendant - intervenor, massachusetts wholesalers of malt 16 f. supp. 2d 41, * 41 ; 1998 u', 'answer': 'opinion [ * 42 ] order wolf, d. j. february 3, 1998 the following analysis is based upon the transcript of the decision rendered orally on january 27, 1998, granting plaintiffs\\'motions for summary judgment ( docket nos. 158 & 166 ) and denying defendants\\'motions for summary judgment ( docket nos. 162 & 169 ). this memorandum adds citations, revises and amplifies some [ * * 2 ] of the discussion, and deletes certain non - essential matters. the transcripts of the hearings on january 26 and 27, 1998, are being prepared and may be acquired from the court reporter. * * * * the present plaintiffs in this case are sea shore corporation, which does business as canterbury liquors and pantry, a licensed retailer of alcoholic beverages, 1 and an intervenor as plaintiff, whitehall company limited, a licensed wholesaler of alcoholic [ * 43 ] beverages 2 [ * * 3 ] ( collectively, the \" plaintiffs \" ). the defendants are the chairman and commissioners of the massachusetts alcoholic beverages and control commission ( \" the commission \" ), sued in their official capacities, 3 and a defendant - intervenor, massachusetts wholesalers of malt 16 f. supp. 2d 41, * 41 ; 1998 u'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': '. s. dist. lexis 1155, * * 1', 'answer': '. s. dist. lexis 1155, * * 1'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': '. s. dist. lexis 1155, * * 1', 'answer': '. s. dist. lexis 1155, * * 1'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': 'page 7 of 15', 'answer': 'page 7 of 15'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': 'page 7 of 15', 'answer': 'page 7 of 15'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': 'beverages, inc. ( \" mwmbi \" ), a trade association whose members are engaged primarily in the wholesale sale beer 4 ( collectively, the \" defendants \" ). the plaintiffs brought this action seeking declaratory and injunctive relief. count i seeks a declaration that m. g. l. c. 138, § 25a which relates to the pricing of wholesale liquor, violates § 1 of the sherman act both on its face and as applied, and that it is not shielded from invalidation by the immunity doctrine enunciated in parker v. brown, 317 u. s. 341, 87 l. ed. 315, 63 s. ct. 307 ( 1943 ). count i also seeks an order permanently enjoining the commission from enforcing the state statute. see verified complaint at 7 - 9, pp 25 - 30. count ii seeks the same declaratory and injunctive relief with respect to the regulations [ * * 4 ] promulgated by the commission to implement § 25a. 204 c. m. r. § § 6. 00 - 6. 07. see verified complaint at 9 - 11, pp 31 - 36. the plaintiffs\\'fundamental contention is that the massachusetts regulatory scheme concerning the pricing of wholesale liquor violates § 1 of', 'answer': 'beverages, inc. ( \" mwmbi \" ), a trade association whose members are engaged primarily in the wholesale sale beer 4 ( collectively, the \" defendants \" ). the plaintiffs brought this action seeking declaratory and injunctive relief. count i seeks a declaration that m. g. l. c. 138, § 25a which relates to the pricing of wholesale liquor, violates § 1 of the sherman act both on its face and as applied, and that it is not shielded from invalidation by the immunity doctrine enunciated in parker v. brown, 317 u. s. 341, 87 l. ed. 315, 63 s. ct. 307 ( 1943 ). count i also seeks an order permanently enjoining the commission from enforcing the state statute. see verified complaint at 7 - 9, pp 25 - 30. count ii seeks the same declaratory and injunctive relief with respect to the regulations [ * * 4 ] promulgated by the commission to implement § 25a. 204 c. m. r. § § 6. 00 - 6. 07. see verified complaint at 9 - 11, pp 31 - 36. the plaintiffs\\'fundamental contention is that the massachusetts regulatory scheme concerning the pricing of wholesale liquor violates § 1 of'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': 'beverages, inc. ( \" mwmbi \" ), a trade association whose members are engaged primarily in the wholesale sale beer 4 ( collectively, the \" defendants \" ). the plaintiffs brought this action seeking declaratory and injunctive relief. count i seeks a declaration that m. g. l. c. 138, § 25a which relates to the pricing of wholesale liquor, violates § 1 of the sherman act both on its face and as applied, and that it is not shielded from invalidation by the immunity doctrine enunciated in parker v. brown, 317 u. s. 341, 87 l. ed. 315, 63 s. ct. 307 ( 1943 ). count i also seeks an order permanently enjoining the commission from enforcing the state statute. see verified complaint at 7 - 9, pp 25 - 30. count ii seeks the same declaratory and injunctive relief with respect to the regulations [ * * 4 ] promulgated by the commission to implement § 25a. 204 c. m. r. § § 6. 00 - 6. 07. see verified complaint at 9 - 11, pp 31 - 36. the plaintiffs\\'fundamental contention is that the massachusetts regulatory scheme concerning the pricing of wholesale liquor violates § 1 of', 'answer': 'beverages, inc. ( \" mwmbi \" ), a trade association whose members are engaged primarily in the wholesale sale beer 4 ( collectively, the \" defendants \" ). the plaintiffs brought this action seeking declaratory and injunctive relief. count i seeks a declaration that m. g. l. c. 138, § 25a which relates to the pricing of wholesale liquor, violates § 1 of the sherman act both on its face and as applied, and that it is not shielded from invalidation by the immunity doctrine enunciated in parker v. brown, 317 u. s. 341, 87 l. ed. 315, 63 s. ct. 307 ( 1943 ). count i also seeks an order permanently enjoining the commission from enforcing the state statute. see verified complaint at 7 - 9, pp 25 - 30. count ii seeks the same declaratory and injunctive relief with respect to the regulations [ * * 4 ] promulgated by the commission to implement § 25a. 204 c. m. r. § § 6. 00 - 6. 07. see verified complaint at 9 - 11, pp 31 - 36. the plaintiffs\\'fundamental contention is that the massachusetts regulatory scheme concerning the pricing of wholesale liquor violates § 1 of'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': 'the sherman act. section 1 states, in pertinent part, that \" every contract, combination in the form of trust or otherwise, or conspiracy, in restraint of trade or commerce among the several states... is illegal. \" 15 u. s. c. § 1. the supreme court has explained that : the sherman act was designed to be a comprehensive charter of economic liberty aimed at preserving free and unfettered competition as the rule of trade. it rests on the premise that the unrestrained interaction of competitive forces will yield the best allocation of our economic resources, the lowest prices, the highest quality, and the greatest material progress the policy unequivocally laid down by the act is competition. northern pacific ry. co. v. united states, 356 u. s. 1, 4, 2 l. ed. 2d 545, 78 s. ct. 514 ( 1958 ). the supreme court has held that hn1 [ ] some conduct constitutes a per se violation of § 1. id. at [ * * 5 ] 5. all other conduct is subject to a rule of reason test. broadcast music, inc. v. columbia broad. sys., inc., 441 u. s. 1, 8, 60 l', 'answer': 'the sherman act. section 1 states, in pertinent part, that \" every contract, combination in the form of trust or otherwise, or conspiracy, in restraint of trade or commerce among the several states... is illegal. \" 15 u. s. c. § 1. the supreme court has explained that : the sherman act was designed to be a comprehensive charter of economic liberty aimed at preserving free and unfettered competition as the rule of trade. it rests on the premise that the unrestrained interaction of competitive forces will yield the best allocation of our economic resources, the lowest prices, the highest quality, and the greatest material progress the policy unequivocally laid down by the act is competition. northern pacific ry. co. v. united states, 356 u. s. 1, 4, 2 l. ed. 2d 545, 78 s. ct. 514 ( 1958 ). the supreme court has held that hn1 [ ] some conduct constitutes a per se violation of § 1. id. at [ * * 5 ] 5. all other conduct is subject to a rule of reason test. broadcast music, inc. v. columbia broad. sys., inc., 441 u. s. 1, 8, 60 l'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': 'the sherman act. section 1 states, in pertinent part, that \" every contract, combination in the form of trust or otherwise, or conspiracy, in restraint of trade or commerce among the several states... is illegal. \" 15 u. s. c. § 1. the supreme court has explained that : the sherman act was designed to be a comprehensive charter of economic liberty aimed at preserving free and unfettered competition as the rule of trade. it rests on the premise that the unrestrained interaction of competitive forces will yield the best allocation of our economic resources, the lowest prices, the highest quality, and the greatest material progress the policy unequivocally laid down by the act is competition. northern pacific ry. co. v. united states, 356 u. s. 1, 4, 2 l. ed. 2d 545, 78 s. ct. 514 ( 1958 ). the supreme court has held that hn1 [ ] some conduct constitutes a per se violation of § 1. id. at [ * * 5 ] 5. all other conduct is subject to a rule of reason test. broadcast music, inc. v. columbia broad. sys., inc., 441 u. s. 1, 8, 60 l', 'answer': 'the sherman act. section 1 states, in pertinent part, that \" every contract, combination in the form of trust or otherwise, or conspiracy, in restraint of trade or commerce among the several states... is illegal. \" 15 u. s. c. § 1. the supreme court has explained that : the sherman act was designed to be a comprehensive charter of economic liberty aimed at preserving free and unfettered competition as the rule of trade. it rests on the premise that the unrestrained interaction of competitive forces will yield the best allocation of our economic resources, the lowest prices, the highest quality, and the greatest material progress the policy unequivocally laid down by the act is competition. northern pacific ry. co. v. united states, 356 u. s. 1, 4, 2 l. ed. 2d 545, 78 s. ct. 514 ( 1958 ). the supreme court has held that hn1 [ ] some conduct constitutes a per se violation of § 1. id. at [ * * 5 ] 5. all other conduct is subject to a rule of reason test. broadcast music, inc. v. columbia broad. sys., inc., 441 u. s. 1, 8, 60 l'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': '. ed. 2d 1, 99 s. ct. 1551 ( 1979 ). with regard to the procedural history of this matter, which began in 1994, the parties engaged in the limited discovery relevant to the alleged per se violation of § 1. the parties filed cross - motions for summary judgment on this issue. the parties agreed at the hearing on january 26, 1998, that this issue can be decided on the cross - motions or summary judgment. more specifically, the defendants assert that there are no material facts in dispute. the plaintiffs contend that the question of whether the state actually monitors its regulatory scheme is not undisputed, but that this factual question is not material. as i will describe later, i agree that this issue is not material. thus, the issues relating to the alleged per se violation can and should be decided on the cross - motions for summary judgement. at the inception of this case i referred it to the magistrate judge. the magistrate judge heard argument on the cross - motions for summary judgment in december, 1996. he issued a report and recommendation [ * * 6 ] on june 27, 1997 ( the \" report \" ). he recommended that summary judgment be entered for the defendants. the plaintiffs filed objections to the report. the defendants responded.', 'answer': '. ed. 2d 1, 99 s. ct. 1551 ( 1979 ). with regard to the procedural history of this matter, which began in 1994, the parties engaged in the limited discovery relevant to the alleged per se violation of § 1. the parties filed cross - motions for summary judgment on this issue. the parties agreed at the hearing on january 26, 1998, that this issue can be decided on the cross - motions or summary judgment. more specifically, the defendants assert that there are no material facts in dispute. the plaintiffs contend that the question of whether the state actually monitors its regulatory scheme is not undisputed, but that this factual question is not material. as i will describe later, i agree that this issue is not material. thus, the issues relating to the alleged per se violation can and should be decided on the cross - motions for summary judgement. at the inception of this case i referred it to the magistrate judge. the magistrate judge heard argument on the cross - motions for summary judgment in december, 1996. he issued a report and recommendation [ * * 6 ] on june 27, 1997 ( the \" report \" ). he recommended that summary judgment be entered for the defendants. the plaintiffs filed objections to the report. the defendants responded.'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': '. ed. 2d 1, 99 s. ct. 1551 ( 1979 ). with regard to the procedural history of this matter, which began in 1994, the parties engaged in the limited discovery relevant to the alleged per se violation of § 1. the parties filed cross - motions for summary judgment on this issue. the parties agreed at the hearing on january 26, 1998, that this issue can be decided on the cross - motions or summary judgment. more specifically, the defendants assert that there are no material facts in dispute. the plaintiffs contend that the question of whether the state actually monitors its regulatory scheme is not undisputed, but that this factual question is not material. as i will describe later, i agree that this issue is not material. thus, the issues relating to the alleged per se violation can and should be decided on the cross - motions for summary judgement. at the inception of this case i referred it to the magistrate judge. the magistrate judge heard argument on the cross - motions for summary judgment in december, 1996. he issued a report and recommendation [ * * 6 ] on june 27, 1997 ( the \" report \" ). he recommended that summary judgment be entered for the defendants. the plaintiffs filed objections to the report. the defendants responded.', 'answer': '. ed. 2d 1, 99 s. ct. 1551 ( 1979 ). with regard to the procedural history of this matter, which began in 1994, the parties engaged in the limited discovery relevant to the alleged per se violation of § 1. the parties filed cross - motions for summary judgment on this issue. the parties agreed at the hearing on january 26, 1998, that this issue can be decided on the cross - motions or summary judgment. more specifically, the defendants assert that there are no material facts in dispute. the plaintiffs contend that the question of whether the state actually monitors its regulatory scheme is not undisputed, but that this factual question is not material. as i will describe later, i agree that this issue is not material. thus, the issues relating to the alleged per se violation can and should be decided on the cross - motions for summary judgement. at the inception of this case i referred it to the magistrate judge. the magistrate judge heard argument on the cross - motions for summary judgment in december, 1996. he issued a report and recommendation [ * * 6 ] on june 27, 1997 ( the \" report \" ). he recommended that summary judgment be entered for the defendants. the plaintiffs filed objections to the report. the defendants responded.'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': 'i held a hearing on january 26, [ * 44 ] 1998, on the issues relating to the alleged per se violation of § 1. if the plaintiffs do not prevail on their claims of a per se violation, discovery and further litigation will be necessary with regard to their rule of reason claim. 16 f. supp. 2d 41, * 43 ; 1998 u. s. dist. lexis 1155, * * 3', 'answer': 'i held a hearing on january 26, [ * 44 ] 1998, on the issues relating to the alleged per se violation of § 1. if the plaintiffs do not prevail on their claims of a per se violation, discovery and further litigation will be necessary with regard to their rule of reason claim. 16 f. supp. 2d 41, * 43 ; 1998 u. s. dist. lexis 1155, * * 3'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': 'i held a hearing on january 26, [ * 44 ] 1998, on the issues relating to the alleged per se violation of § 1. if the plaintiffs do not prevail on their claims of a per se violation, discovery and further litigation will be necessary with regard to their rule of reason claim. 16 f. supp. 2d 41, * 43 ; 1998 u. s. dist. lexis 1155, * * 3', 'answer': 'i held a hearing on january 26, [ * 44 ] 1998, on the issues relating to the alleged per se violation of § 1. if the plaintiffs do not prevail on their claims of a per se violation, discovery and further litigation will be necessary with regard to their rule of reason claim. 16 f. supp. 2d 41, * 43 ; 1998 u. s. dist. lexis 1155, * * 3'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': 'page 8 of 15', 'answer': 'page 8 of 15'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': 'page 8 of 15', 'answer': 'page 8 of 15'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': \"the magistrate judge's report and recommendation is instructive. however, hn2 [ ] as a matter of law, this court is required to decide de novo the portions of the report placed in dispute by the objections. 28 u. s. c. § 636 ( b ) ( 1 ) ( c ). see also fed. r. civ. p. 72 ( b ) ; local rule 3 ( b ) of the rules for united states magistrates in the united states district court for the district of massachusetts. hn3 [ ] as these are motions for summary judgment, i am, with regard to each motion, required to look at the record in the light most favorable to the opposing party, woods v. friction materials, inc., 30 f. 3d 255, 259 ( 1st cir. 1994 ), and to decide if any material fact is genuinely placed in dispute by the admissible evidence. fed. r. civ. p. 56 ( c ) ; [ * * 7 ] anderson v. liberty lobby, 477 u. s. 242, 247 - 48, 91 l. ed. 2d 202, 106 s. ct. 2505 ( 1986 ). i agree with the parties that the evidence does not place any\", 'answer': \"the magistrate judge's report and recommendation is instructive. however, hn2 [ ] as a matter of law, this court is required to decide de novo the portions of the report placed in dispute by the objections. 28 u. s. c. § 636 ( b ) ( 1 ) ( c ). see also fed. r. civ. p. 72 ( b ) ; local rule 3 ( b ) of the rules for united states magistrates in the united states district court for the district of massachusetts. hn3 [ ] as these are motions for summary judgment, i am, with regard to each motion, required to look at the record in the light most favorable to the opposing party, woods v. friction materials, inc., 30 f. 3d 255, 259 ( 1st cir. 1994 ), and to decide if any material fact is genuinely placed in dispute by the admissible evidence. fed. r. civ. p. 56 ( c ) ; [ * * 7 ] anderson v. liberty lobby, 477 u. s. 242, 247 - 48, 91 l. ed. 2d 202, 106 s. ct. 2505 ( 1986 ). i agree with the parties that the evidence does not place any\"}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': \"the magistrate judge's report and recommendation is instructive. however, hn2 [ ] as a matter of law, this court is required to decide de novo the portions of the report placed in dispute by the objections. 28 u. s. c. § 636 ( b ) ( 1 ) ( c ). see also fed. r. civ. p. 72 ( b ) ; local rule 3 ( b ) of the rules for united states magistrates in the united states district court for the district of massachusetts. hn3 [ ] as these are motions for summary judgment, i am, with regard to each motion, required to look at the record in the light most favorable to the opposing party, woods v. friction materials, inc., 30 f. 3d 255, 259 ( 1st cir. 1994 ), and to decide if any material fact is genuinely placed in dispute by the admissible evidence. fed. r. civ. p. 56 ( c ) ; [ * * 7 ] anderson v. liberty lobby, 477 u. s. 242, 247 - 48, 91 l. ed. 2d 202, 106 s. ct. 2505 ( 1986 ). i agree with the parties that the evidence does not place any\", 'answer': \"the magistrate judge's report and recommendation is instructive. however, hn2 [ ] as a matter of law, this court is required to decide de novo the portions of the report placed in dispute by the objections. 28 u. s. c. § 636 ( b ) ( 1 ) ( c ). see also fed. r. civ. p. 72 ( b ) ; local rule 3 ( b ) of the rules for united states magistrates in the united states district court for the district of massachusetts. hn3 [ ] as these are motions for summary judgment, i am, with regard to each motion, required to look at the record in the light most favorable to the opposing party, woods v. friction materials, inc., 30 f. 3d 255, 259 ( 1st cir. 1994 ), and to decide if any material fact is genuinely placed in dispute by the admissible evidence. fed. r. civ. p. 56 ( c ) ; [ * * 7 ] anderson v. liberty lobby, 477 u. s. 242, 247 - 48, 91 l. ed. 2d 202, 106 s. ct. 2505 ( 1986 ). i agree with the parties that the evidence does not place any\"}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': 'material fact genuinely in dispute. thus, as i said, it is necessary and appropriate to decide now who is entitled to prevail as a matter of law. i have decided that the plaintiffs are entitled to prevail on their motion for summary judgment. i reach this conclusion essentially for the reasons stated by the ninth circuit in addressing a similar regulatory scheme in miller v. hedlund 813 f. 2d 1344 ( 9th cir. 1987 ), cert. denied, 484 u. s. 1061, 98 l. ed. 2d 983, 108 s. ct. 1018 ( 1988 ). comparable analysis was done by judge ralph winter in his dissent concerning the virtually identical regulatory scheme involved in battipaglia v. new york state liquor auth., 745 f. 2d 166, 179 ( 2d cir. 1984 ) ( winter, j., dissenting ), cert. denied, 470 u. s. 1027, 84 l. ed. 2d 782, 105 s. ct. 1393 ( 1985 ). the ninth circuit and judge winter essentially implement the analysis previously done by professor phillip areeda and his colleagues in their treatise on antitrust law ; the reasoning of the ninth circuit and judge winter has been endorsed [ * * 8 ] in the most recent', 'answer': 'material fact genuinely in dispute. thus, as i said, it is necessary and appropriate to decide now who is entitled to prevail as a matter of law. i have decided that the plaintiffs are entitled to prevail on their motion for summary judgment. i reach this conclusion essentially for the reasons stated by the ninth circuit in addressing a similar regulatory scheme in miller v. hedlund 813 f. 2d 1344 ( 9th cir. 1987 ), cert. denied, 484 u. s. 1061, 98 l. ed. 2d 983, 108 s. ct. 1018 ( 1988 ). comparable analysis was done by judge ralph winter in his dissent concerning the virtually identical regulatory scheme involved in battipaglia v. new york state liquor auth., 745 f. 2d 166, 179 ( 2d cir. 1984 ) ( winter, j., dissenting ), cert. denied, 470 u. s. 1027, 84 l. ed. 2d 782, 105 s. ct. 1393 ( 1985 ). the ninth circuit and judge winter essentially implement the analysis previously done by professor phillip areeda and his colleagues in their treatise on antitrust law ; the reasoning of the ninth circuit and judge winter has been endorsed [ * * 8 ] in the most recent'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': 'material fact genuinely in dispute. thus, as i said, it is necessary and appropriate to decide now who is entitled to prevail as a matter of law. i have decided that the plaintiffs are entitled to prevail on their motion for summary judgment. i reach this conclusion essentially for the reasons stated by the ninth circuit in addressing a similar regulatory scheme in miller v. hedlund 813 f. 2d 1344 ( 9th cir. 1987 ), cert. denied, 484 u. s. 1061, 98 l. ed. 2d 983, 108 s. ct. 1018 ( 1988 ). comparable analysis was done by judge ralph winter in his dissent concerning the virtually identical regulatory scheme involved in battipaglia v. new york state liquor auth., 745 f. 2d 166, 179 ( 2d cir. 1984 ) ( winter, j., dissenting ), cert. denied, 470 u. s. 1027, 84 l. ed. 2d 782, 105 s. ct. 1393 ( 1985 ). the ninth circuit and judge winter essentially implement the analysis previously done by professor phillip areeda and his colleagues in their treatise on antitrust law ; the reasoning of the ninth circuit and judge winter has been endorsed [ * * 8 ] in the most recent', 'answer': 'material fact genuinely in dispute. thus, as i said, it is necessary and appropriate to decide now who is entitled to prevail as a matter of law. i have decided that the plaintiffs are entitled to prevail on their motion for summary judgment. i reach this conclusion essentially for the reasons stated by the ninth circuit in addressing a similar regulatory scheme in miller v. hedlund 813 f. 2d 1344 ( 9th cir. 1987 ), cert. denied, 484 u. s. 1061, 98 l. ed. 2d 983, 108 s. ct. 1018 ( 1988 ). comparable analysis was done by judge ralph winter in his dissent concerning the virtually identical regulatory scheme involved in battipaglia v. new york state liquor auth., 745 f. 2d 166, 179 ( 2d cir. 1984 ) ( winter, j., dissenting ), cert. denied, 470 u. s. 1027, 84 l. ed. 2d 782, 105 s. ct. 1393 ( 1985 ). the ninth circuit and judge winter essentially implement the analysis previously done by professor phillip areeda and his colleagues in their treatise on antitrust law ; the reasoning of the ninth circuit and judge winter has been endorsed [ * * 8 ] in the most recent'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': 'edition of that treatise. 1 phillip e. areeda & herbert hovenkamp, antitrust law p 217, at 310 - 12 ( 1997 ). 5 [ * * 9 ] the statute now at issue, hn4 [ ] m. g. l. c. 138, § 25a, provides in pertinent part that : no licensee authorized under this chapter to sell alcoholic beverages to wholesalers or retailers shall - - ( a ) discriminate, directly or indirectly, in price,... between one wholesaler and another wholesaler, or between one retailer and another retailer purchasing alcoholic beverages bearing the same brand or trade name and of like age and quality. and then, in a further paragraph, it states : all price lists or price quotations made to a licensee by a wholesaler shall remain in effect for at least thirty days after the establishment of such price list or quotation. any sale by a wholesaler of any alcoholic beverages at prices lower than the price reflected in such price list or quotation within such thirty day period shall constitute price discrimination under this section. the plaintiffs do not challenge the broad prohibition against price discrimination contained in the first quoted paragraph, subsection ( a ). rather, they challenge only the provisions of the second quoted paragraph. h', 'answer': 'edition of that treatise. 1 phillip e. areeda & herbert hovenkamp, antitrust law p 217, at 310 - 12 ( 1997 ). 5 [ * * 9 ] the statute now at issue, hn4 [ ] m. g. l. c. 138, § 25a, provides in pertinent part that : no licensee authorized under this chapter to sell alcoholic beverages to wholesalers or retailers shall - - ( a ) discriminate, directly or indirectly, in price,... between one wholesaler and another wholesaler, or between one retailer and another retailer purchasing alcoholic beverages bearing the same brand or trade name and of like age and quality. and then, in a further paragraph, it states : all price lists or price quotations made to a licensee by a wholesaler shall remain in effect for at least thirty days after the establishment of such price list or quotation. any sale by a wholesaler of any alcoholic beverages at prices lower than the price reflected in such price list or quotation within such thirty day period shall constitute price discrimination under this section. the plaintiffs do not challenge the broad prohibition against price discrimination contained in the first quoted paragraph, subsection ( a ). rather, they challenge only the provisions of the second quoted paragraph. h'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': 'edition of that treatise. 1 phillip e. areeda & herbert hovenkamp, antitrust law p 217, at 310 - 12 ( 1997 ). 5 [ * * 9 ] the statute now at issue, hn4 [ ] m. g. l. c. 138, § 25a, provides in pertinent part that : no licensee authorized under this chapter to sell alcoholic beverages to wholesalers or retailers shall - - ( a ) discriminate, directly or indirectly, in price,... between one wholesaler and another wholesaler, or between one retailer and another retailer purchasing alcoholic beverages bearing the same brand or trade name and of like age and quality. and then, in a further paragraph, it states : all price lists or price quotations made to a licensee by a wholesaler shall remain in effect for at least thirty days after the establishment of such price list or quotation. any sale by a wholesaler of any alcoholic beverages at prices lower than the price reflected in such price list or quotation within such thirty day period shall constitute price discrimination under this section. the plaintiffs do not challenge the broad prohibition against price discrimination contained in the first quoted paragraph, subsection ( a ). rather, they challenge only the provisions of the second quoted paragraph. h', 'answer': 'edition of that treatise. 1 phillip e. areeda & herbert hovenkamp, antitrust law p 217, at 310 - 12 ( 1997 ). 5 [ * * 9 ] the statute now at issue, hn4 [ ] m. g. l. c. 138, § 25a, provides in pertinent part that : no licensee authorized under this chapter to sell alcoholic beverages to wholesalers or retailers shall - - ( a ) discriminate, directly or indirectly, in price,... between one wholesaler and another wholesaler, or between one retailer and another retailer purchasing alcoholic beverages bearing the same brand or trade name and of like age and quality. and then, in a further paragraph, it states : all price lists or price quotations made to a licensee by a wholesaler shall remain in effect for at least thirty days after the establishment of such price list or quotation. any sale by a wholesaler of any alcoholic beverages at prices lower than the price reflected in such price list or quotation within such thirty day period shall constitute price discrimination under this section. the plaintiffs do not challenge the broad prohibition against price discrimination contained in the first quoted paragraph, subsection ( a ). rather, they challenge only the provisions of the second quoted paragraph. h'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': '##n5 [ ] the related, relevant regulations require each wholesaler by the fifth day [ * * 10 ] of each month to post the prices that it will charge during the next following month for each item that the wholesaler sells. 204 c. m. r. § 6. 03 ( 3 ). the regulations permit the wholesaler [ * 45 ] by the fifteenth day of the month to amend its posted prices, but only to meet a specific lower price or a specific greater discount for an individual item filed by a competitor. 204 c. m. r. § 6. 05 ( 1 ). in other words, a wholesaler may not increase its previously posted price or decrease its price to any price other than the exact lower price for the same product that was posted by a rival wholesaler. the prices, as amended, become effective on the first day of the following month and they must remain unchanged throughout that calendar month. 204 c. m. r. § 6. 03 ( 3 ). plaintiffs challenge the validity of these regulations. 16 f. supp. 2d 41, * 44 ; 1998 u. s. dist. lexis 1155, * * 6', 'answer': '##n5 [ ] the related, relevant regulations require each wholesaler by the fifth day [ * * 10 ] of each month to post the prices that it will charge during the next following month for each item that the wholesaler sells. 204 c. m. r. § 6. 03 ( 3 ). the regulations permit the wholesaler [ * 45 ] by the fifteenth day of the month to amend its posted prices, but only to meet a specific lower price or a specific greater discount for an individual item filed by a competitor. 204 c. m. r. § 6. 05 ( 1 ). in other words, a wholesaler may not increase its previously posted price or decrease its price to any price other than the exact lower price for the same product that was posted by a rival wholesaler. the prices, as amended, become effective on the first day of the following month and they must remain unchanged throughout that calendar month. 204 c. m. r. § 6. 03 ( 3 ). plaintiffs challenge the validity of these regulations. 16 f. supp. 2d 41, * 44 ; 1998 u. s. dist. lexis 1155, * * 6'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': '##n5 [ ] the related, relevant regulations require each wholesaler by the fifth day [ * * 10 ] of each month to post the prices that it will charge during the next following month for each item that the wholesaler sells. 204 c. m. r. § 6. 03 ( 3 ). the regulations permit the wholesaler [ * 45 ] by the fifteenth day of the month to amend its posted prices, but only to meet a specific lower price or a specific greater discount for an individual item filed by a competitor. 204 c. m. r. § 6. 05 ( 1 ). in other words, a wholesaler may not increase its previously posted price or decrease its price to any price other than the exact lower price for the same product that was posted by a rival wholesaler. the prices, as amended, become effective on the first day of the following month and they must remain unchanged throughout that calendar month. 204 c. m. r. § 6. 03 ( 3 ). plaintiffs challenge the validity of these regulations. 16 f. supp. 2d 41, * 44 ; 1998 u. s. dist. lexis 1155, * * 6', 'answer': '##n5 [ ] the related, relevant regulations require each wholesaler by the fifth day [ * * 10 ] of each month to post the prices that it will charge during the next following month for each item that the wholesaler sells. 204 c. m. r. § 6. 03 ( 3 ). the regulations permit the wholesaler [ * 45 ] by the fifteenth day of the month to amend its posted prices, but only to meet a specific lower price or a specific greater discount for an individual item filed by a competitor. 204 c. m. r. § 6. 05 ( 1 ). in other words, a wholesaler may not increase its previously posted price or decrease its price to any price other than the exact lower price for the same product that was posted by a rival wholesaler. the prices, as amended, become effective on the first day of the following month and they must remain unchanged throughout that calendar month. 204 c. m. r. § 6. 03 ( 3 ). plaintiffs challenge the validity of these regulations. 16 f. supp. 2d 41, * 44 ; 1998 u. s. dist. lexis 1155, * * 6'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': 'page 9 of 15', 'answer': 'page 9 of 15'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': 'page 9 of 15', 'answer': 'page 9 of 15'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': 'hn6 [ ] in the federal system of government established by our constitution, the laws of the united states are the \" supreme law of the land, \" u. s. const. art. vi, and thus preempt and invalidate inconsistent state laws in certain circumstances. see, e. g., gibbons v. ogden, 22 u. s. ( 9 wheat. ) 1, 210 - 11, 6 l. ed. 23 ( 1824 ). the standard for determining whether the statute and regulations [ * * 11 ] now at issue are preempted by § 1 of the sherman act is set forth in the supreme court\\'s decision in rice v. norman williams co., 458 u. s. 654, 73 l. ed. 2d 1042, 102 s. ct. 3294 ( 1982 ). the supreme court in rice said : hn7 [ ] [ a ] state statute, when considered in the abstract, may be condemned under the antitrust laws only if it mandates or authorizes conduct that necessarily constitutes a violation of the antitrust laws in all cases, or if it places irresistible pressure on a private party to violate the antitrust laws in order to comply with the statute. such condemnation will follow', 'answer': 'hn6 [ ] in the federal system of government established by our constitution, the laws of the united states are the \" supreme law of the land, \" u. s. const. art. vi, and thus preempt and invalidate inconsistent state laws in certain circumstances. see, e. g., gibbons v. ogden, 22 u. s. ( 9 wheat. ) 1, 210 - 11, 6 l. ed. 23 ( 1824 ). the standard for determining whether the statute and regulations [ * * 11 ] now at issue are preempted by § 1 of the sherman act is set forth in the supreme court\\'s decision in rice v. norman williams co., 458 u. s. 654, 73 l. ed. 2d 1042, 102 s. ct. 3294 ( 1982 ). the supreme court in rice said : hn7 [ ] [ a ] state statute, when considered in the abstract, may be condemned under the antitrust laws only if it mandates or authorizes conduct that necessarily constitutes a violation of the antitrust laws in all cases, or if it places irresistible pressure on a private party to violate the antitrust laws in order to comply with the statute. such condemnation will follow'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': 'hn6 [ ] in the federal system of government established by our constitution, the laws of the united states are the \" supreme law of the land, \" u. s. const. art. vi, and thus preempt and invalidate inconsistent state laws in certain circumstances. see, e. g., gibbons v. ogden, 22 u. s. ( 9 wheat. ) 1, 210 - 11, 6 l. ed. 23 ( 1824 ). the standard for determining whether the statute and regulations [ * * 11 ] now at issue are preempted by § 1 of the sherman act is set forth in the supreme court\\'s decision in rice v. norman williams co., 458 u. s. 654, 73 l. ed. 2d 1042, 102 s. ct. 3294 ( 1982 ). the supreme court in rice said : hn7 [ ] [ a ] state statute, when considered in the abstract, may be condemned under the antitrust laws only if it mandates or authorizes conduct that necessarily constitutes a violation of the antitrust laws in all cases, or if it places irresistible pressure on a private party to violate the antitrust laws in order to comply with the statute. such condemnation will follow', 'answer': 'hn6 [ ] in the federal system of government established by our constitution, the laws of the united states are the \" supreme law of the land, \" u. s. const. art. vi, and thus preempt and invalidate inconsistent state laws in certain circumstances. see, e. g., gibbons v. ogden, 22 u. s. ( 9 wheat. ) 1, 210 - 11, 6 l. ed. 23 ( 1824 ). the standard for determining whether the statute and regulations [ * * 11 ] now at issue are preempted by § 1 of the sherman act is set forth in the supreme court\\'s decision in rice v. norman williams co., 458 u. s. 654, 73 l. ed. 2d 1042, 102 s. ct. 3294 ( 1982 ). the supreme court in rice said : hn7 [ ] [ a ] state statute, when considered in the abstract, may be condemned under the antitrust laws only if it mandates or authorizes conduct that necessarily constitutes a violation of the antitrust laws in all cases, or if it places irresistible pressure on a private party to violate the antitrust laws in order to comply with the statute. such condemnation will follow'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': \"under § 1 of the sherman act when the conduct contemplated by the statute is in all cases a per se violation. if the activity addressed by the statute does not fall into that category, and therefore must be analyzed under the rule of reason, the statute cannot be condemned in the abstract. analysis under the rule of reason requires an examination of the circumstances underlying a particular economic practice, and therefore does not lend itself to a conclusion that a statute is facially inconsistent with federal antitrust laws. 458 u. s. at 661. as i said earlier, at this point [ * * 12 ] the motions for summary judgment address only the plaintiffs'claim of a per se violation of § 1 and, therefore, the supreme court's formulation of the issue in the rice case applies here. the analysis involved in deciding the cross - motions for summary judgment requires two steps. first, i must determine whether the massachusetts regulatory scheme violates § 1 under the rice test. if so, second, i must determine whether the state has and exercises sufficient power to prevent anticompetitive conduct to provide that regulatory scheme antitrust immunity as state action pursuant to the supreme court's decision in parker, supra. each of these two steps also has two parts to the analysis. hn8 [ ]\", 'answer': \"under § 1 of the sherman act when the conduct contemplated by the statute is in all cases a per se violation. if the activity addressed by the statute does not fall into that category, and therefore must be analyzed under the rule of reason, the statute cannot be condemned in the abstract. analysis under the rule of reason requires an examination of the circumstances underlying a particular economic practice, and therefore does not lend itself to a conclusion that a statute is facially inconsistent with federal antitrust laws. 458 u. s. at 661. as i said earlier, at this point [ * * 12 ] the motions for summary judgment address only the plaintiffs'claim of a per se violation of § 1 and, therefore, the supreme court's formulation of the issue in the rice case applies here. the analysis involved in deciding the cross - motions for summary judgment requires two steps. first, i must determine whether the massachusetts regulatory scheme violates § 1 under the rice test. if so, second, i must determine whether the state has and exercises sufficient power to prevent anticompetitive conduct to provide that regulatory scheme antitrust immunity as state action pursuant to the supreme court's decision in parker, supra. each of these two steps also has two parts to the analysis. hn8 [ ]\"}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': \"under § 1 of the sherman act when the conduct contemplated by the statute is in all cases a per se violation. if the activity addressed by the statute does not fall into that category, and therefore must be analyzed under the rule of reason, the statute cannot be condemned in the abstract. analysis under the rule of reason requires an examination of the circumstances underlying a particular economic practice, and therefore does not lend itself to a conclusion that a statute is facially inconsistent with federal antitrust laws. 458 u. s. at 661. as i said earlier, at this point [ * * 12 ] the motions for summary judgment address only the plaintiffs'claim of a per se violation of § 1 and, therefore, the supreme court's formulation of the issue in the rice case applies here. the analysis involved in deciding the cross - motions for summary judgment requires two steps. first, i must determine whether the massachusetts regulatory scheme violates § 1 under the rice test. if so, second, i must determine whether the state has and exercises sufficient power to prevent anticompetitive conduct to provide that regulatory scheme antitrust immunity as state action pursuant to the supreme court's decision in parker, supra. each of these two steps also has two parts to the analysis. hn8 [ ]\", 'answer': \"under § 1 of the sherman act when the conduct contemplated by the statute is in all cases a per se violation. if the activity addressed by the statute does not fall into that category, and therefore must be analyzed under the rule of reason, the statute cannot be condemned in the abstract. analysis under the rule of reason requires an examination of the circumstances underlying a particular economic practice, and therefore does not lend itself to a conclusion that a statute is facially inconsistent with federal antitrust laws. 458 u. s. at 661. as i said earlier, at this point [ * * 12 ] the motions for summary judgment address only the plaintiffs'claim of a per se violation of § 1 and, therefore, the supreme court's formulation of the issue in the rice case applies here. the analysis involved in deciding the cross - motions for summary judgment requires two steps. first, i must determine whether the massachusetts regulatory scheme violates § 1 under the rice test. if so, second, i must determine whether the state has and exercises sufficient power to prevent anticompetitive conduct to provide that regulatory scheme antitrust immunity as state action pursuant to the supreme court's decision in parker, supra. each of these two steps also has two parts to the analysis. hn8 [ ]\"}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': 'with regard to a possible per se violation of § 1, i must in this case first decide if the restraint on trade at issue is what the supreme court has called a \" unilateral restraint \" or a \" hybrid restraint. \" fisher v. city of berkeley, 475 u. s. 260, 267 - 68, 89 l. ed. 2d 206, 106 s. ct. 1045 ( 1986 ). second, if it is a \" hybrid restraint, \" i must decide if it involves a per se violation of § 1. hn9 [ ] with regard to possible immunity, i must decide, first, whether there is a clearly [ * * 13 ] articulated and affirmatively expressed state policy that the restraint at issue is intended to serve. california retail liquor dealers ass\\'n v. midcal aluminum, inc., 445 u. s. 97, 105, 63 l. ed. 2d 233, 100 s. ct. 937 ( 1980 ) ( citing city of lafayette v. louisiana power & light co., 435 u. s. 389, 410, 55 l. ed. 2d 364, 98 s. ct. 1123 ( 1978 ) ). if so, second, i must decide whether that policy', 'answer': 'with regard to a possible per se violation of § 1, i must in this case first decide if the restraint on trade at issue is what the supreme court has called a \" unilateral restraint \" or a \" hybrid restraint. \" fisher v. city of berkeley, 475 u. s. 260, 267 - 68, 89 l. ed. 2d 206, 106 s. ct. 1045 ( 1986 ). second, if it is a \" hybrid restraint, \" i must decide if it involves a per se violation of § 1. hn9 [ ] with regard to possible immunity, i must decide, first, whether there is a clearly [ * * 13 ] articulated and affirmatively expressed state policy that the restraint at issue is intended to serve. california retail liquor dealers ass\\'n v. midcal aluminum, inc., 445 u. s. 97, 105, 63 l. ed. 2d 233, 100 s. ct. 937 ( 1980 ) ( citing city of lafayette v. louisiana power & light co., 435 u. s. 389, 410, 55 l. ed. 2d 364, 98 s. ct. 1123 ( 1978 ) ). if so, second, i must decide whether that policy'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': 'with regard to a possible per se violation of § 1, i must in this case first decide if the restraint on trade at issue is what the supreme court has called a \" unilateral restraint \" or a \" hybrid restraint. \" fisher v. city of berkeley, 475 u. s. 260, 267 - 68, 89 l. ed. 2d 206, 106 s. ct. 1045 ( 1986 ). second, if it is a \" hybrid restraint, \" i must decide if it involves a per se violation of § 1. hn9 [ ] with regard to possible immunity, i must decide, first, whether there is a clearly [ * * 13 ] articulated and affirmatively expressed state policy that the restraint at issue is intended to serve. california retail liquor dealers ass\\'n v. midcal aluminum, inc., 445 u. s. 97, 105, 63 l. ed. 2d 233, 100 s. ct. 937 ( 1980 ) ( citing city of lafayette v. louisiana power & light co., 435 u. s. 389, 410, 55 l. ed. 2d 364, 98 s. ct. 1123 ( 1978 ) ). if so, second, i must decide whether that policy', 'answer': 'with regard to a possible per se violation of § 1, i must in this case first decide if the restraint on trade at issue is what the supreme court has called a \" unilateral restraint \" or a \" hybrid restraint. \" fisher v. city of berkeley, 475 u. s. 260, 267 - 68, 89 l. ed. 2d 206, 106 s. ct. 1045 ( 1986 ). second, if it is a \" hybrid restraint, \" i must decide if it involves a per se violation of § 1. hn9 [ ] with regard to possible immunity, i must decide, first, whether there is a clearly [ * * 13 ] articulated and affirmatively expressed state policy that the restraint at issue is intended to serve. california retail liquor dealers ass\\'n v. midcal aluminum, inc., 445 u. s. 97, 105, 63 l. ed. 2d 233, 100 s. ct. 937 ( 1980 ) ( citing city of lafayette v. louisiana power & light co., 435 u. s. 389, 410, 55 l. ed. 2d 364, 98 s. ct. 1123 ( 1978 ) ). if so, second, i must decide whether that policy'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': 'is actively supervised by the state itself id. as i will soon describe, i find that this case involves a \" hybrid restraint. \" furthermore, i find that the requirements that prices be announced, not be changed except to meet a competitor\\'s lower announced price, and be maintained for a month effect a per se violation of § 1. in addition, i find that there is no state action immunity for this regulatory scheme because, while massachusetts has a clearly articulated policy concerning what the restraint on trade is intended to serve, the [ * 46 ] state does not have the power to attempt to assure that the resulting prices are reasonable. rather, the commission merely enforces a per se violation of § 1, and this conduct is insufficient to generate state action immunity. the reasons for [ * * 14 ] these conclusions are as follows. first, as i said, the massachusetts regulatory scheme creates a \" hybrid restraint \" as defined by the supreme court in its 1986 decision in fisher. in fisher, the supreme court found a city rent control ordinance was not preempted by § 1 because the city unilaterally established the maximum rents that could be charged in berkeley, california. 475 u. s. at 267. the supreme court went on to say, however : 16 f. supp', 'answer': 'is actively supervised by the state itself id. as i will soon describe, i find that this case involves a \" hybrid restraint. \" furthermore, i find that the requirements that prices be announced, not be changed except to meet a competitor\\'s lower announced price, and be maintained for a month effect a per se violation of § 1. in addition, i find that there is no state action immunity for this regulatory scheme because, while massachusetts has a clearly articulated policy concerning what the restraint on trade is intended to serve, the [ * 46 ] state does not have the power to attempt to assure that the resulting prices are reasonable. rather, the commission merely enforces a per se violation of § 1, and this conduct is insufficient to generate state action immunity. the reasons for [ * * 14 ] these conclusions are as follows. first, as i said, the massachusetts regulatory scheme creates a \" hybrid restraint \" as defined by the supreme court in its 1986 decision in fisher. in fisher, the supreme court found a city rent control ordinance was not preempted by § 1 because the city unilaterally established the maximum rents that could be charged in berkeley, california. 475 u. s. at 267. the supreme court went on to say, however : 16 f. supp'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': 'is actively supervised by the state itself id. as i will soon describe, i find that this case involves a \" hybrid restraint. \" furthermore, i find that the requirements that prices be announced, not be changed except to meet a competitor\\'s lower announced price, and be maintained for a month effect a per se violation of § 1. in addition, i find that there is no state action immunity for this regulatory scheme because, while massachusetts has a clearly articulated policy concerning what the restraint on trade is intended to serve, the [ * 46 ] state does not have the power to attempt to assure that the resulting prices are reasonable. rather, the commission merely enforces a per se violation of § 1, and this conduct is insufficient to generate state action immunity. the reasons for [ * * 14 ] these conclusions are as follows. first, as i said, the massachusetts regulatory scheme creates a \" hybrid restraint \" as defined by the supreme court in its 1986 decision in fisher. in fisher, the supreme court found a city rent control ordinance was not preempted by § 1 because the city unilaterally established the maximum rents that could be charged in berkeley, california. 475 u. s. at 267. the supreme court went on to say, however : 16 f. supp', 'answer': 'is actively supervised by the state itself id. as i will soon describe, i find that this case involves a \" hybrid restraint. \" furthermore, i find that the requirements that prices be announced, not be changed except to meet a competitor\\'s lower announced price, and be maintained for a month effect a per se violation of § 1. in addition, i find that there is no state action immunity for this regulatory scheme because, while massachusetts has a clearly articulated policy concerning what the restraint on trade is intended to serve, the [ * 46 ] state does not have the power to attempt to assure that the resulting prices are reasonable. rather, the commission merely enforces a per se violation of § 1, and this conduct is insufficient to generate state action immunity. the reasons for [ * * 14 ] these conclusions are as follows. first, as i said, the massachusetts regulatory scheme creates a \" hybrid restraint \" as defined by the supreme court in its 1986 decision in fisher. in fisher, the supreme court found a city rent control ordinance was not preempted by § 1 because the city unilaterally established the maximum rents that could be charged in berkeley, california. 475 u. s. at 267. the supreme court went on to say, however : 16 f. supp'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': '. 2d 41, * 45 ; 1998 u. s. dist. lexis 1155, * * 10', 'answer': '. 2d 41, * 45 ; 1998 u. s. dist. lexis 1155, * * 10'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': '. 2d 41, * 45 ; 1998 u. s. dist. lexis 1155, * * 10', 'answer': '. 2d 41, * 45 ; 1998 u. s. dist. lexis 1155, * * 10'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': 'page 10 of 15', 'answer': 'page 10 of 15'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': 'page 10 of 15', 'answer': 'page 10 of 15'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': 'hn10 [ ] not all restraints imposed upon private actors by government units necessarily constitute unilateral action outside the purview of § 1. certain restraints may be characterized as \" hybrid, \" in that nonmarket mechanisms merely enforce private marketing decisions. see rice v. norman williams co., 458 u. s. at 665 ( stevens, j., concurring in judgment ). where private actors are thus granted \" a degree of private regulatory power, \" id. at 666 n. 1, the regulatory scheme may be attacked under § 1. 475 u. s. at 267 - 68. the supreme court has characterized its decisions in schwegmann bros. v. calvert distillers corp., 341 u. s. 384, 95 l. ed. 1035, 71 s. ct. 745 ( 1951 ), midcal, supra, and 324 liquor corp. v. duffy, [ * * 15 ] 479 u. s. 335, 93 l. ed. 2d 667, 107 s. ct. 720 ( 1986 ), as involving \" hybrid restraints. \" each of those cases involved the liquor industry and some form of retail price maintenance. schwegmann ; 341 u. s. at 386', 'answer': 'hn10 [ ] not all restraints imposed upon private actors by government units necessarily constitute unilateral action outside the purview of § 1. certain restraints may be characterized as \" hybrid, \" in that nonmarket mechanisms merely enforce private marketing decisions. see rice v. norman williams co., 458 u. s. at 665 ( stevens, j., concurring in judgment ). where private actors are thus granted \" a degree of private regulatory power, \" id. at 666 n. 1, the regulatory scheme may be attacked under § 1. 475 u. s. at 267 - 68. the supreme court has characterized its decisions in schwegmann bros. v. calvert distillers corp., 341 u. s. 384, 95 l. ed. 1035, 71 s. ct. 745 ( 1951 ), midcal, supra, and 324 liquor corp. v. duffy, [ * * 15 ] 479 u. s. 335, 93 l. ed. 2d 667, 107 s. ct. 720 ( 1986 ), as involving \" hybrid restraints. \" each of those cases involved the liquor industry and some form of retail price maintenance. schwegmann ; 341 u. s. at 386'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': 'hn10 [ ] not all restraints imposed upon private actors by government units necessarily constitute unilateral action outside the purview of § 1. certain restraints may be characterized as \" hybrid, \" in that nonmarket mechanisms merely enforce private marketing decisions. see rice v. norman williams co., 458 u. s. at 665 ( stevens, j., concurring in judgment ). where private actors are thus granted \" a degree of private regulatory power, \" id. at 666 n. 1, the regulatory scheme may be attacked under § 1. 475 u. s. at 267 - 68. the supreme court has characterized its decisions in schwegmann bros. v. calvert distillers corp., 341 u. s. 384, 95 l. ed. 1035, 71 s. ct. 745 ( 1951 ), midcal, supra, and 324 liquor corp. v. duffy, [ * * 15 ] 479 u. s. 335, 93 l. ed. 2d 667, 107 s. ct. 720 ( 1986 ), as involving \" hybrid restraints. \" each of those cases involved the liquor industry and some form of retail price maintenance. schwegmann ; 341 u. s. at 386', 'answer': 'hn10 [ ] not all restraints imposed upon private actors by government units necessarily constitute unilateral action outside the purview of § 1. certain restraints may be characterized as \" hybrid, \" in that nonmarket mechanisms merely enforce private marketing decisions. see rice v. norman williams co., 458 u. s. at 665 ( stevens, j., concurring in judgment ). where private actors are thus granted \" a degree of private regulatory power, \" id. at 666 n. 1, the regulatory scheme may be attacked under § 1. 475 u. s. at 267 - 68. the supreme court has characterized its decisions in schwegmann bros. v. calvert distillers corp., 341 u. s. 384, 95 l. ed. 1035, 71 s. ct. 745 ( 1951 ), midcal, supra, and 324 liquor corp. v. duffy, [ * * 15 ] 479 u. s. 335, 93 l. ed. 2d 667, 107 s. ct. 720 ( 1986 ), as involving \" hybrid restraints. \" each of those cases involved the liquor industry and some form of retail price maintenance. schwegmann ; 341 u. s. at 386'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': '; midcal, 445 u. s. at 103 ; duffy, 479 u. s. at 341. all involved the state authorizing price setting and enforcing the price established by private parties. schwegmann, 341 u. s. at 387 ; midcal, 445 u. s. at 105 ; duffy 479 u. s. at 341. in its 1987 decision in miller, after examining schwegmann and midcal, the ninth circuit characterized a regulatory scheme relating to wholesale liquor prices that was comparable to the scheme at issue in the instant case as a \" hybrid restraint \" because it allowed private parties to set prices and the state did not review the reasonableness of them ; rather, the regulatory scheme merely enforced private pricing decisions. 813 f. 2d at 1350 - 51. implicit in the miller decision is the understanding that there may be a \" hybrid restraint \" concerning wholesale liquor prices as well as concerning retail liquor prices. i agree and, essentially for the reasons stated by the ninth circuit, find that this case involves a [ * * 16 ] \" hybrid restraint. \" id. at 1351. 6 in addition, i find that the massachusetts regulatory scheme effects a per se violation of § 1. it is important to recognize, however, that', 'answer': '; midcal, 445 u. s. at 103 ; duffy, 479 u. s. at 341. all involved the state authorizing price setting and enforcing the price established by private parties. schwegmann, 341 u. s. at 387 ; midcal, 445 u. s. at 105 ; duffy 479 u. s. at 341. in its 1987 decision in miller, after examining schwegmann and midcal, the ninth circuit characterized a regulatory scheme relating to wholesale liquor prices that was comparable to the scheme at issue in the instant case as a \" hybrid restraint \" because it allowed private parties to set prices and the state did not review the reasonableness of them ; rather, the regulatory scheme merely enforced private pricing decisions. 813 f. 2d at 1350 - 51. implicit in the miller decision is the understanding that there may be a \" hybrid restraint \" concerning wholesale liquor prices as well as concerning retail liquor prices. i agree and, essentially for the reasons stated by the ninth circuit, find that this case involves a [ * * 16 ] \" hybrid restraint. \" id. at 1351. 6 in addition, i find that the massachusetts regulatory scheme effects a per se violation of § 1. it is important to recognize, however, that'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': '; midcal, 445 u. s. at 103 ; duffy, 479 u. s. at 341. all involved the state authorizing price setting and enforcing the price established by private parties. schwegmann, 341 u. s. at 387 ; midcal, 445 u. s. at 105 ; duffy 479 u. s. at 341. in its 1987 decision in miller, after examining schwegmann and midcal, the ninth circuit characterized a regulatory scheme relating to wholesale liquor prices that was comparable to the scheme at issue in the instant case as a \" hybrid restraint \" because it allowed private parties to set prices and the state did not review the reasonableness of them ; rather, the regulatory scheme merely enforced private pricing decisions. 813 f. 2d at 1350 - 51. implicit in the miller decision is the understanding that there may be a \" hybrid restraint \" concerning wholesale liquor prices as well as concerning retail liquor prices. i agree and, essentially for the reasons stated by the ninth circuit, find that this case involves a [ * * 16 ] \" hybrid restraint. \" id. at 1351. 6 in addition, i find that the massachusetts regulatory scheme effects a per se violation of § 1. it is important to recognize, however, that', 'answer': '; midcal, 445 u. s. at 103 ; duffy, 479 u. s. at 341. all involved the state authorizing price setting and enforcing the price established by private parties. schwegmann, 341 u. s. at 387 ; midcal, 445 u. s. at 105 ; duffy 479 u. s. at 341. in its 1987 decision in miller, after examining schwegmann and midcal, the ninth circuit characterized a regulatory scheme relating to wholesale liquor prices that was comparable to the scheme at issue in the instant case as a \" hybrid restraint \" because it allowed private parties to set prices and the state did not review the reasonableness of them ; rather, the regulatory scheme merely enforced private pricing decisions. 813 f. 2d at 1350 - 51. implicit in the miller decision is the understanding that there may be a \" hybrid restraint \" concerning wholesale liquor prices as well as concerning retail liquor prices. i agree and, essentially for the reasons stated by the ninth circuit, find that this case involves a [ * * 16 ] \" hybrid restraint. \" id. at 1351. 6 in addition, i find that the massachusetts regulatory scheme effects a per se violation of § 1. it is important to recognize, however, that'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': 'hn11 [ ] a \" hybrid restraint \" does not necessarily involve any violation of § 1, let alone a per se violation. it depends upon the particular facts of the case. hertz corp. v. city of new york, 1 f. 3d 121, 129 ( 2d cir. 1993 ), cert. denied, 510 u. s. 1111, 127 l. ed. 2d 375, 114 s. ct. 1054, 114 s. ct. 1055 ( 1994 ). the supreme court has defined a per se violation most notably in northern pacific ry. co., 365 u. s. 1, 5, 81 s. ct. 435, 5 l. ed. 2d 377, stating that hn12 [ ] \" there are certain agreements or practices which because of their pernicious effect on competition and lack of any redeeming virtue are conclusively presumed [ * * 17 ] to be unreasonable and therefore illegal without elaborate inquiry as to the precise harm they have caused or the business excuse for their use. \" these agreements or practices are deemed unlawful per se \" without further examination under the rule of reason generally applied in sherman act cases. \" broadcast music, inc., 441 u. s. at 8. the ninth', 'answer': 'hn11 [ ] a \" hybrid restraint \" does not necessarily involve any violation of § 1, let alone a per se violation. it depends upon the particular facts of the case. hertz corp. v. city of new york, 1 f. 3d 121, 129 ( 2d cir. 1993 ), cert. denied, 510 u. s. 1111, 127 l. ed. 2d 375, 114 s. ct. 1054, 114 s. ct. 1055 ( 1994 ). the supreme court has defined a per se violation most notably in northern pacific ry. co., 365 u. s. 1, 5, 81 s. ct. 435, 5 l. ed. 2d 377, stating that hn12 [ ] \" there are certain agreements or practices which because of their pernicious effect on competition and lack of any redeeming virtue are conclusively presumed [ * * 17 ] to be unreasonable and therefore illegal without elaborate inquiry as to the precise harm they have caused or the business excuse for their use. \" these agreements or practices are deemed unlawful per se \" without further examination under the rule of reason generally applied in sherman act cases. \" broadcast music, inc., 441 u. s. at 8. the ninth'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': 'hn11 [ ] a \" hybrid restraint \" does not necessarily involve any violation of § 1, let alone a per se violation. it depends upon the particular facts of the case. hertz corp. v. city of new york, 1 f. 3d 121, 129 ( 2d cir. 1993 ), cert. denied, 510 u. s. 1111, 127 l. ed. 2d 375, 114 s. ct. 1054, 114 s. ct. 1055 ( 1994 ). the supreme court has defined a per se violation most notably in northern pacific ry. co., 365 u. s. 1, 5, 81 s. ct. 435, 5 l. ed. 2d 377, stating that hn12 [ ] \" there are certain agreements or practices which because of their pernicious effect on competition and lack of any redeeming virtue are conclusively presumed [ * * 17 ] to be unreasonable and therefore illegal without elaborate inquiry as to the precise harm they have caused or the business excuse for their use. \" these agreements or practices are deemed unlawful per se \" without further examination under the rule of reason generally applied in sherman act cases. \" broadcast music, inc., 441 u. s. at 8. the ninth', 'answer': 'hn11 [ ] a \" hybrid restraint \" does not necessarily involve any violation of § 1, let alone a per se violation. it depends upon the particular facts of the case. hertz corp. v. city of new york, 1 f. 3d 121, 129 ( 2d cir. 1993 ), cert. denied, 510 u. s. 1111, 127 l. ed. 2d 375, 114 s. ct. 1054, 114 s. ct. 1055 ( 1994 ). the supreme court has defined a per se violation most notably in northern pacific ry. co., 365 u. s. 1, 5, 81 s. ct. 435, 5 l. ed. 2d 377, stating that hn12 [ ] \" there are certain agreements or practices which because of their pernicious effect on competition and lack of any redeeming virtue are conclusively presumed [ * * 17 ] to be unreasonable and therefore illegal without elaborate inquiry as to the precise harm they have caused or the business excuse for their use. \" these agreements or practices are deemed unlawful per se \" without further examination under the rule of reason generally applied in sherman act cases. \" broadcast music, inc., 441 u. s. at 8. the ninth'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': 'circuit in miller and judge winter in his dissent in battipaglia deemed the regulatory schemes at issue in each of [ * 47 ] those cases to be a per se violation of § 1. they relied on statements of the supreme court in reaching their respective conclusions. i am persuaded by the reasoning and statements of the supreme court to concur with the ninth circuit and judge winter in this case. contrary to plaintiffs\\'contention, however, i do not find that merely exchanging price information, as the massachusetts regulatory scheme in effect requires, is a per se violation, despite certain language in united states v. container corp., 393 u. s. 333, 21 l. ed. 2d 526, 89 s. ct. 510 ( 1969 ). there, the supreme court ruled that the result of a \" reciprocal exchange of prices was to stabilize prices though at a downward level. \" 393 u. s. [ * * 18 ] at 336. the court held that \" the limitation or reduction of price competition \" resulting in this stabilization in prices \" brings the case within the ban [ of the sherman act ], for interference with the setting of price by free market forces is unlawful per se. \" 393 u. s. at 337 ( citation omitted ). justice', 'answer': 'circuit in miller and judge winter in his dissent in battipaglia deemed the regulatory schemes at issue in each of [ * 47 ] those cases to be a per se violation of § 1. they relied on statements of the supreme court in reaching their respective conclusions. i am persuaded by the reasoning and statements of the supreme court to concur with the ninth circuit and judge winter in this case. contrary to plaintiffs\\'contention, however, i do not find that merely exchanging price information, as the massachusetts regulatory scheme in effect requires, is a per se violation, despite certain language in united states v. container corp., 393 u. s. 333, 21 l. ed. 2d 526, 89 s. ct. 510 ( 1969 ). there, the supreme court ruled that the result of a \" reciprocal exchange of prices was to stabilize prices though at a downward level. \" 393 u. s. [ * * 18 ] at 336. the court held that \" the limitation or reduction of price competition \" resulting in this stabilization in prices \" brings the case within the ban [ of the sherman act ], for interference with the setting of price by free market forces is unlawful per se. \" 393 u. s. at 337 ( citation omitted ). justice'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': 'circuit in miller and judge winter in his dissent in battipaglia deemed the regulatory schemes at issue in each of [ * 47 ] those cases to be a per se violation of § 1. they relied on statements of the supreme court in reaching their respective conclusions. i am persuaded by the reasoning and statements of the supreme court to concur with the ninth circuit and judge winter in this case. contrary to plaintiffs\\'contention, however, i do not find that merely exchanging price information, as the massachusetts regulatory scheme in effect requires, is a per se violation, despite certain language in united states v. container corp., 393 u. s. 333, 21 l. ed. 2d 526, 89 s. ct. 510 ( 1969 ). there, the supreme court ruled that the result of a \" reciprocal exchange of prices was to stabilize prices though at a downward level. \" 393 u. s. [ * * 18 ] at 336. the court held that \" the limitation or reduction of price competition \" resulting in this stabilization in prices \" brings the case within the ban [ of the sherman act ], for interference with the setting of price by free market forces is unlawful per se. \" 393 u. s. at 337 ( citation omitted ). justice', 'answer': 'circuit in miller and judge winter in his dissent in battipaglia deemed the regulatory schemes at issue in each of [ * 47 ] those cases to be a per se violation of § 1. they relied on statements of the supreme court in reaching their respective conclusions. i am persuaded by the reasoning and statements of the supreme court to concur with the ninth circuit and judge winter in this case. contrary to plaintiffs\\'contention, however, i do not find that merely exchanging price information, as the massachusetts regulatory scheme in effect requires, is a per se violation, despite certain language in united states v. container corp., 393 u. s. 333, 21 l. ed. 2d 526, 89 s. ct. 510 ( 1969 ). there, the supreme court ruled that the result of a \" reciprocal exchange of prices was to stabilize prices though at a downward level. \" 393 u. s. [ * * 18 ] at 336. the court held that \" the limitation or reduction of price competition \" resulting in this stabilization in prices \" brings the case within the ban [ of the sherman act ], for interference with the setting of price by free market forces is unlawful per se. \" 393 u. s. at 337 ( citation omitted ). justice'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': 'fortas, however, wrote in a concurrence to container corp. that, \" i do not understand the court\\'s opinion to hold that the exchange of specific information 16 f. supp. 2d 41, * 46 ; 1998 u. s. dist. lexis 1155, * * 14', 'answer': 'fortas, however, wrote in a concurrence to container corp. that, \" i do not understand the court\\'s opinion to hold that the exchange of specific information 16 f. supp. 2d 41, * 46 ; 1998 u. s. dist. lexis 1155, * * 14'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': 'fortas, however, wrote in a concurrence to container corp. that, \" i do not understand the court\\'s opinion to hold that the exchange of specific information 16 f. supp. 2d 41, * 46 ; 1998 u. s. dist. lexis 1155, * * 14', 'answer': 'fortas, however, wrote in a concurrence to container corp. that, \" i do not understand the court\\'s opinion to hold that the exchange of specific information 16 f. supp. 2d 41, * 46 ; 1998 u. s. dist. lexis 1155, * * 14'}, {'question': \"What is the judge's opinion regarding entry barriers in case 1?\", 'context': 'page 11 of 15', 'answer': 'page 11 of 15'}, {'question': \"What is the prosecutor's argument regarding entry barriers in case 1?\", 'context': 'page 11 of 15', 'answer': 'page 11 of 15'}]\n"
     ]
    }
   ],
   "source": [
    "# Display a sample of the generated Q&A pairs\n",
    "print(train_data[:50])  # Show first 5 entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 檢查裁切文本長度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 檢查段落的長度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length: 784 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   595,  1314,  3688,   184,   365,   469,  6047,   588,\n",
      "          380,   403,  2486,   604,   243,   210, 10958,   178,   115,  3587,\n",
      "          111,   380,   403,   112,   212,  1285,   569,   239,  2043,   211,\n",
      "         6410,   145,   388,   217,   604,   771,   565,   359,   854,   211,\n",
      "          145,   993,  5471,   213,   229,   595,   545,   145,   423,   126])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 199, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4639 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   606,   595,   212,   380,   403,   678,  1273, 14993,  3883,\n",
      "          117,   207,   729,   240,  1463,   380,   403,   110,   163,   867,\n",
      "          217,   371,   911,   207,  3925,   212,   595,   110,   163,   867,\n",
      "          217,   145,   350,   729,   115,   310,  4124,   301,   614,   380,\n",
      "          403,   145,   350,   729,   621,   595,   878,   211,  2059,   207])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 200, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3982 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   238,   207,   663,   373,   207,  2753,   117, 21439,  1612,\n",
      "          237,  3927,  7827,   235,   207,  5471,   238,   207,   663,   111,\n",
      "          394,   205,   112,   222,   760,   404,   117,   226,  3927,  5271,\n",
      "          211,   219,   440,   221,   210,   463,   198,   115,  1167,   117,\n",
      "          213,  2912,  1154,   115,  5025,  4876,   222,   595,   110,   163])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5165 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  207,  759,  110,  163, 1567, 2481, 1637, 4162,  212, 5427, 1668,\n",
      "        1322,  242,  316,  212, 4866,  189,  473,  248, 3454, 3384,  149,  190,\n",
      "        2830,  117,  351,  380,  403,  212,  145,  179,  171,  173,  217,  105,\n",
      "        5690,  213, 3427, 1016,  105,  213,  207,  105, 3121,  126,  210,  595,\n",
      "         110,  163])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4870 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  4392,   217,   237,  2594,   110,   163,  3399,  2618, 17735,\n",
      "         1485,   126,   113,   368,   128,   215,   681,   210,   207,  1999,\n",
      "          210,   667,  1270,  2666,   213,   207,   663,   117,   117,   117,\n",
      "          117,   809,   115,   117,   117,   117,   346,   266,   470,  1253,\n",
      "          221,   145,  1685,   384,   421,   222,   229,   237,  2498,   245])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4450 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   372,   207,  1336,  2591,   239,   629,  3925,   115,   207,\n",
      "          240,  3124,   207, 10237,   195,   213,   207,  1601,   225,   661,\n",
      "          212,  4048,  4276,   145,   179,   171,   173,   105,   223,   227,\n",
      "         1311,   217,  2067,   215,  6109,  1346,   117,   105,   595,  2042,\n",
      "          207,   729,   240,  4342,   218,  9018,   235,   207,  1336,   110])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4647 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   663,   115,   233,   587,   216,   207,  2594,   305,   219,\n",
      "         1311,   213,   145,  3593,   347,   217,   216,  1271,   117,   207,\n",
      "         1194,   110,   163,   686,   244,   237,   636,   826,   116,   949,\n",
      "          116, 12297,   301,  1351,   218,   207,  3593,   117,   126,  5377,\n",
      "          117,   128,   105,   111,   126,   113,   478,   128,   447,   117])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4226 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   750,   588,   211,  1025,   207,  4335,   291,   595,   223,\n",
      "         2041,   238,   700,   215,   502,   117,   105,   111,  2811,   235,\n",
      "          166,   117,  3489, 27080,   235,   535,   117,   115,  2559,   115,\n",
      "          206,  3000,   117,  2153,   117,   201,   475,   236,   160,   117,\n",
      "         8232,   117,   112,  2811,   235,  2494,   239,   525,   221,   587])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4210 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1309,   115,   145,   257,   356, 10899,  1150,  3780,  1762,\n",
      "          217,   207,   376,  4885,   343,   857,   215,  6959, 21410,   189,\n",
      "          244,  2258,   212,  4286,  1311,   217,   207,   376,  4885,   117,\n",
      "          111,   203,  9390,  5551,   115,  1152,   210,  3000,   117,   266,\n",
      "          115,  2559,   115, 12931,   115,   142, 15450,   115,   236,   160])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4800 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   372,   795,  3293,   210,  2498,   189,   110,   470,   225,\n",
      "         3612,   253,   220,  2557,   115,   335,  3557,   885,  1045,   217,\n",
      "          207,   266,   211, 24192,   270,   872,   565,   126,   113,   875,\n",
      "          128,   324,   470,   211,   207,   469,   117,   105,   111,   447,\n",
      "          117,   236,   160,   186,   117,  6776,   116,  9772,   117,   112])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4832 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1063,   174,   117,   111,   146,   112,   120,  4595,   175,\n",
      "        19854,   191,   577,   166,   117,   310,  2864,  2198,   535,   117,\n",
      "          111,  1751,   112,   774,  3000,   117,   200,   174,  5846,   115,\n",
      "         7144,   116,  6002,   115,  5092,  3000,   117, 10900,   117,  3978,\n",
      "          115,  9008,   160,   117,   199,   174,  8788,   120,   126,   113])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5020 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5158,   540,   115,   521,   117,   115,  2559,   115,  7950,\n",
      "          165,   117,   163,   117,   236,   160,   117,  7294,   112,   115,\n",
      "          145,  4624,   240,   311,  1262,   105,   584,  4990, 11418,   189,\n",
      "          105,   119,   105,   207,  2030,   210, 13563, 20012, 10382,   126,\n",
      "          210,   207, 27610,   110,   163,   818,   128,   120,   207, 12610])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2988 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  117,  117,  513,  117,  117,  117,  213,  117,  117,  117, 3281,\n",
      "         211,  126,  128,  145,  388,  105,  217,  604,  771,  115,  215,  211,\n",
      "         105, 6109,  115,  215,  117,  117,  117, 2827,  211, 2097,  207, 1616,\n",
      "         210,  126,  128,  237,  377,  216, 1049,  189,  220,  322,  110,  163,\n",
      "         914,  215])\n",
      "Original length: 31 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   634,   165,   117,   163,   117,  6394,   117, 13648,   189,\n",
      "         5089,  3567,   115,   113,   198,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 200, 210, 203, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3784 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   308,   212,   528,   222,   557,   441,   115,   634,\n",
      "          115,   226,   240,  2752,  1744,  1151,   222,  3139,   184,   158,\n",
      "          195,  8900,   110,   163,   111,   105,   595,   105,   112,   867,\n",
      "          217,  3403,   242,   508,   165,   117,   163,   117,   147,   117,\n",
      "          142, 12002,   351,   661,   217,  9024,   194,   243,   115,   521])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 203, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4733 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   508,   165,   117,   163,   117,   147,   117,   142, 12002,\n",
      "          126,   113,   202,   128,   111,  1631,   117,  1077,   112,   117,\n",
      "          152,   184,   973,   126,   128,   207,   908,   210,  3403,   242,\n",
      "          248, 12002,   223,   213,   207,  1045,   210,   207,   483,   240,\n",
      "          117,   321, 29815,   178,  1181,   166,   117, 20981, 25045,   115])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 203, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4701 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  233, 1763,  216,  207,  396, 3757,  246,  227,  221, 9365,  221,\n",
      "         207,  914, 3757,  217, 1298,  853,  117,  296,  115,  207,  396, 3757,\n",
      "        3503, 2581,  210, 5859,  266,  351,  207,  595,  307,  212,  227,  207,\n",
      "         595,  110,  163, 1883,  117,  934,  226,  240,  223,  227, 3644,  211,\n",
      "         865,  207])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 203, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1974 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1700,   115,   207,   595,  2042,   216,   207,   482,   216,\n",
      "          207,   894,  3813,  8743,   392,  3757,   189,   367,   211,   145,\n",
      "          886,   373,   207,   240,   115,   216,   335,   247, 16992,  6919,\n",
      "          216,   392,  3757,   189,   244, 28342,   117,   390,   115,   207,\n",
      "          240, 12259,   225,   226,  4040,   117,   207,  1313,   240,   249])\n",
      "Original length: 54 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  4602,   150,   117,   162,   117,   148,   117,  4935,   115,\n",
      "          113,  4935,   120,   634,   165,   117,   163,   117,  6394,   117,\n",
      "        13648,   189,  5727,  9075,   115,   113,   113,   198,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3299 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  5502,   128,  2231,   528,   212,   308,\n",
      "          373,   207,   240,   223,   820,   110,   212,   588, 15123,   175,\n",
      "          171,  1381,   110,   163,   111, 15123,   175,   171,   112,  3071,\n",
      "          857,   867,   217,  1366,   672,   210,   954,   906,   317,   820,\n",
      "          212, 15123,   175,   171,   111,  3192,   117,   230,   117,  8706])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4167 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113,  113,  204,  128,  207,  634,  906,  232,  249,  255,\n",
      "         957,  218, 1761,  820,  212,  207,  954,  687, 1276,  238,  226,  347,\n",
      "         117,  207,  634,  906,  110,  163, 1654,  223, 4124,  222,  207,  672,\n",
      "         210,  207,  906,  232,  218,  207,  584,  986,  225,  585,  569,  207,\n",
      "         470,  210])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3418 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   954,   906,   687,   117,   296,   115,   820,  1968,  2122,\n",
      "          210,   145,   105, 12521,   171,   187,   687,   105,   216,   297,\n",
      "          698,   115,   105,   241,   652,   443,  1536,   215,   378,  1405,\n",
      "          315,   210,   207,   346,   320,   210, 10197,   116,   168,  6398,\n",
      "         1392,   117,   222,   207, 12521,   171,   187,   477,   238,   979])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4328 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  2450,   126,   128,   207,  1245,   525,   516,\n",
      "          111,   145,   112, 17002,   698,   119,   111,   198,   112, 11961,\n",
      "          189,   658,   120,   111,   199,   112,   346, 13148,   120,   111,\n",
      "          200,   112,  4596,   658,   120,   212,   111,   201,   112, 15198,\n",
      "        12827,   173,   195,   210,  1555,   117, 19131, 11268,   338,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4918 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   894,  4293,   216,   207,   906,  2916,   230,  6248,   116,\n",
      "          303,   552,   213,   207,   820,   110,   212, 15123,   175,   171,\n",
      "          110,   163,   867,   211,  3042,   906,   207,   276,   264,   115,\n",
      "          105,   126,   145,   128,  1377,   110,  6248,   116,   303,   110,\n",
      "          218,   145, 15123,   175,   171,   906,   687,   262,   297,   219])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4959 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  152,  184, 4019,  126,  128,  253,  207, 1366, 1940,  210,  207,\n",
      "         954,  906,  386,  227, 2097, 1030,  211, 1866,  239, 5678,  215,  231,\n",
      "        4529, 5015,  115,  228,  221, 7001, 3550,  723,  210,  145,  687,  900,\n",
      "         215,  210, 7047,  210,  207,  687,  115,  215,  210, 2520,  748,  217,\n",
      "        1883,  115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4137 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   906,   117,   393,   115,   894,   388,   216,   552,   207,\n",
      "          687,  1276,   217,   906,   504,   244,   207,   376,   221,   207,\n",
      "          954, 12521,   171,   187,   687,  1276,   115,   270,   770,   211,\n",
      "          207,   906,   687,   212,   207, 12521,   171,   187,   687,  1630,\n",
      "          117,   207,   240,   386,   227,   753,   216,   145,  1630,  2431])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 568 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  361, 1266,  216,  207,  906,  687, 1362,  189,  207,  525,  516,\n",
      "         506,  221,  145, 1366,  691,  115,  361, 1266,  216,  207,  906, 1362,\n",
      "         189,  207, 1366, 5678,  506,  115,  212,  361, 1266,  216,  207,  326,\n",
      "         223,  207, 1432, 2461,  242,  207,  620,  115,  207,  240,  457, 1730,\n",
      "         216,  820])\n",
      "Original length: 1033 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1957,  119, 6672,  212, 5661,  117, 3650,  333, 3802,  115, 3497,\n",
      "         210, 1699,  585,  115, 5240,  115, 2817,  115, 1699,  585,  115,  483,\n",
      "         240,  115, 2148,  115, 1491,  115,  986,  115,  649,  115, 4225, 5855,\n",
      "         115,  522,  240,  115, 4386,  115, 6945,  115, 3367,  115, 1066, 5240,\n",
      "         115,  938])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 199, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2495 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  495,  266,  123, 1263,  709,  123,  709, 1051,  123,  585,  109,\n",
      "        3605,  152,  184, 1570,  126,  128,  709, 1051,  115,  585,  109, 3605,\n",
      "        1699,  585,  223,  145,  865,  116,  280, 3497,  117,  495,  266,  123,\n",
      "        1263,  709,  123,  709, 1051,  123,  585,  109, 3605, 5859,  109,  616,\n",
      "         266,  123])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 200, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2537 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  2450,   126,   128,   709,  1051,   115,   585,\n",
      "          109,  3605,   221,   207,  1401,   212,  7257,   210,   207,  1699,\n",
      "          585,  3497,  2859,   115,   207,   853,   217,   239,  1373,   212,\n",
      "          207,   504,   233,  6373,   244,   428, 14291,   119,   207,  2598,\n",
      "          217,  2242,   658,   212,   207,  2664,   222,   495,  5240,   117])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2366 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  495,  266,  123, 1263,  709,  123,  709, 1051,  123,  585,  109,\n",
      "        3605,  495,  266,  123, 2117,  210, 1482,  123, 1699,  585,  152,  184,\n",
      "        4290,  126,  128,  709, 1051,  115,  585,  109, 3605, 1699,  585, 1585,\n",
      "         343,  237,  784,  110,  163, 5240,  245, 2587, 2907, 7286,  211,  207,\n",
      "         240,  213])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3516 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   495,   266,   123,  1263,   709,   123,   709,  1051,   123,\n",
      "          585,   109,  3605,   495,   266,   123,  2117,   210,  1482,   123,\n",
      "         1699,   585,   152,   184,  5045,   126,   128,  2117,   210,  1482,\n",
      "          115,   585,   213,   207,  1699,   585,  1158,   115,   331,   237,\n",
      "          784,   223, 16106,  8595,   755,   211,  3248,   145,   382,   550])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4602 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2081,  6236,   211,   226,   485,   115,   212, 10708,   189,\n",
      "          195,  2345,  2817,   238, 16509,   235,  5024,   236, 10926,   189,\n",
      "         6943,   117, 10708,   189,   195,  3503,   216, 10926,   189,  6943,\n",
      "          212,   239,  1066,   921,   115,   588,  1704,   117, 11763,   383,\n",
      "         3764,  9535,   172,   115,  6513,   174,   351,   700,   222,   366])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3943 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   223,  2318,  7583,   306, 20246,  7593,   212,  6122,   460,\n",
      "          115,   217,   207,  1667,  4536,   210,   229, 21972,   225,  1970,\n",
      "          213, 11202,  1990,   889,   210,  3115,   223,  7525,   452,   115,\n",
      "          212,   228, 21972,   223,  6242,   211,   219,   765,   307,   213,\n",
      "          145,  1207,   210,  2604,   117,   447,   117,   236,  5903,   117])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4741 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   282,   128,   225,   267,   211,   207,   296,\n",
      "          592,   115, 10926,   189,  6943,   386,   227,  3194,   216,  1799,\n",
      "          207,  3497,  1438,   297,  2675,  6055,   212,  2242,   658,   115,\n",
      "          212,   532,   465,   227,   321,  1459,   233,   481,   117,   152,\n",
      "          184,  2758,   126,   128,   207,  2161,   217,  6055,   212,  2242])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3427 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1699,   585,   126,   113,   342,   128,  3161,   216,  3489,\n",
      "         5374,   211,   207,  4383,   173,   373,  2041,  4392,   213,   522,\n",
      "          240,   117,   207,   461,   331,   894,   263,   145,  1549,  1066,\n",
      "          744,   211,  1168,  3489,   110,   163,  3802,  1379,   145, 10238,\n",
      "         1940,   210,   331,  3489,   271,  3672,   723,   211,   158,   195])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3714 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   388,   487,   222,   207,   588,  3603,   110,   163,   663,\n",
      "          210,   569, 12276,   235,   239,  7152,   117,  8998,   235,   211,\n",
      "          549,   207,  1699,   585,  3497,   115,   207,   240,   545,   119,\n",
      "         6781,   210,   207,  4611,   383,   550,   211,   207,   126,   966,\n",
      "        23197,   189,   128,   515,   356, 10899,   219,  1948,   218,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4014 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  247,  255, 1869,  281,  207,  629, 3367,  210,  237,  495, 1207,\n",
      "         120,  213,  228,  145,  256,  207, 1263,  736,  223, 2817, 1282, 6781,\n",
      "         210,  228, 1540,  211,  207,  495, 1207,  217,  239, 4254,  117,  447,\n",
      "         117,  236, 1433,  116, 1449,  117,  126,  113,  478,  128,  152,  184,\n",
      "        5045,  126])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4358 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 3802,  223, 1731,  218,  158,  117,  169,  117, 6819,  117,  780,\n",
      "         266,  142, 3869,  866,  116,  146,  111,  198,  112,  117,  248, 3869,\n",
      "         866,  116,  146,  111,  199,  112,  783,  216,  145,  322, 3614,  216,\n",
      "         145, 2319,  210,  275, 3802,  246,  213,  683,  210, 2069,  111,  146,\n",
      "         112,  111])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2703 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 16119,   351, 13733,   210,  9325,   818,   218,   145, 12571,\n",
      "          115,   443,   661,   189,  6730,  2070,   115,   268,   351,   145,\n",
      "        13815,   215, 28878,   117,   126,   113,   509,   128,  1700,   115,\n",
      "          207,  1375,   386,   227,   785,  1263,  2901,   115,   145,   393,\n",
      "         2025,   216,   115,   213,   775,   211,  6055,   213,  5839,   212])\n",
      "Original length: 999 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  1237,   128,   146,   117,   148,   117,\n",
      "         8284,   115,  3461,   117,   115,   881,   865,   119,   595,  1704,\n",
      "          117, 26032,   185, 10708,   189,   195,   115,   145, 12571,   115,\n",
      "         1125,   226,   347,   217,   949,   212,   126,   113,  1431,   128,\n",
      "          217,  6945,   210,   275,  1066,  3802,   236,   588, 10926,   189])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4319 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   111,   886,  6408,   115,   245,   201,   115,   689,   111,\n",
      "          105,  1741,   117,   105,   112,   115,   236,   279,   112,   115,\n",
      "          212, 10708,   189,   195,  3451,   117,   552,   532,  1800,   207,\n",
      "         1699,   585,  3497,  6985,   115,   532,  6986,   212,  2698,   117,\n",
      "         2791,   623,   211,   207,  2148,   210,   207,   759,   115,   229])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4830 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207, 10565,   292,   702,   211,  3248,  1491,   802,   207,\n",
      "         5104,   210,   207,  1632,   216,   233,   263,   207,  1550,   211,\n",
      "          284,   212,  1960,   119,   217,   253,   115,   314,  1558,   347,\n",
      "          218,   207,   126, 10565,   128,   115,   643,  1219,   219,  8617,\n",
      "          252,   218,   986,   212, 20180,  1309,   211,  1025,   207,  5104])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3807 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   626,   207,   521, 17735,  1485,   210,   207,  3497,   115,\n",
      "          152,   184,  2567,   126,   128,   986,   247,  7394,   252,  4527,\n",
      "          220,   975,   409,   215,  3297,   189,   217,   239,   352,   117,\n",
      "          321,   167,   117,  7725,   117,   162,   117,   162,   117,   535,\n",
      "          117,   115,  5995,   165,   117,   163,   117,   236,  1449,   117])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4013 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   244,  5093,   126,   113,  1047,   128,   211,   207,  4383,\n",
      "          173,   110,   163,  5240,   117,   213,  1598,   115, 10926,   189,\n",
      "         6943,  4906,   222,   152,   184,  2967,   126,   128,   350,   871,\n",
      "          408,   780,   266,   248,  3869,   866,   116,   146,   111,   198,\n",
      "          112,   115,   229,   265,   216,   145,  2218,   245,  4119,   145])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4652 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   416,   128, 10926,   189,  6943,  2042,\n",
      "          216,  3489,  6029,   217,   207,  5696,   216,   220,  5758,  5467,\n",
      "          207,   474,   210,  3802,   222,   220,   421,   311,   938,   145,\n",
      "          759,   225,   207,  4383,   173,   373, 15807,  2759,   213,   522,\n",
      "          240,   117,  3489,   503,   227,  1204,   228,   145,  2499,   525])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3824 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   219,   740,  3737,   301,   117,   213,  1408,   216,   207,\n",
      "         3338,   210,   105,  2095,   105,   213,   248,  3869,   866,   116,\n",
      "          146,   111,   198,   112,   329, 10708,   189,   195,   211,  2617,\n",
      "          373,   207,  4383,   173,   213,   207,   296,  1052,   115,   207,\n",
      "          483,   240,  3108,   211,   247,  9960, 15429,   174,  1699,   585])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4243 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  245, 2617,  213,  207,  483,  240,  117,  217,  392,  853,  115,\n",
      "         532, 6986,  207,  371,  210,  207,  483,  240,  212, 2698,  217,  417,\n",
      "        1545,  225,  226,  528,  117, 7446,  218,  119, 2341,  157,  117, 7096,\n",
      "         115, 3461,  117, 7446, 2341,  157,  117, 7096,  115, 3461,  117,  115,\n",
      "        1509,  865])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3967 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2332,   744,   211,  4119, 10708,   189,   195,   110,   163,\n",
      "         3802,   115,   207,  2218,   126,   113,  1033,   128,   249,  1710,\n",
      "         2948,  3278,  8860,   213,   207,   376,   710,   212,   331,   207,\n",
      "         1066, 20191,   210,   231,  6000,   246, 15457,   117,   353,  4471,\n",
      "          115,  6042,   210, 10708,   189,   195,   110,   163,   470,   218])\n",
      "Original length: 3160 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  7589,   128, 18978,   184,   115,   881,\n",
      "          865,   117,   226,   256,  3335,   238,   608,   835,   288,   218,\n",
      "        26937,  5793,  1175,   115,   521,   117,   115,   212,   207,   380,\n",
      "         1102,   210,   334,  3942,   115,   521,   117,   111, 12521,   112,\n",
      "          115,   565,   207,   821,   212,   439,   210,  2608,   116,   487])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4350 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   232,   246,   227,  1382,   115,  1528,   246,  1387,\n",
      "          211,   219,  1739,   425,   115,   212,   503,   227,  3217,   207,\n",
      "          439,   210, 12521,   110,   163,   147,   172,   190, 10423,   211,\n",
      "          325,   117,   145,   393,   954,   232,   317,   325,   212, 12521,\n",
      "          115,   229,  2081,   246,   745,   218, 12521,   115,   246,   213])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3057 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   325,   830,   189,  1590,  1626,   211,  1298,  1483,   210,\n",
      "          207,   879,   211,   635,   239,   811,   216,   207,   461,   210,\n",
      "        26937,   110,   163,  1002,   210,   207,  8271,   212,   207,  2045,\n",
      "          232,   305,   219,  1287,   218,   145,  1336,   117,   200,   126,\n",
      "          113,   113,   258,   128,   296,   115,   325,  1118,   303,   216])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4924 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   221,   217,   207,   954,   671,   232,   115, 26937,   110,\n",
      "          163,  1002,   210,   233,   223,   126,   113,  8626,   128,  7759,\n",
      "          552,   325,   110,   163,   460,   223,  2661,   221,   145,   691,\n",
      "          210,   266,   211,  2587,   216,   207,   671,   246,  2144,   145,\n",
      "          438,   117,   152,   184,  2567,   126,   128,   105,   253,   330])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4961 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   435,   115,   233,   223,  5220,   216,   207, 12521,   515,\n",
      "          210,   778,  2081,   957,   207,   333,   210,   207,   671,   221,\n",
      "          329,   218,   207,  1412,   210,   207,   671,  1092,   115,   212,\n",
      "          340,   207,  2269,   116,   459,   609,  1099,   232,   216,   325,\n",
      "          954,   297,  2397,   212,  1184,   207,   954,   671,   232,  2081])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4585 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,   635,   210,   239,  1151,   216,  2238,  1540,  1253,\n",
      "          216,   305,   219,  1287,   218,   145,  1336,   115,   325,  4906,\n",
      "         3093,   222,   145,   651,   211,   229,   207,   483,   240,  2629,\n",
      "         1739, 10598,   176,   190,   213,  5169,   239,   273,   117,   325,\n",
      "         3407,   189,   217,  1143,   105,  6352,  2734,   189,   105,   238])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5298 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   325,  2042,   216,   207,   369,   460,   210, 11081,   223,\n",
      "          765,   213,   126,   113,   113,   498,   128, 26937,   110,   163,\n",
      "         2680,  5774,   210,   242, 25057,   174,   235,   325,   222,   207,\n",
      "        12521,   438,   213,   308,   105,   211,  9285,   207,  7704, 30518,\n",
      "          105,   126,   113,  7655,   128,   325,   110,   163,  1282,  1290])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4598 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1743,   217,  2287,   210,  1045,   115,   911,   216,   207,\n",
      "          273,  3160,   213,  1152,   371,   217,   207,   588,   112,   120,\n",
      "         4585,  2013,  9433,   166,   117,  9763, 11525,   117,   535,   117,\n",
      "          115,  3474,   150,   117,   200,   174, 22817,   115, 17846,   111,\n",
      "          205,   475,   682,   117,   751,   112,   111,   376,   112,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3623 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   215,  5859,  1563,   117,   233,   587,   216,  1152,   371,\n",
      "          246,  1758,   614,   211, 26937,   222,   325,   110,   163,   388,\n",
      "          210,   145,   142,   199,   683,   117,   953,   117,   213,   256,\n",
      "          394,   527,   116, 26975,   115,   325,  1409,   238,   207,   308,\n",
      "          210,   207,   483,   240,  4197,   325,   110,   163,   867,   217])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2041 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   966,   347,   230,   117,   198,   119,   362,   116,  4984,\n",
      "          116, 13882,  2450,   116,  6753,   176,  4301,  5033,   150,   117,\n",
      "         1772,   117,   199,   174, 20278,   113,   120,   634,   165,   117,\n",
      "          163,   117,  6394,   117, 13648,   189, 26408,  2138,   113,   113,\n",
      "          120,   634,   116,   199,   616,  2987,   117,   111,  4693,   178])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  199,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 2733 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5859,   109,   616,   266,   123,   117,   117,   117,   123,\n",
      "         1035,  5508,  3064,   123,  1395, 13408,   109, 16822,   195,  1748,\n",
      "          123,   336,  9283,   152,   184,   973,   126,   128,  1035,  5508,\n",
      "         3064,   115,  1395, 13408,   109, 16822,   195,  1748,   330,   223,\n",
      "          230,  1059,   216,   207,  7892,  1104,   213,   207,   354,   265])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  200,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3180 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   233,   223,  2624,   266,   216,   145,  2695,  1121,   210,\n",
      "         1425,  5859,  3854,   388,   115,   331,  1125,   242,   142,   198,\n",
      "          215,   142,   199,   210,   207, 10104,   325,   115,   223,   237,\n",
      "          232,   211,  7349,   616,   117,   211,  2587,   216,   228,   237,\n",
      "          232,  2431,   317,   428,   215,   353,   652,   115,   145,   595])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  201,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 1978 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   966,   492,   123,  1126,   123,   949,   123,  2538,   949,\n",
      "          460,   123,  6295,   109, 14020,   123,   336,  9283,   152,   184,\n",
      "         2967,   126,   128,   949,   115,  2538,   949,   213,   145,  4596,\n",
      "          966,  1333,   217,  1364,   949,   115,   820,   311,  2587,   270,\n",
      "          256,   218,   145,  9074,  1303,   210,   207,   460,   117,  5859])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  202,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 1906 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  966,  492,  123,  117,  117,  117,  123, 1152,  371,  123, 3095,\n",
      "         221,  691,  210,  266,  123,  336, 9283,  152,  184, 5443,  126,  128,\n",
      "        1152,  371,  115, 3095,  221,  691,  210,  266,  145,  240,  249,  237,\n",
      "         590,  211, 1262, 1152,  371,  221,  233, 2448,  211,  145,  256,  213,\n",
      "         901,  210])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  203,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 2182 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   460,   123,  6295,   109, 14020,   123,   336,  9283,   460,\n",
      "          123,  2895,   123,   444,   460,   152,   184,  3875,   126,   128,\n",
      "          460,   115,  6295,   109, 14020,   213,  2775,   225,  1451,   452,\n",
      "         6295,   115,   145,   240,   211,   145,  1794,  2030,   223,   560,\n",
      "          225, 28315,   195,   116,   116,   216,   223,   115,   331,   233])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  204,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 2057 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   460,   123,  6295,   109, 14020,   123,   336,  9283,   152,\n",
      "          184,  6897,   126,   128,   460,   115,  6295,   109, 14020,   237,\n",
      "         5827,   261,   219,  3615,   253,   233,   223,  2318,   555,  2260,\n",
      "        25576,  1264,  3066,   117,  5859,   109,   616,   266,   123,   117,\n",
      "          117,   117,   123,  1035,  5508,  3064,   123,  1395, 13408,   109])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  205,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 1967 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5859,   109,   616,   266,   123, 10104,   325,   123,   336,\n",
      "         9283,   152,   184,  7913,   126,   128,  1035,  5508,  3064,   115,\n",
      "         1395, 13408,   109, 16822,   195,  1748,  5904,   235,  1748,   244,\n",
      "          227,  4524,  6735,   985,   213,   207,   649,   221,   145,  1440,\n",
      "         2025,   115,   310,   335,  1769,   222,   207,   636,  2327,   210])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  206,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 2416 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  7071,   126,   128,  5859,   109,   616,   266,\n",
      "          115, 10104,   325,   145,  7926,   218,   289,   243,   216,   233,\n",
      "          356,  1915,   145, 27550,   217,   239,  2381,   189,   211,  2789,\n",
      "          213,   405,  1016,   211,   207,  6537,   210,  1714,  1051,   223,\n",
      "          227,   460,   210,   145, 22908,   232,   211,  4034,   907,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  258,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 1958 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  6905,   126,   128,  1961,  1000,   115,  1366,\n",
      "         1491,   207,   266,  1309, 16723,   189,   207,   355,   210,   105,\n",
      "         4998,   105,   460,   117,  5859,   109,   616,   266,   123, 10104,\n",
      "          325,   123,   336,  9283,   152,   184,  5189,   126,   128,  5859,\n",
      "          109,   616,   266,   115, 10104,   325, 10815,  5120, 12353,   213])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  282,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 1825 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   966,   492,   123,   117,   117,   117,   123,  2779,   123,\n",
      "         1781,   210,  2779,   123,  1093,  2779,   460,   123,  2441,   123,\n",
      "          818,   460,   123,   367,  1275,   115,  4794,   109,  4885,   189,\n",
      "          152,   184,  4438,   126,   128,  1781,   210,  2779,   115,  1093,\n",
      "         2779,  1954,   117,   162,   117, 12548,   117,  4326,   111,   146])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  279,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 2775 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5859,   109,   616,   266,   123,   117,   117,   117,   123,\n",
      "         1035,  5508,  3064,   123,  1395, 13408,   109, 16822,   195,  1748,\n",
      "          123,   336,  9283,  5859,   109,   616,   266,   123, 10104,   325,\n",
      "          123,   336,  9283,   152,   184,  5265,   126,   128,  1035,  5508,\n",
      "         3064,   115,  1395, 13408,   109, 16822,   195,  1748,  3821,  2960])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  337,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 2493 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   217,   588,   119,  9367,   156,   117, 27582,   176,   115,\n",
      "        25853,   171,   160,   117, 16225,   115, 17822,   115,   149,   178,\n",
      "        13166,   184,   115,  2303,   109, 10076,   191, 18221,   175,   115,\n",
      "         3021,   115,   148,   173,   117,   126,   113,   113,   199,   128,\n",
      "          217,   588,   119,  3223,   164,   117,  3921,   115, 13813, 13599])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  368,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3804 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   201,   128,   146,   117,  1961,  3176,\n",
      "          222,   760,   422,   115,   722,   115,   145,  4179,   246,   678,\n",
      "          213,   226,   240,  3187,  5859,  2581,   351,  1298,  3706,   918,\n",
      "          117,   293,  1298,   536,  3728,   292,   678,   115,   207,  1263,\n",
      "         2980,   222,  2186, 25157,  1641,  1193,  1245,   536,   578,   872])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  342,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3679 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1991,  210, 1154,  115,  207,  327,  617,  210, 2632,  212, 5791,\n",
      "        2632, 7892,  189, 1703,  238,  597,  268,  198,  108,  210,  241, 7892,\n",
      "         189, 1165,  213,  207,  354,  265,  213, 1697,  211,  875,  117,  197,\n",
      "         108,  213, 2912, 1167,  212, 2012, 1154,  117,  889,  115,  160,  258,\n",
      "         117,  213])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  404,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4376 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   255,  1624,   117,   820,   435,  3456,   216,   894,  1609,\n",
      "          207,   405, 19587,   317,  1926,   212,  2632,  7892,   189,   213,\n",
      "          308,   211,   468,   207,  3854, 10290,   188,   211,   432,   212,\n",
      "         3583,   117,   820,  3194,   216,   894,   263,  3865,  3345,   211,\n",
      "        14142,   555,   526,   210,   661,  2073,   236,   207,  3706,  3070])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  416,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3724 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6333,   165,   117,   163,   117,  6277,   115,  1076,   156,\n",
      "          117,   149,   174,   117,   199,   174,  7941,   115,  2220,   163,\n",
      "          117,   756,   117, 15516,   111,  1465,   112,   115,   212,  1148,\n",
      "          210, 24424, 11561,   189,   171,   166,   117,  5524,   173,  7097,\n",
      "         1821,   189,   115,   521,   117,   115,  3976,   150,   117,   200])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  422,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5093 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   237,   232,  2431,   317,   428,   215,   353,   652,   115,\n",
      "          145,   595,   311,  2859,   110,   145,   981,   195,   210,   592,\n",
      "          215,   145,   346,  1650,   212,  2466,   115,   215,   145,  1085,\n",
      "          210, 18883,   213,   237,  2093,  1693,   117,   105,   110,   117,\n",
      "          117,   117,   105,   532,  5138,   216,   233,   223,   307,   213])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  433,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4774 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  270,  256,  218,  145, 9074, 1303,  210,  207,  460,  117,  105,\n",
      "         112,  117, 1489,  115,  728,  253,  820,  244, 2742,  211,  207, 1683,\n",
      "         216,  311,  219, 1564,  236,  207, 1152,  371, 1925,  115,  270, 4040,\n",
      "        1568, 8616,  395,  207, 1233,  117, 5524,  173, 7097, 1280, 1931,  216,\n",
      "         152,  184])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  339,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5181 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   371,  1355,   115,   207,   240,  3448,   233,  1549,   211,\n",
      "         1262,  1067,   244,  1451,   452,  6295,   210,   237,   105,  1682,\n",
      "          115,  3867,   252,   232,   115,   105,   213,   665,  1026, 14784,\n",
      "         5267,  6219,  6366,  3272,   115,  5479,   150,   117,   200,   174,\n",
      "         8028,   115,   634,   167,   182, 21269, 12326,   115,   236,   113])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  441,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4909 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1700,   115,   152,   184,  6897,   126,   128,   237,  5827,\n",
      "          297,   219,  3615,   253,   233,   223,  2318,   555,  2260, 25576,\n",
      "         1264,  3066,   117,   217,   353,   268, 17456,   637,   115,  8283,\n",
      "         3255,   189,   247,   560,  2149,   225,  2763,   212, 16318,   358,\n",
      "          210,  5169,   145,  5578,   117,   569,   259,   115,   145,   394])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  478,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4485 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2932,   233,   225,   220,   117,   221,   145,   945,   115,\n",
      "          207,   513,  2001,   783,   145, 29919,  2466,   210,   207,   828,\n",
      "          210,   207,   906,   222,   145,   558,   116,  5736,   421,   117,\n",
      "          126,   113, 17521,   128,   213,   482,   115,   553,   117, 17080,\n",
      "         1656,  2522,   216,   300,  3586,   252,   207,  8129,   235,   811])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  516,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4739 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,  1043,   128,   198,   117,  5415,   235,\n",
      "          105,  1440,  1674,   105,   820,  3456,  7374,   105,  1440,  1674,\n",
      "          105,   211,   635,   270,   388,   216,   894, 14142,   174,   211,\n",
      "         4034,   907,   119,   111,   198,   112,  4053,   235,   210, 11430,\n",
      "          120,   111,   199,   112,  2192,  1725,   189,  1190,   189,   120])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  498,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4320 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1975,  4748,   182,  6563,   117,   152,   184,  6140,   126,\n",
      "          128,   213,   308,   211,   725,   145,   105,  1440,  2025,   115,\n",
      "          105,   237,  1312,   311,   105,  6368,   211,  2990,   207,  1814,\n",
      "          216,   207,   894,  2001,   292,  2104,   213,  2542, 10326,  3821,\n",
      "         5960,   117,   105,  5524,   173,  7097,   115,  3976,   150,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  442,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4265 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  286,  246, 5493, 1797, 1013,  212,  467,  211, 1026,  680,  448,\n",
      "         189,  210,  260,  210,  207,  894,  115, 3275,  373,  207, 2960,  235,\n",
      "        1789,  363,  117, 5468,  115,  207,  240,  614, 1152,  371,  211,  207,\n",
      "         894,  552,  330,  246,  230,  460,  216,  392,  286, 6152,  263,  220,\n",
      "         363,  222])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  556,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4403 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   986,  1309,  4264,  3854,   470,   216,   110,  1968,   211,\n",
      "         8247,   237,   232,   238,   117,   117,   117,  1798,  2642,   145,\n",
      "         1591,   210,   972,   126,   113, 21817,   128,   460,  6368,   235,\n",
      "          211,  1596,   237,   232,   110,   105,   112,   117,  2262,   115,\n",
      "          820,  3456,  7117,  6149,   105,  3212,   252,   211,   207,  1285])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  554,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3625 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  207, 3026,  425,  115,  105, 2632, 6572,  297,  247,  211, 2326,\n",
      "         459,  225,  789,  710,  211, 4310,  117,  226,  513,  223, 1898,  310,\n",
      "         145, 1538,  211, 7845,  405, 1016,  115,  221,  820, 4327,  117,  233,\n",
      "         223,  115,  213,  482,  115,  145, 1444,  210,  237, 1870,  211,  465,\n",
      "         340,  253])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  508,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4311 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113,  113, 1185,  128,  820, 4293,  216, 7117, 6149,  110,\n",
      "         105,  627, 4053,  105,  246,  237, 4026,  222,  713,  201,  115, 1154,\n",
      "         115,  210, 1725, 1524,  217, 6809,  189,  117,  820, 3194,  216,  226,\n",
      "         246,  237, 1971,  216, 7117, 6149,  752,  211, 2807,  907,  212,  246,\n",
      "         145,  105])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  509,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4224 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  221,  207,  716, 6136,  210,  270, 4053,  235, 3880,  115,  820,\n",
      "        4293,  216,  894,  407, 2709,  211,  105, 4053,  105,  222,  207,  298,\n",
      "        4027,  119,  111,  198,  112,  213, 2012,  647, 1154,  115, 2709, 1905,\n",
      "         216, 7117, 6149,  212,  162,  180,  188, 2577, 5856,  189,  238,  270,\n",
      "        5791, 2632])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  345,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4095 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113, 22726,   128,   292,  2458,   112,   120,   236,\n",
      "         2273,   111,   105,   153,  4644,  1357,   259,  7439,   235,   211,\n",
      "         6830,   115,  6809,   189,   115,   216,   110,   163,   343,   153,\n",
      "         7516,  1024,   210,  1357,   286,   117,   105,   112,   120,   236,\n",
      "         4148,   111,   105,   330,   246,   230,  3612, 10707, 14047,  1561])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  415,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4535 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   356, 10899,   725,   145,   105,  1440,  2025,   105,   552,\n",
      "          335,   261,   227,   635,   145,   480,  5827,   210,   535,   116,\n",
      "        10815,   105,  4053,   235,   117,   105,   553,   117,  2709,   110,\n",
      "          163,  1681,   246,  4596,   210,  1681,   229, 11964,   210, 11950,\n",
      "          189,  1086,   222,  2481,  2940,  3198,   189,  1425,   414,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  846,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4744 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,  1299,   128,   213,   245,  1154,   115,\n",
      "          146,   109,   167,  2281,   216,   207,   418,   210,   616,  4690,\n",
      "         2318,   107,   846,   117,   206,  1105,   558,   398,   117,   321,\n",
      "         8256, 14596,  6760,   116,   416,   115,   236,  5382,   115,   398,\n",
      "          116,   711,   616,  4690,  3176,   115,   777,   245,   556,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  875,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3885 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1703,  1590,  1725,   217, 30313,   901,  2665,   117,   105,\n",
      "        12891,  2246, 13902,   117,   115,   236,  1057,   117,   553,   117,\n",
      "        12891,  2246,   323,  3586,   252,   216,   300,  6476,  1725,   222,\n",
      "          350,  4619,   189,   212,  1640,   207, 22886,  8852,   188,   174,\n",
      "          804,  1032,   443,  1703,   207,  1725,   225,   105,   230,  2906])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  912,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4922 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6311,   253,   532,  2468,   233,   117,   117,   117,   117,\n",
      "          253,   153,   263,  6476,   207,  1042,   115,  4122,   222,  1459,\n",
      "        10031,   153,  2468,   207,   424,   115,   153,   481,   247,  1640,\n",
      "          207,   483,  1032,   115,   787,   233,   212,  7516,   233,   117,\n",
      "          153,  2081,  6764,   288,   145,  2906,   117,   105,   112,   126])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  774,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4497 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   223,  2159,   211,  8347,   207,   620,   215,  1158,   210,\n",
      "          207,   671,   117,   213,   220,   377,   115,   207,   671,  3120,\n",
      "          307,   216,  3983, 13590,   110,   163,   711,   210,   207,   398,\n",
      "         4473,  6309,   427,  3699,   239,  6058,   162,   180,   188,  5424,\n",
      "          117,   233,   386,   227,   635,   145,   388,   216,   894,   105])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  910,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5181 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   240,   110,   163,  2120,   115,   390,   115,   211,   450,\n",
      "          116, 15887,   207,   316,   371,   210,   207,  7892,   918,   212,\n",
      "         1800,   216,   894,   105,   305,   247,   105,  3484,   207,  1190,\n",
      "          189,  1435,   117, 29962,  4216,   189,   216,   152,   184,  8577,\n",
      "          126,   128,   343,   330,   223,   237,   972,   316,  3483,   217])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  984,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3931 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113, 14920,   128,   894,   126,   113,   113,   456,\n",
      "          128,  4051,  2415,  3327,   116,   211,   116,  4551,  1198,   586,\n",
      "          238,  1620,   171,   118, 10958,   178,  1991,   213,   207,  2012,\n",
      "         3024,   189,   117,   889,   115,   160,  3739,   117,  7117,  6149,\n",
      "         4051,   239,  6809,  1510,   189,  1190,   213,  1154,   117,   226])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1043,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3953 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   161,   119,   340,   216,   207,   307,   710,   216,   799,\n",
      "          481,  8247,   216,   330,   223,   145,   405,  2459,   223,   253,\n",
      "          799,  4445,   145,  1198,  1114,   120,   223,   216,  1367,   124,\n",
      "          145,   119,   216,   110,   163,   319,   117,  3290, 13902,   117,\n",
      "          115,   236,  3866,   115,  3926,   117,   508,   126,   113,   113])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  909,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4292 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  161,  119,  225,  267,  211,  207,  477,  210,  327,  617,  586,\n",
      "         115,  213, 1216,  631,  115,  356,  216,  477,  210,  586, 2508,  405,\n",
      "        2687,  124,  145,  119, 2958,  117,  161,  119, 1459,  124,  145,  119,\n",
      "         289,  210,  207, 2762,  117,  117,  117,  216,  110,  163,  329,  215,\n",
      "         216, 2907])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  673,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4056 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1016,   211,  7083,   211,  1403,  1216,  2381,   189,   238,\n",
      "         2417,   235,   617,   213, 21405,   189,   288,  1216,   424,   804,\n",
      "          117,   509,  1015,   115,   207,   240,  1723,   126,   113, 20865,\n",
      "          128,   216,   207,  1743,   210,   820,   110,  1909,   223,  8325,\n",
      "          221,   211,   331,   207,   286,  1329,   211,  1620,   171,   118])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  895,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5191 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1743,   212,  5602,   402,  1596,   216,   162,   180,   188,\n",
      "          115,   146,   109,   167,   115,   212, 22886,  8852,   188,   174,\n",
      "          765,   270,  1604,   126,   113,   113,  2367,   128,  5970,   488,\n",
      "          293,  7117,  6149,   110, 30313, 10338,  4026,   117,   242,   324,\n",
      "          620,   115,   894,  4293,   115,   233,   503,   227,   500,  2578])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  925,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5266 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   365,  4382,   216,   532,   263,  2150,   221,   145,\n",
      "          519,   210, 30313,   110,   163,   872,   115,   233,  6051,  1789,\n",
      "         1143,   597,   268,   145,   450,   211,   468,   216,   273,   216,\n",
      "          210,   971,   532,  6311,   353,  1364,   115,   212,   353,  1714,\n",
      "         1051,   115,   340,   216,  1355,   297,   247,   255,  4614,  2959])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1027,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4860 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1640,   211,   851,  5748,   459,   216,   145,   405,  1114,\n",
      "         1219,   219,  3644,   303,   281,   207,  1595,   428,  3806,   117,\n",
      "          105,   207,   231,  1468,   119,   105,  1640, 18248,   195,   126,\n",
      "        13625,  8137,   115, 22886,  8852,   188,   174,  1771,   707,   210,\n",
      "          804,   128,   211,   321,   253,   741,   246,  3644,   211,  3589])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1007,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4847 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2948,   115,   820,  1115,   230,   635,   217,   270,  3147,\n",
      "          216,   293, 30313, 10338,   233,   246,   105,   656,  1004,   211,\n",
      "          207,  2212,   116,   695,   210,   146,   109,   167,   212,   162,\n",
      "          180,   188,   105,   211,  2807,   207,   907,   210,  2632,   252,\n",
      "         7892,   189,   117,   321,  2945,   189,   117,   110,  9867,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  899,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4112 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113,  113, 3266,  128,  820, 1115, 1569,  353,  268,  207,\n",
      "        8983,  235,  189,  210,  553,  117, 2709,  211,  635,  270, 3147,  216,\n",
      "         162,  180,  188,  212,  146,  109,  167,  105,  305,  247,  105, 1589,\n",
      "        2937,  222,  207, 2632,  327,  117,  820,  247,  227, 2923,  207, 5602,\n",
      "        1885,  271])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1100,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4504 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   424,  3586,   229,  7644,   252,  1590,   126,   113,   113,\n",
      "         2531,   128,  4730,   864,   212,  2629,  1590,  2381,   189,   237,\n",
      "         1588,   126,   113, 19483,   128,   211,  5463,  1590,  1926,  4091,\n",
      "         3534,   117,   270,  1703,  9512,   235,  5651,  1926,  6572,   115,\n",
      "         2305,   252,   218,   207,   245,   405,  1114,   115,   249,  4987])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1195,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4654 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,  3453,   128,   203,   117,   817,   210,\n",
      "          327,   820,  3194,   216,   207,   105,   636,   189,   210,   207,\n",
      "        11990,   105,   223,   145,  1440,  2025,   552,   207,   105,  7892,\n",
      "         1104,   110,   163,  1694,   223,  1750, 15362,  3079,   211,  4748,\n",
      "          182,  1312,   117,   105,  2945,   189,   117,   110,  4947,   189])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1048,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4249 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   111,   227,   235,   216,   207,  2699,   210,   105, 10815,\n",
      "         5120, 12353,   105,   223,  8325,   552,   233,  2001,  9312,   189,\n",
      "          207, 20935,   175,   216,  6632,   189,   237, 23959, 26372,   327,\n",
      "          112,   117,   984,   126,   113,   113,  3608,   128,  1296,   115,\n",
      "          330,   244,  3865,  7080, 10237,   547,   225,   820,   110,   811])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1036,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3161 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   820,   247,   227,  1564,   460,   238,   229,   145,   480,\n",
      "         5827,   481,   219,  1965,   216,   894,  1532,   207,   394,   210,\n",
      "          405, 19587,   211,  2508,   270,  3854,   115,  1750,   213,   901,\n",
      "          210,  1704,   117,  3290,   110,   163,  8325,  1743,   222,   226,\n",
      "          550,   117,   207,   482,   216,   207,  1252,   210,   405, 19587])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  541,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3659 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   236,   207,   913, 19623,   967,   207,  3706,   243,   448,\n",
      "          189,   117,   207,  1140,   563,   210,   820,   110,  3147,   189,\n",
      "          802,   331,   392,  2073,  1325,  2366,   189,   210,  2960,   235,\n",
      "          223,   207,   298,   119,   207,   526,   210,   661,  1624,  5039,\n",
      "          115,   221,   503,   207,  3865,   231,  6515,   216,   292,   332])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1039,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4989 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1280,   115,   820,  4327,   216,   213,   207,  7244,   189,\n",
      "          207,  7892,   918,  1287,   211,  3017,  4328,   301,   225,   207,\n",
      "          105,   780,  4283,   115,   105,   207,  5140,   213,  7892,  1879,\n",
      "          383, 13003,   497,   211,   207,   296,  1066,  2746,   189,  2646,\n",
      "          235,  9250,   235,   225,  2539,   117,   321,  2945,   189,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1133,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4577 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   832,  1395, 13408,   608,   967,   894,   391,   207,   259,\n",
      "          287,   210,   245,   198,   115,  1167,   555,   760,   205,   115,\n",
      "          722,   211,   111,   198,   112,  4621,   238,  4320,   225,   374,\n",
      "          211,   207,   780,   212,  1218,  2749,   210,   270,   338,   115,\n",
      "          111,   199,   112,  4621,   238,  2796,   222,   207,   421,   210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1124,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 2893 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   405,  2080,   608,   213,  2795,   115,  8280,  5770,  2504,\n",
      "          115,  2356, 13798,   115, 13741,   115,   212, 10156, 10195,   111,\n",
      "         6980,   265,   112,   117,   936,  5550,   175,   465,   820,  3673,\n",
      "          207,   384, 13628,   213,   392,   779,   216,   297,   468,   220,\n",
      "          228,   704,  2093,   115,   696,   465,   820,   465,   220,   353])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1291,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5187 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,  3866,   128,   126,   113, 17883,   128,\n",
      "         1704,   117,  3290,  3704,   230,   207,  7791,   221,   211,   207,\n",
      "          812,   213,   229,   226,   382,   832,   232,   481,   247,   255,\n",
      "         2534,   569,   207,   971,   210,  2391,   637,   310,  1625, 13969,\n",
      "          189,   222,   207,   105,  1440,  1674,   117,   105,   207, 27795])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1144,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4518 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   233,  3748,  2493,   225,   405,  3732,   115,   310,   153,\n",
      "         3652,   207,  5696,   216,  6252,   216,   216,   245,   219,  1791,\n",
      "        23959, 26372,  6104,   658,   115,   728,   253,   207,  1285,   115,\n",
      "          218,   207,   710,   115,   728,   253,   207,  1285,   247,   975,\n",
      "          233,   211,   468,   216,   126,   113,   113,  3898,   128,  2687])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1185,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4983 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1359,  887,  212, 8998,  235, 1222,  115,  894,  305,  227,  247,\n",
      "         255, 4589,  175,  211, 2807,  907,  117,  221, 1468,  213, 8313,  175,\n",
      "         540,  115,  390,  115,  907, 5180,  391,  207, 2010,  287,  210, 1697,\n",
      "         555, 1154,  229,  246,  323,  145,  287, 3131,  218, 1359,  887,  212,\n",
      "        8998,  235])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1323,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4467 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   628,   117, 14394,   145,   117,  5238,   213,   207,   327,\n",
      "         1625,   268,  2483,   237,  5827,   210,  3854,   115,   402,   987,\n",
      "          218,   606,   820,   212,   126,   113,   113,  3711,   128,   894,\n",
      "         2859,   216,   260,   210,   207,   894,   246,  6528,   221,   211,\n",
      "         1067,   297, 10733,   213,   207,  7892,  1104,   391,   207,   259])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1306,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4436 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   447,   117,   236,  1272,  4214,   116,  4911,  2311,   111,\n",
      "          227,   235,   323,   105,   207,   302,   235,  1814,   210,  6891,\n",
      "          166,   176,   183,  6572,   216,   289,   414,   213,   207,  1149,\n",
      "          261,  3054,  4763,   466,   211,   270,  5101,   211,  7805,   405,\n",
      "        12919,   188,   189,   221,   330,   261,   219,  4477,  3425,  2765])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1123,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4179 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   636,   695,   212,   270, 20935,   175,   225,   267,   211,\n",
      "          405,   212,  3499,  1041,   117,   105,   112,   120, 20462,   116,\n",
      "          241,  1610,   117,   166,   117,  4272,  3100,  4057,  3710,   213,\n",
      "          189,   190,   117,   115,  7879,   150,   117,   199,   174,  7497,\n",
      "          115,  7352,   111,   198,   189,   190,   682,   117,  1474,   112])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  743,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3715 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   894,   323,   564,   303,   216,   207,  1601,   210,   820,\n",
      "          110,  1909,   236,   289,   564,   246, 20881,   222,   207,   599,\n",
      "          550,   210,   331,   894,  2104,   213, 22908,  6563,   117,  1039,\n",
      "         1704,   117,  3290,   763,   119,   105,   207,   872,   210,   207,\n",
      "         3539,  7892,  3573,   126,   113,   113,  2760,   128,   247,  8152])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1247,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5090 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   979,   200,   115,  1020,   115,  5691,   947,   637,   293,\n",
      "          207,   832,  3854,  4051,   117,  2356, 15062,  4313,   594,   115,\n",
      "          676,   117,   204,   120,   889,   115,   160,   509,   117,   372,\n",
      "         1899,   217,  7547,   115,   906,   126,   113, 23073,   128,   418,\n",
      "          115,   212,   522,  3363,   833,   115,   207,  4551,   916,   405])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1342,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4396 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  503,  212,  211, 2269, 6621,  231, 1808,  110,  163,  405,  115,\n",
      "         216,  223,  145,  870,  461,  117,  310,  207,  907,  297, 2107,  459,\n",
      "         115,  216,  223,  145, 1415, 2761,  210, 1067,  246, 3644,  222,  117,\n",
      "         216,  110,  163,  207,  296,  332,  210,  233,  117,  161,  119,  244,\n",
      "         799, 8174])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1433,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 2973 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1104,  698,  111,  198,  112, 2778, 6865,  189,  115,  830, 1717,\n",
      "         189,  211, 6830,  217, 3622,  235,  405,  222, 8188, 6572,  115, 1124,\n",
      "         111,  199,  112,  105, 2778, 9134,  189,  105,  343,  145, 1547, 3704,\n",
      "         145, 2778,  116,  805,  116, 4317,  116,  805,  116,  677, 2672,  115,\n",
      "         215,  111])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1449,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3540 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1704,   117,  3290,  2522,   216,   213,   207,   584,   637,\n",
      "          293,   207,  2728,   210,   207,  3854,   115,  1104,  1046,  1703,\n",
      "          353,   268,   107,   198,   117,  1043,  4181,   934,  5765,   562,\n",
      "         1703,   307,   107,  7950,  1105,   117,   321,  3290, 14394,   115,\n",
      "          160,   186,   416,   116,   422,   117,  1704,   117,  3290,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1237,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4580 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   243,  1154,   751,   722,   241,  1285,   116,   199,   117,\n",
      "          200,   108,   200,   117,   204,   108,   889,   115,   160,   186,\n",
      "         1306,   116,   743,   117,  7117,  6149,   110,   327,   617,  1703,\n",
      "          416,   117,   202,   108,   115, 22886,  8852,   188,   174,   110,\n",
      "          163,  1703,  1100,   117,   202,   108,   115,   162,   180,   188])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1431,  210, 1431,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 2889 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   240,  1723,   216,   207,   460,  6443,   218,   820,\n",
      "          386,   227,  5243,   301,   215,   213,  1614,  2990,   207,  1814,\n",
      "          216,   894,  3350,   972,   301,   115,   221,   329,   218, 11938,\n",
      "        20564,   171,   117,   728,   242,   207,   353,   847,  1152,   371,\n",
      "         1355,   115,   207,  6295,  8946,   213,   635,   210,   820,   110])\n",
      "Original length: 3610 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,   113,   199,   128,   126,   113,  8300,\n",
      "          128,  5521,   115,   148,   117,   154,   117,   115,   588,   266,\n",
      "         3573, 24423,   109, 24177,   115,  3041,   189,   189,  5483,  2094,\n",
      "          115,  3361, 27222,   115,  9265,   383,   109,  3045,   175,   111,\n",
      "          105,  3041,   189,   189,  5483,  2094,   105,   112,   115,   212])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2746 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  236,  145,  886,  222,  692,  416,  115,  634,  115, 1516,  126,\n",
      "         113,  113,  202,  128,  211,  938,  207,  393,  396,  759,  246,  614,\n",
      "         115,  212,  207,  894,  110, 3883,  211, 2310,  207,  450,  396,  759,\n",
      "         212, 4106,  411, 2148,  292, 1463,  213, 1529, 5530,  221,  335,  230,\n",
      "        1579, 1356])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4621 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1696,   210,   207,   889,   284,   387,   688,  2277,  1601,\n",
      "          218,   207,   240,   117,   221,   219, 12499,   189,   145,   867,\n",
      "          211,  2310,   242,   525,   279,   111,   146,   112,   111,   203,\n",
      "          112,   115,   207,   889,   244,  2209,   211,   219,   221,   832,\n",
      "          213,   207,   759,   217,   504,   210,   207,  3107,   867,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4794 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207, 20222,   210,   424,  1793,   246,   227,   488,   211,\n",
      "          207,  1534,   210,   463,  1138,   735,   348,   117,   388,  1764,\n",
      "          678,   218,   207, 24423,   109, 24177,   942,  1254,   323,   950,\n",
      "         3049,   424,  1793,   189,   271,   218,   145, 24423,   109, 24177,\n",
      "          533,   117,   145, 23269,  1292,   242,   207,  2557,   210, 24423])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4648 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   502,   115,   211,   468,   145,  1081,   210,   207,   651,\n",
      "          300,   263,  1125,   225,   700,   212,   211,  5059,   303,   145,\n",
      "          485,   432,   217,   207,   256,   938,   117,   300,   503,   340,\n",
      "          117,   221,   300,   246,   472,  8855,   207,   256,   938,   115,\n",
      "        24160,   188,  4012,   218,   126,   113,  7950,   128,   275,  2336])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4237 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 4196,  166,  212,  953,  392, 4196, 4327, 1478,  212, 2422, 2067,\n",
      "         213, 1102,  225,  145,  394,  210, 2350,  189,  117,  213,  207,  296,\n",
      "         759,  115, 1554,  263,  227, 1541,  207,  649,  213,  229,  207, 2350,\n",
      "         189,  292,  820,  117,  207,  759,  936, 4142,  216,  207, 2350,  189,\n",
      "         292,  820])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3421 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1001,   163,   117,   756,   117, 25973,   111,  2706,   112,\n",
      "          112,   117,   145,   759,   245,   307,   219,  1629,   372,   105,\n",
      "          233,  1763,  1886,  1866,   216,   207,   595,   356,  2587,   230,\n",
      "          284,   210,   889,   213,   635,   210,   275,   388,   229,   297,\n",
      "          575,   700,   211,  1322,   117,   105,  3209,  2094,   166,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4706 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   217,   599,   579,   212,  2067,   552,   595,   246,  1622,\n",
      "          211,   126,   113,   113,   556,   128,  5818,  1623,   207,  7791,\n",
      "          112,   111,  3189,  7656,   182,   166,   117,   350,   871, 11429,\n",
      "          117,   115,  1247,   158,   117,   169,   117,   199,   174,  4501,\n",
      "          115,  4638,   115,  7130,   158,   117,   169,   117,   163,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2873 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  552,  525,  205,  111,  145,  112,  105,  386,  227,  264,  216,\n",
      "         145,  257,  311, 4299, 2600, 1623, 4098,  115,  212,  207,  240,  261,\n",
      "         227, 2310,  595,  110,  163,  759,  222,  216,  421,  105,  112,  117,\n",
      "        1554,  503,  227, 4299,  264,  126,  113, 8316,  128,  216,  233,  246,\n",
      "        3187, 4196])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4318 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2231,   213,   239,  1470,   217,   569,   584,   637,   117,\n",
      "          145,   650,   211,  2688,   220,   750,  3049,   768,   212,   207,\n",
      "          254,   189,   212,  1729,   335,   292,  3691,   223, 11332,   211,\n",
      "         1554,   110,   470,   117,   151,   116,   153,  1554,   115,  4387,\n",
      "          150,   117,  1772,   117,   199,   174,   236,  5253,   117,  1554])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3896 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1554,  3503,   216,   213,   463,  1138,   115, 23295, 23269,\n",
      "          189,   236, 24423,   109, 24177,   326,   174,   216,  2064,  1586,\n",
      "         6378,   678,   213, 23295,  6166,   649,   351,   595,   212,   231,\n",
      "         6166,   894,   292,  5275,   369,   286,   117,  4263,   160,   186,\n",
      "         1371,   115,   456,   117,  1554,  3503,   216, 16512,   179,   175])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4804 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 18857,   110,   163,   724,   288, 24423,   109, 24177,   110,\n",
      "          163,  4141,  2067,   218,   115,  1044,  1945,   115,  2041,   211,\n",
      "         3950,   225, 18857,   110,   163,  2053,   211,  9657,   212,  4281,\n",
      "         1398, 24423,   109, 24177,   795,   117,   126,   113,   113,   895,\n",
      "          128,   207,   450,   881,   249,   545,   216,   152,   184,  4214])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4950 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   330,   246,   230,  8518,   944,   211,  1025,   207, 24609,\n",
      "         2201,   210,   260,   212,  1425,  3629,   117,   213,   482,   115,\n",
      "          211,   465,   340,   297, 18947,   207,  7531,  5651,   207,  2418,\n",
      "          906,   189,   117,   809,   115,   207,  3036,   482,   216, 18857,\n",
      "          115,   555,   239,   313,   115,   481,   247,  3807,   174,   260])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 516, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3811 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   199,   117,  4435,   210, 13482,   235,  1312,   152,   184,\n",
      "         5484,   126,   128,  1554,   311,  4327,   145,   105,  4435,   210,\n",
      "        13482,   235,  1312,   105,  2619,   210,   105,   236,   684,   428,\n",
      "         1275,   210, 13482,   235,  1312,   105,  3030,   281,  1556,   637,\n",
      "          210,   260,   231,   117,   422,   165,   117,   163,   117,   147])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 498, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5481 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  5531,   126,   128,  3555,  3043,   116,  3307,\n",
      "          126,   113,   113,  1124,   128,  6630,   115,  1505,   116,  3307,\n",
      "         6630,   105,   386,   227,   960,   145,  1436,  9241,  2419,   117,\n",
      "          105,  6524,   111,  8775,   171,   112,   213,   192,   117,   535,\n",
      "          117,   166,   117, 10580,  2637,   213,   192,   117,  1392,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 442, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4221 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   243,   763,   216,   207,   908,   105,   261,   227,  1049,\n",
      "          239,  2053,   211,  2236,   207,   231,   649,   112,   120,  6779,\n",
      "          110,   156,   156,   117,   154,   117,   115,   107,   202,   117,\n",
      "          203,  1105,   213,  4851,   189,   246,  2225,   211, 24082,   115,\n",
      "          236,   146,   973,  5387,   117,   198,   111,   557,   258,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 556, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3493 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,  7211,   128,  3498, 14399,   173,  6582,   189,\n",
      "        24699,   249, 12535,   252,   226,   525,   117,   282,   330,   223,\n",
      "          230,   944,   211,  1025,   226, 20929,   550,   552,   115,   991,\n",
      "          207,   889,   213,   207,   901,  1024,  2907,  7286,   211,  1554,\n",
      "          115,   207,   759,  3279,   189,   242,   207,  3226,  3664,   189])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 554, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3334 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   894,  4327,   216,  1554,   249,  1096,   211,  4327,   480,\n",
      "         2664,   222,   207,  2965,  3049,  6378,   552,   111,   198,   112,\n",
      "        18857,   481,   227,   625,  2357,   222,   207, 10028,  6378,   115,\n",
      "          212,   111,   199,   112,  1554,  2048,   211,  4327,   145,   394,\n",
      "          210,   599,  2762,   802,   207,  6378,   233, 13003,   571,   238])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 508, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4158 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,  6166,   748,   325,   111,   105, 12735,   171,   105,\n",
      "          112,   117,   216,  2481,   246,  1629,   221,   211,   207,  3685,\n",
      "         4423,  2148,   222,  1298,  6485,   117,  4387,   150,   117,  1772,\n",
      "          117,   199,   174,   236,  5014,   116,  1456,   117,  2481,  1420,\n",
      "          210,   207,   393,   396,   759,   936,  4142,  1480, 11996,   235])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 509, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3843 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1554, 2421,  207, 3399, 6339,  388,  223,  487,  222,  145,  107,\n",
      "        1586, 1105,  344,  595,  280, 6216,  145,  107, 3872, 1105,  906,  210,\n",
      "         411, 6166,  470,  213, 2664,  222,  207,  894,  110,  974,  216,  335,\n",
      "         297, 1086,  242,  207, 8056,  232,  117,  207, 8056,  232, 1092,  223,\n",
      "         832,  211])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 345, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4380 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 24423,   109, 24177,  2042,   216,  2481,  4035,   305,   219,\n",
      "         1629,   552,   233,  2048,   211,  1362, 10140,   506,   115,   386,\n",
      "          227,  4327, 12487,  2664,   115,   212,   223,  6640,   487,   222,\n",
      "          286,   212,  4942,   117,   207,   296,   428,  2148,   247,  1078,\n",
      "          255,  3124,   212,  1653,   115,  2559,   117,   221,   217,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 415, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3612 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1554, 1137,  189,  216,  207, 2674,  210,  207,  599,  256,  213,\n",
      "         229,  226, 1605,  174, 5934, 1402,  223, 5275,  115,  310,  265,  216,\n",
      "         207,  286,  223,  105, 1750,  281,  207,  894,  110,  494,  212, 8773,\n",
      "         189, 2779,  211, 1150,  207,  369, 1865,  460,  117,  105, 2945,  117,\n",
      "         110,  163])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 846, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4240 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1554,  249,  263, 1245, 5221,  189,  569,  207,  971,  210,  353,\n",
      "         268, 3437,  720,  211, 2771,  239,  759,  115,  212,  249,  571,  145,\n",
      "        7770,  528,  216,  249, 1560,  207, 1877,  212, 5766,  506,  217, 1024,\n",
      "         210,  207,  470, 2621, 1438,  117,  213,  775,  115,  661,  217, 1554,\n",
      "         763,  236])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 875, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2847 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   221,   145,   519,   126,   113,   113,  1288,   128,   210,\n",
      "          332,   153,   210,   226,   528,   115,   307,   584,   470,  1253,\n",
      "          351,  3041,   189,   189,  5483,  2094,   119,   111,   198,   112,\n",
      "        11628,  2295,   225,   316,  2718,   441,   111,  2481,   290,   112,\n",
      "          120,   111,   199,   112, 11628,  2295,   225,   438,   478,   111])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 912, 210, 912, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 24 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 3080,  167,  117, 5521,  165,  117,  163,  117,  148,  117,  154,\n",
      "         117,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 2146 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  8387,   128,  2231,   528,   226,   223,\n",
      "          145,  1059,   569,   207,  1210,   210,   207,  4113,   505,   105,\n",
      "        24447,   185,   189,   117,  1871,   117,   105,   820,   115,   289,\n",
      "          210,   229,   223,   207,   997,   706,   210,   207,  2923,  4113,\n",
      "          505,   115,  1968,   145,  1444,  5252,   207, 10775,   195,   210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4246 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   588, 24447,   185,   189,   274,  2251,   189, 20764,  9071,\n",
      "        11484,   223,   145,  1018,   301,   116,   545,  7877,   243,   216,\n",
      "          223,   332,   210,   145,  1018,   301,   116,   545,  1437,   210,\n",
      "          918,   242,   346,  1210,   117,   233,  1763,   238,   207,   879,\n",
      "          216,   588,   110,   163,  2120,   213,   226,  1437,   210,   918])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3304 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   229,   820,   297,   815,   207,  1024,   489,  3566,  4009,\n",
      "          189,   233,   571,   238,  1636,  9779,   301,  2041,   211,  1919,\n",
      "          588,  2278,   207, 24447,   185,   189,   117,  1871,  3287,   117,\n",
      "          226,  2366,   115,   211,   185,   115,  6810,  3307,   225,   230,\n",
      "         3017,   472,  6874,   317,   151,  6579,   212,   820,   117,   145])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2541 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   396,   759,   117,   222,   207,   421,   210,   207,  1434,\n",
      "          879,   115,   207,  2679,  4276,   216,   207,  4113,   505, 24447,\n",
      "          185,   189,   117,  1871,   305,   219,  1193,   211,   588,   111,\n",
      "          153,   112,   552,   207,  4113,   505, 24447,   185,   189,   117,\n",
      "         1871,   246,  2797,   215, 13438,   301,   578,   211,   588,   110])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2830 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   111,   426,   112,   588,   110,   163,   388,   210,   346,\n",
      "          266,  3427,  1016,   120,   212,   111,   628,   112,   588,   110,\n",
      "          163,   388,   210, 10306,   189, 11642,  3481,   213,   683,   210,\n",
      "          207,  4509,   171,   115,   342,   165,   117,   163,   117,   147,\n",
      "          117,   142, 14601,   111,   148,   112,   117,   290,   117,   152])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3677 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1877,   111,   153,   112,   212,   111,   166,   112,   244,\n",
      "         5593,  1169,  1438,   212,   809,   588,   110,   163,  3245,  1354,\n",
      "          388,  4678,   222,   331,   207,  5220,   879,  4722,   189,   221,\n",
      "          145,   691,   210,   266,   216,   588,  8192,   145,  1839,  6842,\n",
      "         1233,   212,   331,   820,   110,  4113,   505, 24447,   185,   189])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4141 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1304,   210,   521, 27839,  7748,  4655,   292,   207,   424,\n",
      "          210,  2067,   222,   207,   160,  1386,   117,   335,   323,   247,\n",
      "          227,  1624,   270,  1874,   210,  2419,   218,   969,   212,  6469,\n",
      "          460,   422,   588,   110,   163,  6844,   210,   207,  2023,   117,\n",
      "          433,   126,   113,   113,   478,   128,   152,   184,  4290,   126])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3124 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   321,  9289,  8836,   235, 10423,   115,   156,   117,   160,\n",
      "          117,   166,   117,  3097,  3226,  3288,   864,   115,   521,   117,\n",
      "          115,  2848,   150,   117,   200,   174,  1174,   115,   917,   111,\n",
      "          201,   475,   682,   117,  1094,   112,   126,   113,   113,   442,\n",
      "          128,   111,  3189,  4698, 10074, 12376,   668,   185,  1610,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4176 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   111,   290,   112,   213,   207,   256,   210,   145, 19788,\n",
      "          189,  1233,   216,   223, 19788,   189,   126,   113,   113,   509,\n",
      "          128,   236,   207,   259,   210,   570,   210,   207,  4113,   505,\n",
      "          115,   223,  2797,   215, 13438,   301,   578,   211,   215, 20377,\n",
      "         1982,   210,   216,  1233,   100,   117,   342,   165,   117,   163])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5054 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   450,  2025,   223,   331,   207,   333,   105, 24447,\n",
      "          185,   189,   105,   215,   105, 24447,   185,   189, 11197,  8648,\n",
      "          105,   725,   145,   384,   505,   215,  6242,   116,   407,   505,\n",
      "          210,   820,   117,   321,   342,   165,   117,   163,   117,   147,\n",
      "          117,   142, 14601,   111,   148,   112,   111,   198,   112,   111])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3854 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  4117,  3078,  1270,  2025,   246,  1649,   218,   207,   820,\n",
      "          110,   570,   210,  4455,   116,   584,  4113,  2302,  3177,   235,\n",
      "          606,   207,  2923,  1233,   115,   105,  4749, 19394,   115,   105,\n",
      "          212, 18037,   207,  1104,   213,   229,   207,   588,   407,   207,\n",
      "         1233,   115,   153,   117,   149,   117,   115, 10037,   235,   410])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1389 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  588,  323, 4142,  145,  264,  346,  266, 3245, 1354,  388,  351,\n",
      "         820,  115,  310, 1073,  257, 3243,  229,  585,  110,  163, 2865,  266,\n",
      "        1585,  215,  207,  489, 1877,  117,  509,  126,  113,  113,  895,  128,\n",
      "         217,  226,  744,  115,  552,  207, 3427, 1016,  534,  210, 1970,  265,\n",
      "         244,  578])\n",
      "Original length: 3139 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  528,  126,  113, 9342,  128, 2231,  528,  226,  691,  223,  373,\n",
      "         207,  240,  222,  126,  113,  113,  199,  128,  145,  867,  211, 2310,\n",
      "        4196,  290,  212,  426,  210,  588,  110,  163,  296,  396, 3757,  115,\n",
      "         678,  222,  767,  210,  595,  115,  207, 3422, 4782, 2106,  243,  212,\n",
      "         393,  116])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4224 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   198,  3650, 12838,  4310,   189,   225,   163,   172,   173,\n",
      "          211,   462,   958,  2106,   126,   113,   113,   202,   128,   410,\n",
      "          111,   283,   477,   808,   115,  3492,   808,   115,  1788,  1924,\n",
      "          189,   115,   586,  1924,   189,   115, 21580,   153,   117,   148,\n",
      "          117,   410,   115,  1842,  5274,   235,   115,  1842,  1613,   235])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4565 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   522,  1798,   250,   111,   105,  7619,   105,   112,\n",
      "          117,   241,   210,   392,   650,   189,   115,  4284,   212, 20621,\n",
      "          189,   244,   832,   211,   247,  3433,  1810,  3650, 12838,   110,\n",
      "          163,   212,   231,  2381,   189,   110,   418,   211,   145,   564,\n",
      "          343,   335,   356, 10899,  2834,  4310,   115,   212,   211,   247])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4606 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   163,   172,   173,  4906,  8491,   301,   222,   207,  3764,\n",
      "        13393,   273,   303,   210,   207,  3366,   881,   117,   213,  3764,\n",
      "        13393,   166,   117,  7473,  4847,   469,   115,  4835,   150,   117,\n",
      "          200,   174,  6074,   111,   204,   475,   682,   117,   722,   112,\n",
      "          115,   207,   240,   210,  1409,  1629,   145,  2078,   388,   217])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4996 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  7517, 15333,  3900, 15069,   325,   212,   207, 10104,   325,\n",
      "          284,   387, 19968, 19987, 18533,   547,   217, 28840,   189,   117,\n",
      "          117,   207,   428,  1275,   247,   870,  1064,   115,   870,  1126,\n",
      "          115,   212,   870,  1961,   506,   115,  1951,   335,   244, 27447,\n",
      "         1668,   213,   592,   117,   392,  1275,   244,  1528,   227, 28336])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4137 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3650, 12838,  2042,   216,   239,  3757,   323,  3503,   231,\n",
      "         1395,   116,  2010,   704,   216,   481,   432,   207,   421,   217,\n",
      "          145,   683,   210,   248,   428,   210,   207, 10104,   325,   283,\n",
      "          115,  6508,   210,  3650, 12838,   963,   115,   405,   116,  1050,\n",
      "        23387,   115, 10442,  1641,   115,   212,   145,   336,   650,   211])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3236 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1794,  1011,  1136,   212,   233,   105,  4131,   216,  3720,\n",
      "        19808, 17460,  9144,   292,   227,  1612,   211,   462,   145,   927,\n",
      "          301,   630,   452,   668,   175,   116,   160,   424,   117,   105,\n",
      "          213,   689,   163,   172,   173,  1502,  3650, 12838,   216,   233,\n",
      "          246,  5293,   211,  3836,   211,   668,   175,   116,   160,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 224 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   210,   241,   444,   266,   115,   217,   207,   853,   284,\n",
      "          387,   213,   207,  3761,   235,  2231,   528,   115,   207,  3720,\n",
      "        19808, 17460,  9144,   110,   867,   211,  2310,   223,  1463,   117,\n",
      "          233,   223,   340,  1677,   117,  7822,   147,   117, 21871,  4616,\n",
      "          354,   265,   483,   865,   777,   119,   589,   442,   115,   634])\n",
      "Original length: 3553 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  5648,   128,  8549,   184,   171, 15985,\n",
      "          172,   172,   383,  5483,   196,   115,   881,   865,   119,   428,\n",
      "          354,   265,   918,   216,   427,  4213,  6783,  1125,   226,  1018,\n",
      "         5859,   347,   115,  3187,   145,   405,   116,  2080,  3854,  2904,\n",
      "          218, 13359, 12379,   868,   189,   126,   113,   113,   199,   128])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4887 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1094,   112,   117,   367,   211,   729,   115,   216,   240,\n",
      "         1463,   687,  2122,   117,   293,  1024,   894,  2624,   115,  6069,\n",
      "          211,  1868,   115,   215,   292,  1629,   115,   207,   256,   351,\n",
      "          207,   947,  6593, 15203,   868,   189,   212,   207,   354,   265,\n",
      "          734,   210,   289,   210,   603,   111,  6549, 11999,  2977,   112])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2807 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   406,   175,   116,   155,  1409,   115,  4293,   235,   216,\n",
      "          207,  1436,   116,   363,   893,   111,   236,   684,   221,   763,\n",
      "          213,   207,  1336,  1638,   212,   629,  3925,   112,   386,   227,\n",
      "         3743,   226,   256,   117,   198,   126,   113,   113,   206,   128,\n",
      "          290,   117,   213, 11200,  2486,   604,   535,   117,   166,   117])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4862 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 11624,   117, 10964,   117,   166,   117, 16773,   178,  2582,\n",
      "         1610,   117,   115,  6333,   165,   117,   163,   117,  6277,   115,\n",
      "         8659,   115,  1076,   156,   117,   149,   174,   117,   199,   174,\n",
      "         7941,   115,  2220,   163,   117,   756,   117, 15516,   111,  1465,\n",
      "          112,   152,   184,  2138,   126,   128,   111,   105,  1985,  5859])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4149 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1912,   210,   207,   818,   238,   207,  2613,  3313,   115,\n",
      "          626,   239,   893,   740,   307,   211,   649,  1918,  1093,   818,\n",
      "          117,   201,   321,   115,   149,   117,   151,   117,   115, 12004,\n",
      "         1522,   115,  5635,   150,   117,   200,   174,   236,  7031,   116,\n",
      "         1001,   111,   105,   242,   126, 13112,   185,   171,   110,   163])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4944 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   441,   128,   426,   117,   406,   175,\n",
      "          116,   155,  2913,   211,  2357,   222,   207, 11200,  2486,   240,\n",
      "          110,   163, 11734,   222,   392,  1540,   211,  3194,   216,   233,\n",
      "          944,   227,  1790,   207,  1436,   116,   363,   893,   735,   213,\n",
      "        11200,  2486,   117,   126,   113,  5207,   128,  1280,   115,   406])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4663 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,  5903,   128, 11101,   540,   212, 17336,   383,\n",
      "         1513,   110,   163, 21478,   860,   210, 11200,  2486,   221,  1918,\n",
      "          105,  2404,  1093,   105,   818,   245,   115,   390,   115,  2641,\n",
      "          247,   255,   145,  1739,  7967,  1248,   210,   207,   256,   115,\n",
      "          217,   213,  1073, 11101,   540,   696, 17336,   383,  1513,   386])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3916 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,  2043,   649,   115,   986,   247,   227, 22224,   174,\n",
      "          211,   960,  5859,   126,   113,   113,   509,   128,   820,   211,\n",
      "         4327,   212,  2587,   145,  1436,   363,   222,   207,   354,   265,\n",
      "          117,   213,   775,   211,   207, 11200,  2486,   240,  1092,   115,\n",
      "          207,  4201,   881,   329,   228,  1683,   213,   145,   256,  1918])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4730 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   532,  4085,  4264,   207,   868,   189,   110,  3507,  9046,\n",
      "          631,   221,   211,   207,  4266,   210, 11200,  2486,   117,   207,\n",
      "          868,   189,  4293,   216,   213,  3356,   331,   105,   818,   105,\n",
      "          223,   105,  1093,   105,   111,   212,   809,   309,   211,   207,\n",
      "         1436,   116,   363,   115,  1625,   268,   207,   790,  3983, 12522])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4859 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  111,  221, 8863, 1824, 5361,  112,  115,  247, 1564, 9213, 4254,\n",
      "         115, 4000,  235,  222, 1338, 1877,  210,  818,  221, 6008,  213, 5964,\n",
      "         331, 4040,  210,  585,  223,  489,  117,  467,  392, 2044,  115,  126,\n",
      "         113, 5479,  128,  532, 2743,  986,  305, 1253, 4589,  175,  211, 1262,\n",
      "         207,  576])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 9 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 3382,  117,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 31 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   634,   165,   117,   163,   117,  6394,   117, 13648,   189,\n",
      "        14688,  4214,   115,   113,   198,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3826 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  528, 2231,  109,  308, 5628, 9347,  115,  154,  117,  589,  415,\n",
      "         115,  634,  820,  678,  145, 1609,  212,  396,  687,  347,  759,  111,\n",
      "         207,  105,  396,  759,  105,  112,  222,  767,  210,  241, 2070,  212,\n",
      "        1499,  443, 1536, 3182, 8012, 6909, 4653,  235, 8477,  213,  207,  354,\n",
      "         265,  656])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4425 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1130,  5814,   176,  2772,  1419,  3907,   241,   207,   315,\n",
      "          210,  5814,   176,   469,   115,   789,   588,   213,   226,   256,\n",
      "          216,   223,   227,  8936,   585,   115,  5814,   176,  2772,  1280,\n",
      "        18515,   216,   233,   386,   227, 16075,   215,   494,  5814,   176,\n",
      "          469,   117,   447,   117,   160,   186,   368,   116,   442,   117])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3169 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   219, 10348,   936,   119,  1067,   297,   728,   207,  1024,\n",
      "        13970,  7478,  3249,  1645,  3212,  1486,   314,   207,   489,  8477,\n",
      "        12011,   124,   113,   113,   113,  5814,   176,   223,   936,   289,\n",
      "          210,   207,  1506,   327, 10689,   213,  1026,   116,  1050,  3182,\n",
      "         8012,  9178,   212,  6909,  4653,  6848,   189,   212,  1949,  6848])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4808 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1038,  585,  115,  336,  215,  599,  115,  552, 5814,  176, 2772,\n",
      "         249, 3677,  230, 1919,  225, 3587,  212,  820,  110, 3244,  210,  347,\n",
      "         465,  227, 2140,  303,  210, 5814,  176, 2772,  110,  163, 5517,  225,\n",
      "        3587,  117, 5814,  176, 2772, 5918,  117,  213, 1772,  117,  236,  202,\n",
      "         116,  203])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4557 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 21704,  2390,   213,   184,   115,   521,   117,   115, 10058,\n",
      "          150,   117,  1772,   117,  5727,   115,  4563,   111,   149,   117,\n",
      "          148,   117,  2324,   117,  1344,   112,   111,  5932,   105,   310,\n",
      "          217,   105,  4831,  2646,   317,   126,   113,   404,   128,  6083,\n",
      "          212,  1563,   221,   211,   185,  2499,   115,  2465,   105,  3587])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3600 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1108,   213,   151,   190,   175,   350,  2644,  6733,   189,\n",
      "          117,   521,   117,   166,   117,  4782, 18714,  1610,   117,   115,\n",
      "         6365,   165,   117,   163,   117,  2153,   117,   148,   117,   147,\n",
      "          117,  6173,   115,  5150,   150,   117,   200,   174, 16795,   111,\n",
      "          148,   117,   147,   117,   682,   117,   722,   112,   212,  3194])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4283 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1094,   112,   117,   213,  3260,   235,   248,   279,   210,\n",
      "          207,  8913,  1386,   184,   325,   115,   532,   244, 14990,   216,\n",
      "          105,  5859,   649,   115,  2872,   324,  1918,  7249,   115,   245,\n",
      "         6417,  2517,   217,   220,   289,   483,   240,   211,  1150,   585,\n",
      "          569,   241,   207,   894,   115,   253,  1956,   211,   264,   915])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4054 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1489,   115,   207,   273,   213,  3764,  9160,   188,   503,\n",
      "          227,  2745,   207,   629,   256,   210,   237,  5955,   469,   115,\n",
      "          321,  2107,   116,  4183,   115,  9249,   150,   117,   199,   174,\n",
      "          236, 25201,   120, 21358, 29969,   116,   149,  6510,   115,  5155,\n",
      "          150,   117,  1772,   117,   236, 18117,   116,   925,   117,   216])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4766 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2772,   212,   231,  5814,   176,   891,   117, 14988, 11369,\n",
      "        10228,   188,  5198,   117,   160,   433,   117,  5814,   176,  2772,\n",
      "         1165,  2064,   107,   198,   117,   202,  4181,   213,   737,   211,\n",
      "          207,  5814,   176,   469,  1690,   213, 12621,  3348,   350,  4403,\n",
      "          117,   321, 14988, 11369, 10228,   188,  9622,  1522,   110,   163])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5284 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2570,  229,  207,  600,  469,  297, 2995,  818,  555,  239,  639,\n",
      "        1157,  215, 1254,  189,  120,  206,  112,  207, 1225,  210,  497,  469,\n",
      "         221, 1110, 3659,  210,  207,  600,  469,  115,  215,  221,  237, 1382,\n",
      "        2676,  120,  212,  258,  112,  806,  218,  207,  928,  210,  207,  497,\n",
      "         469,  210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2059 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3163,   189,   215,   220,   231,  4044,   252,   347,  1402,\n",
      "          225,   207,  3369,   117,   321, 18196,   171,   338,   115,   368,\n",
      "          150,   117,  1772,   117,   199,   174,   236,  6757,   117,   126,\n",
      "          113,   673,   128,   361,  1810,   226,  1151,   213,   145,  1090,\n",
      "         2561,   115,   820,   247,  1096,   211,  2600,  6621,  1067,  1275])\n",
      "Original length: 2609 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113, 21949,   128, 10060,  6808,   115,   881,\n",
      "          865,   119, 23812,   174,  1798,   243,   212,  4194,  2504,  1798,\n",
      "          115,   521,   117,   111,  1462,   105, 23812,   174,   105,   112,\n",
      "          198,   615,   207,   483,   240,   110,   163,  2957,   115,   294,\n",
      "          211,   522,   525,   210,   966,   492,   279,   111,   146,   112])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5051 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2353,  2255,   216,   350,   918,  2041,   126,   113,   113,\n",
      "          201,   128,  1003,   288,   207,   327,   481,   227,  4310,   253,\n",
      "          335,   263,   211,  6807,   810,  2106,  1410,   189,   115,   212,\n",
      "         1560,   226,  2161,   218,  5867,   152,   184,   973,   126,   128,\n",
      "          207,  7517, 15333,  3900, 15069,   325,   210,  1099,   111,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4664 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  4310,   217,  3492,   808,  1636,   225,   148,   189,   182,\n",
      "          115,   145, 21883,   188,   212,   353,  8538,   446,   115,  4782,\n",
      "        18714,  1092,  4051,   237, 10584,  7082,   211,  1115,   148,   189,\n",
      "          182,   117,  4782, 18714,  1863,   239,  1395, 13408,  1870,   115,\n",
      "        23812,   174,  6252,   115,   218, 15106, 17377,   235, 23812,   174])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3923 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2366,   153,   117, 10104,   325,   470,   532,  1736,   296,\n",
      "          211,   152,   184,  2758,   126,   128,   207,   461,   210,   331,\n",
      "          207,  6313,  8379,   210,   889,  6605,   213, 23812,   174,   110,\n",
      "          163,   759,   115,   253,   484,   221,  1377,   115,   264,   145,\n",
      "          602,   210,   347,   217,   207,   683,   210,   207, 10104,   325])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4943 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   404,   128,   233,   223,   969,   216,\n",
      "         3647, 28336,   173,   195,   356, 10899,   219,   765,   317,   207,\n",
      "         1099,   325,   212,   207,  5859,   534,   213,   631,   210,   207,\n",
      "         1099,   325,   110,   163,  1618,  1412, 14593,   235,   207,  4266,\n",
      "          210,   207,  5859,   534,   117,   237,   325,   216,  1072,   126])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4142 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   308,   115,   213,   665,   965,   210,   207,   958,  1016,\n",
      "          318,   213,   207,  7517, 15333,  3900, 15069,   325,   210,  1099,\n",
      "          115,   282,   150,   117,   147,   117,   147,   117,   162,   117,\n",
      "        25798,  4214,   115,   160,  3413,   111,  6391,   117,   205,   115,\n",
      "         1099,   112,   111,   162,   973,   116,   204,   116,   145, 12629])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4794 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2743,   216,   207,  1099,   325,   386,   227,  7542,   470,\n",
      "          242,   207, 10104,   325,   221,   145,   691,   210,   266,   115,\n",
      "          532,  1736,   211,   207,   461,   210,   331, 23812,   174,   249,\n",
      "         6605,   145,  1179,  5859,   388,   117,   126,   113,   113,   478,\n",
      "          128,   146,   117,   386, 23812,   174,   110,   163,   759,  2423])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4301 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  4778,  5550,   183, 10964,   117,   115,   521,   117,   166,\n",
      "          117,  4560,   117,   643,   109,   901,   535,   117,   115,  3472,\n",
      "          150,   117,   200,   174, 16139,   115, 22404,   111,   282,   475,\n",
      "          682,   117,  1020,   112,   111,  3189, 19995,   195,  5732,   115,\n",
      "          521,   117,   166,   117,  2595,  1522, 13603,   535,   117,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4872 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  4782, 18714,   110,   163,  1598,   211,   226,   388,  7168,\n",
      "          225,   145, 29637,   210,   207,  1754,  1551,  3497,  1092,   115,\n",
      "         3412,   235,   216,   105,  1754,  1551,   470,   116,   116,  2493,\n",
      "          225,   231,  3497,   189,   216, 23812,   174,  6418,   189,   213,\n",
      "         5867,   116,   116,  2086,   236,   207,  9916,   189,   210,  5859])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4525 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  4048,  2587,   145,   683,   115,   239,   759,   386,  4327,\n",
      "          216,  4782,  1896,  5026,  1463,   808,   211,   239,  1551, 14744,\n",
      "          212,   231,  1729,  1463,   808,   222,   480,   333,   117,   368,\n",
      "          221,   211,  4782, 18714,   110,   163,   450,  1151,   115,   532,\n",
      "         4085,   499,   216,   207,   256,   306,   229,  4782, 18714,  4906])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4281 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   243,  1056,   584,   210,   207,  1245,  5925,   189,   216,\n",
      "          468,   459,   207,   221,  8202,  8208,   235,  3197,   117,   221,\n",
      "         8202, 14820,   189,  8208,   235,   469,  1056,   207,  1567,   117,\n",
      "          207,   428,   918,   263,  1661,   270,   126,   113,   113,   909,\n",
      "          128,  1636,   145,   857, 10080,   115,  2977,  4333,   222,   241])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3995 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,  8714,   240,   545,   216,   728,  1599,   235,  3764,\n",
      "        13393,   213,   239,  1427,   115,   207,   595,   263,   763,   145,\n",
      "         1179,   388,   117,   447,   117,   236, 10881,   116,  1244,   111,\n",
      "          105,  3764, 13393,   503,   227,  1262,   331,  2581,   210,   207,\n",
      "         1099,   325,   115,   253,   946,   213,   145,   110, 16822,   195])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 516, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3003 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   331,   215,   227,  4782, 18714,   249,   220,   494,   569,\n",
      "          207,  4551,   907,   233,  1162,   115,   215,   331,   207,  7660,\n",
      "          317,   207,  4551,   212,  2458,   907,   223,   228,   216, 23812,\n",
      "          174,   212,   231,   297,   116,   219,   148,   189,   182,  2381,\n",
      "          189,   244, 23387,   174,   303,   116,   116,   392,   244,  1491])\n",
      "Original length: 1694 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,  2231,   212,   308, 18513, 19487,   184,   115,   154,\n",
      "          117,   979,   205,   115,   634,   207,   530,   256,  3335,   303,\n",
      "          210,   145,   915,   116,  3945,   316,   212,   384,  1059,   317,\n",
      "          428,  4320,  1547,   189,   210,   629,  2201,  5745,   338,   228,\n",
      "          221, 27034,   179,   188,   115,   145,  6814,   252, 21530,  2304])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4384 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1942,   280,  3503,   216,  1111,  3389,   249,   255,  2104,\n",
      "          213,   237,  1975,  7082,   210,  3427,  1016,  1733,   211,  7226,\n",
      "          212, 11608,   176,  1656,  1942,   126,   113,   200,   128,   280,\n",
      "          110,   163,  2732,   211,  4310,   225,  1111,  3389,   213,   207,\n",
      "          496,   210, 27034,   179,   188,   212,   231,   629,  2201,  5745])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3047 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   204,   128,   530,   301,   373,   207,   240,\n",
      "          223,   145,   867,   211,  2310,   207,   396,   759,   678,   218,\n",
      "          894,  1111,  3389,   212, 28887,  8048,  6089,   117,   200,   335,\n",
      "         3194,   216,   226,  1333,   223,  4570,   218,   207,   333,   210,\n",
      "          207,   906,   232,   212,   815,  1211,   218,   207,   276,   213])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3216 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   888,  2871,  2023,  1396,   112,   117,   213,   775,   115,\n",
      "          105,   207,   332,   212,  3960,  3497,   126,   356, 10899,   128,\n",
      "          219,  1672,   340,  7424,   126,   113,   282,   128,   221,   117,\n",
      "          117,   117,   211,  3190,  2416,   241,  4146,   491,   211, 22055,\n",
      "          547,   832,   211,  1088,  1273,   116,   815,   117,   105,   447])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3974 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   342,   128,   426,   117, 10104,   325,   248,\n",
      "          198,   152,   184,  2450,   126,   128,   248,   198,   210,   207,\n",
      "        10104,   325,   783,   119,   105,  1425,   438,   115,  1614,   213,\n",
      "          207,   432,   210,   559,   215,   378,   115,   215,  3854,   115,\n",
      "          213,  6124,   210,   616,   215,  2525,   967,   207,  1298,   265])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3190 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   444,  3956,   327,   223,   207,   354,   265,   115,\n",
      "          212,   207,   444,   424,   327,   223,   629,  2201,  2261,  5745,\n",
      "         5866,   115,   283, 27034,   179,   188,   117,   258,   321,  1824,\n",
      "          117,  7233,   117,   160,   186,   416,   115,   433,   115,  3141,\n",
      "          117,   126,   113,   422,   128,   152,   184,  4214,   126,   128])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4757 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   759,  1096,   211,   105,  3508,   207,  1373,   215, 29733,\n",
      "         1303,   105,   210,  3995,   338,   105,   212,   220,   444,  1477,\n",
      "          189,   213,  1222,   105,   112,   117,   207,   240,  3119,   216,\n",
      "          207,   396,   759,  2048,   211,  2423,  3822,   145,   444,   424,\n",
      "          327,   213,   635,   210,   207,   248,   198,   388,   117,  1942])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2382 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   706,   210,   339,   108,   210,  1111,  3389,   110,\n",
      "          163,   766,   346,   320,   117, 27362,   175,   110,   163,   832,\n",
      "          339,   108,   302,   213,  1111,  3389,   386,   227,  1868,   211,\n",
      "          219,  3985,   211,  6418,   207,  5119, 20137,   174,   525,   117,\n",
      "          368,   390,   115,   207,  3949,   216, 27362,   175,  2292,   126])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3519 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   242,   248,   199,   115,   221,   242,   248,   198,   115,\n",
      "          207,   444,   327,   223,   512,   218,   480, 11936,  1051,   212,\n",
      "         1687,   116,  8945,  3606,   210,  1222,   117,   447,   117,   115,\n",
      "          236,  5846,   158,   117,   422,   117,   552,   126,   113,   345,\n",
      "          128,   207,   240,   249,   765,   216,  1942,   280,   249,   227])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3667 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1111,  3389,  2042,   216,   552,  1942,   280,   223,  2159,\n",
      "          211,   264,   145,  1179,   522,  5859,   388,   115,  4196,   426,\n",
      "          111,  6124,   210,   616,   112,   212,  1420,   111,  3427,  1016,\n",
      "          112,   305,   323,   219,  1629,   117,   207,   240,   694,   117,\n",
      "         1942,   280,  3503,   216,   207,   872,  2189,   239, 10104,   325])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3387 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 13648,   189,  9174,  2311,   115,   230,   117,  3195,   117,\n",
      "          145,   117,  1191,   116,  3794,  2138,   115,  1094,   167,   182,\n",
      "         9064, 13129,   115,   113,   199,   111,   149,   117,   148,  2324,\n",
      "          117,  2618,   117,   201,   115,  1094,   112,   120,   126,   113,\n",
      "          984,   128,   117,  7065,  2519,   115,  5861,   145,   117,   199])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4622 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  4462,   504,   223,  2661,   211,   264,   145,   388,   217,\n",
      "         9624,  2287,   210,   736,   117,   321,   211,   174,   179,   166,\n",
      "          117, 25319, 12599,   115,   689,   165,   117,   163,   117,  6394,\n",
      "          117, 13648,   189,   655,  2758,   115,   230,   117,  3195,   117,\n",
      "          145,   117,   527,   116, 15285,   115,   689,   167,   182,  3978])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4021 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1028,   189,   105,   212,   105,   231,  2983, 13455,  2716,\n",
      "          316,  1389,   189,   105,   112,   120,  3360, 20539,  4595,   117,\n",
      "         6394,   117,   166,   117,  4265,  2786,  1687,   115,  9249,   150,\n",
      "          117,  1772,   117,  9278,   115,  8821,   111,   149,   117,   148,\n",
      "          117,  2324,   117,  1077,   112,   111,   388,  1629,   343,   759])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 240 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  200,  117,  207,  867,  223, 1463,  126,  113, 1048,  128, 4370,\n",
      "         221,  233, 2913,  145,  353, 2141,  513,  221,  211, 2481, 2014,  111,\n",
      "        6908, 3670,  325,  112,  117,  201,  117,  207,  595,  224,  219,  702,\n",
      "         211,  938,  237,  396,  759,  281, 4455,  116,  947,  111,  899,  112,\n",
      "         437,  117])\n",
      "Original length: 3539 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  8596,   128,  9292,   149,   117,  8761,\n",
      "          115,   881,   865,   119,   595,   116,  1421,   115,  8542,   184,\n",
      "          179, 20653,   184,   115,   521,   117,   111,   105,  8542,   184,\n",
      "          179,   105,   112,   115,  1409,   238,   207,   483,   240,   110,\n",
      "          163,  2957,   210,   239,  5859,   470,   351, 17058,   116, 12400])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4735 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207, 11628,  2295,   212,   966,  3854,   470,   117,   207,\n",
      "          483,   240,   614,   207,   867,   221,   211,   606,   470,   117,\n",
      "         1507,   115,  8542,   184,   179,   678,   145,   867,   211,  3179,\n",
      "          215,  1836,  1152,   371,   115,   229,   207,   483,   240,  1463,\n",
      "          117,   847,   210,   709,   152,   184,   866,   126,   128,   532])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4411 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1596,   216,   207,   105,  1016, 13053,   218,   207,  1693,\n",
      "         1539,   145,   110,  1436,   617,   210,   207,   444,   327,   117,\n",
      "          110,   105,   447,   117,   216,   223,   115,   105,   207,  3345,\n",
      "          217,   231,  3839,   189,   211,   824,   288,   215,  1253,   213,\n",
      "          216,   327,   311,   219,  1106,   301,   488,   117,   105,   447])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4597 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   316,  1312,   213,   207,   444,   424,   327,   211,   219,\n",
      "          633, 15846,   301,  1106,   117,   237,   839,  1281,   307,   145,\n",
      "         1849,  1224,   210,   316,  1312,   245,  2942,   221,   472, 15846,\n",
      "          301,  1106,   253,   207,   444,  1016,   213,   216,   599,   839,\n",
      "          223, 16392,   174,   238,  1176,  1016,  3837,   117,   199,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5247 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   245,   219,   614,   117,   321,   115,   149,   117,   151,\n",
      "          117,   115, 11064,  1148, 20337,   115,   521,   117,   166,   117,\n",
      "        16254,   890,   185,   110,   163, 20337,   115,   521,   117,   115,\n",
      "         3413,   150,   117,   200,   174,  5831,   115,  6499,   111,   200,\n",
      "          174,   682,   117,  1094,   112,   111,  9839,   207,   483,   240])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4821 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  213,  775,  115,  207,  483,  240,  765,  216, 8542,  184,  179,\n",
      "         110,  163, 1151,  246, 9365,  126,  113,  113,  441,  128,  552,  233,\n",
      "         115,  213,  363,  115, 4083,  216,  145,  430,  215,  706,  210,  359,\n",
      "         211,  229, 1498,  640,  189, 2326,  115,  245,  227, 1988, 1067,  338,\n",
      "         233,  261])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5307 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 27657,   207, 15669,   189,   212,   231,  1865,  1503,   105,\n",
      "          407,   213,   207,  1104,   117,   447,   117,   207,   483,   240,\n",
      "          763,   216,   392,  2148,   512,   207,   444,   327,   211,   185,\n",
      "        10668,   552, 17816,  1096,   211,   698,   213,   239,   759,   889,\n",
      "         6456,   216,   207,   327,   217, 17816,   110,   163,   877,   246])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4993 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   427,   207,   338,   210,   207,   712,   216,   249,  1013,\n",
      "          207,  1382,   319,   211,  1240,   207,   424,   281,   207,   599,\n",
      "         1801,   215,   839,   117,  8542,   184,   179,   435,  2042,   216,\n",
      "          115,   221,   145,  2946,   691,   115,   207,  1877,   210, 11936,\n",
      "         1051,   212,  1687,   116,  8945,   658,   210,  1222,   244,   210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5464 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  5531,   126,   128,   207,  1754,  1877,   210,\n",
      "          145, 11628,  2295,   225,   237,   810,   316,   126,   113,  9109,\n",
      "          128,  1389,   602,   210,   347,   244,   111,   198,   112,  2093,\n",
      "          872,  3030,   314,  3483,   215,  6665,   120,   111,   199,   112,\n",
      "          225,  1870,   211,  2984,   120,   212,   111,   200,   112,  1035])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4700 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  152,  184, 3875,  126,  128,  211, 2587,  145,  602,  210,  347,\n",
      "         217,  966, 3854,  242, 2348,  266,  115,  145,  595,  311, 1204,  207,\n",
      "         298, 1877,  119,  105,  111,  198,  112,  428,  215,  353,  652,  120,\n",
      "         111,  199,  112,  237, 2412,  211,  219, 5309,  252,  120,  111,  200,\n",
      "         112,  145])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4187 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  532,  753,  216,  226,  256,  386,  227, 3217,  207, 2030,  210,\n",
      "        7012,  329,  211,  725,  145,  105, 5508,  195,  105,  242,  207, 2348,\n",
      "        2652,  117,  532,  420, 9318,  207,  483,  240,  110,  163, 1113,  222,\n",
      "         226,  550,  117,  213,  775,  211, 1408,  216, 8542,  184,  179,  249,\n",
      "        1096,  211])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 9 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 3382,  117,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 618 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  7463,   128,  3501,   115,   483,   865,\n",
      "          226,   691,  4647,   373,   207,   240,   306,   207,   867,   210,\n",
      "         9248,   217,   126,   113,   113,   199,   128,  2957,   210,   126,\n",
      "          113,  8971,   128,   158, 23087, 18704,   110,   163,  5859,  3757,\n",
      "          212,   393,   257,   759,   115,   212,   306,   158, 23087, 18704])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4367 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   221,   233,  6373,   221,   207,   421,   217,   207,  3107,\n",
      "         1641,   212,   867,  1178,   115,   207,   240,   311,   296,   709,\n",
      "          207,  3176,   212,   592,   210,   207,  7517, 15333,  3900, 15069,\n",
      "          325,   210,  1099,   111,   105,  1099,   325,   105,   215,   105,\n",
      "          207,   325,   105,   112,   115,   229,   396,   207,  1798,   325])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4500 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,   207,   367,  2231,   212,   308,   210,   207, 11066,\n",
      "          175, 12314,   149,   117,  7065,   115,   165,   117,   163,   117,\n",
      "          148,   117,   154,   117,   115,   207,   240,  1266,   216,   158,\n",
      "        23087, 18704,   263,  2423, 11433,   207,  1877,   210,   145,   248,\n",
      "          199, 10104,   325,   388,   217,  2287,   210,  5508,   195,   643])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5275 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1512,   105,   314,  3483,   212,  3615,   117,   105,   447,\n",
      "          117,   158, 23087, 18704,  2421,   216,   207,   879,   308,  1136,\n",
      "          223,   227,  1280,  8235,   213,   207,   438,   317,   207,   276,\n",
      "          115,   212,   223,   307,  1419,  1646,   288,   324,   608,   117,\n",
      "          321,   447,   117,   160,   774,   117,   158, 23087, 18704,  1069])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4047 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   894,   261,   462,   117,   105,   447,   117,   207,   952,\n",
      "          323,  2688,   216, 10336,   195,   212,  7277,  2112,   189,   244,\n",
      "          227,   633,  4033,   210,   207,  6013,   608,   117,   321,   447,\n",
      "          117,   160,  1368,   117,   487,   222,   392,  2148,   115,   158,\n",
      "        23087, 18704,  4142,   470,   217,  2287,   210,  5508,   195,   643])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4618 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  152,  184, 2138,  126,  128,  242,  525,  279,  111,  146,  112,\n",
      "         111,  203,  112,  115,  207,  240,  311,  105, 1599,  207, 2148,  213,\n",
      "         207,  759,  221, 1377,  115,  212, 2588,  241,  480, 2238, 6295,  213,\n",
      "        2009,  210,  207,  595,  117,  126,  207,  867,  356,  219,  614,  128,\n",
      "         307,  253])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4300 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,  5859,   534,   311,   227,   219,   740,   211,  1960,\n",
      "          357,   216,   292,   307,  1920,   218,  1393,   599,  1251,  1053,\n",
      "          117,   321,   447,   117,   158, 23087, 18704,  3618,   189,   296,\n",
      "          216,   145,   867,   211,  2310,   207,  5859,   470,   249,  1078,\n",
      "          255,  1287,   218,   207, 11066,   175, 12314,   149,   117,  7065])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5102 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   470,   244,   227,   487,  1199,   306,   207,   770,  1146,\n",
      "          218,   207,  1099,   325,   115,   212,   216,   207,  2148,   244,\n",
      "          885,   211,   264,   145, 10104,   325,   388,   552,  9248,  2104,\n",
      "          213,   336,  1395,   116,  2010,   818,   117,   126,   113,   113,\n",
      "          556,   128,  9248,  1125,   207,  3107,   867,   487,   306,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4955 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  770,  329,  210,  207,  153, 1656,  173,  242,  207,  325,  115,\n",
      "         212,  420,  481,  227, 1915,  548,  242,  207,  336, 5859,  409,  117,\n",
      "         321,  447,  117,  323, 3531, 1287,  115,  223, 2639,  184, 4294,  115,\n",
      "        5631,  150,  117,  200,  174,  236, 6016,  116,  345,  115,  213,  229,\n",
      "         207,  450])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5220 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1280,   115,   207,   396,   759,   763,   145,   388,   242,\n",
      "          152,   184,  4214,   126,   128,   207,  1754,  1551,  3497,   115,\n",
      "          229,  1379,   145, 28840,   211,   851,   808,   211,   369,   877,\n",
      "          340,   216,  2381,   189,   244,  4589,   175,   211,  4310,   213,\n",
      "          207,   327,   117,   321,   447,   117,   207,   820,   323,  3859])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4786 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   245,   264,   237,  5859,   388,   351,   207,   153,  1656,\n",
      "          173,   225,   229,   233,   249,  3506,   237,  6013,   232,   117,\n",
      "          390,   115,   221,   261,   219,  3124,   115,   207,   889,   832,\n",
      "          213, 23812,   174,   110,   163,   759,   244, 10300,   238,   207,\n",
      "         2148,  1063, 29310,   117,   152,   184,  4290,   126,   128,   226])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4426 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   145,  1686,  3868,   210,   158, 23087, 18704,   110,   163,\n",
      "         2148,  6650,   216,   239,   470,  6106,   307,   238,  9248,   110,\n",
      "          163,   579,   242,   207,  6013,   608,   487,   222,   207,  1099,\n",
      "          325,   117,   207,  3757,  6539,   189,   158, 23087, 18704,   110,\n",
      "          163,  4781, 22879,   225,  9248,   110,   163,   579,   115,   310])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4661 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5996,   184, 14045,  1522,   166,   117,  4143,  1129,   535,\n",
      "          117,   115, 11344,   150,   117,   199,   174,  6692,   115,  6553,\n",
      "          111,   201,   475,   682,   117,  1167,   112,   117,   126,   113,\n",
      "          113,  1048,   128,   158, 23087, 18704,   386,   227,  1115,   220,\n",
      "          470,   213,   635,   210,   239,   264,   266,  5859,   470,   216])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4084 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   388,   242,   248,  4997,   210,   207,  1099,   325,   117,\n",
      "          203,   420,   115,   820,   110,   867,   211,  2310,   207,  3757,\n",
      "          126,   113,   113,  1133,   128,   223,   614,   221,   211,   158,\n",
      "        23087, 18704,   110,   163,  7517, 15333,  3900, 15069,   325,   388,\n",
      "          117,   200,   117,   158, 23087, 18704,   110,   163,   264,   266])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4089 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   321,   447,   117,   158, 23087, 18704,  1069,   189,   216,\n",
      "          552,  9248,   249,   227,   832,  6063,  1315,  1485,   210,   207,\n",
      "          438,   115,   233,   356, 10899,  2617,   222,   145,  8297,   116,\n",
      "         2143,  3880,   117,   321,   447,   117,  9248,  3618,   189,   216,\n",
      "          241,   210,   207,   820,   244,   685,   210,  9248,   126,   113])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 516, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 31 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 18189,  6808,   149,   117,  3501,   115,  3461,   117,   115,\n",
      "          165,   117,   163,   117,   148,   117,   154,   117,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 1403 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  7833,   128,  2231,   528,   212,   308,\n",
      "          226,   256,   223,   373,   207,   240,   222,   207,   894,   110,\n",
      "          867,   211,  2990,   207,  1743,   210,  3097,   146,   117,  6849,\n",
      "        16632,   115,  1784,   207,   820,   247,  1188,   221,   237,  1909,\n",
      "         1519,   211,  5312,   211,   444,  1000,   213,   226,   256,   117])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4442 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 24200,   189,  2064,  1007,  8282,   213, 10470,   115, 13155,\n",
      "          115,  9684,  4986,   110,   163,   115, 11192,   188,   190,   115,\n",
      "         4829,   115,   212,   719,   117,  7750,   110,   163,  2481,   547,\n",
      "          117,  2882,  1410,   223,   145,  1278,   469,   229,  3704,  1166,\n",
      "          212,   380, 15889,   189,   145,   105,   289,  1919,   118,   289])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3272 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   210, 12866, 10648,  4684,   190,  1798,   117,  6849, 16632,\n",
      "          323,   249, 15504,  6441,   190,  2644,   636,   189,   212,  5935,\n",
      "          418,   212,  2960,   235,  1694,   189,   236,  1338,  5075,   189,\n",
      "          212, 10096,   547,  2321,   275,  6102,   117,   290,   117,   145,\n",
      "         2259,  3176,   210,  6849, 16632,   110,   163,   444,   327,   528])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4206 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2318,   226,  1113,   213,   275,   296,  3629,   115,   300,\n",
      "         2494,   213,   226,   450,  3629,   216,   115,   626,   275,   296,\n",
      "          428,  9290,   115,   300,   263,  2950,  3865,  4972, 13245,   212,\n",
      "         3170,   642,   391,  2779,   213,   226,   256,   117,   426,   117,\n",
      "         1327,   217, 10106,  1909,  1743,   242,   522,   525,   210,   460])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5050 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5859,   212,   216,   207,   954,  1909,   110,   163,  1743,\n",
      "          246, 11740,  8105,  4333,   466,   211, 18644, 10101,   189,   117,\n",
      "          321,   447,   117,   236,  8333,   116,   774,   115,  9604,   116,\n",
      "          673,   117,   207,   820,   111,  2381,   189,   210,   207,   588,\n",
      "         4056,   243,   112,  6047,   222,   264,   212,   522,  5859,  1030])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4842 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   971,   115,  1909,  1743,   213,   336,   115,   212,   145,\n",
      "         1480,   110,   163,  1952,   189,   213,   382,   115,   323,   311,\n",
      "          219,   865,   174,   213,   901,   210,   207,  1498,   217,   207,\n",
      "         5399,   210,   482,   211,  1599, 10210,   301,   207,  1743,   210,\n",
      "          145,  1480,   443,   207,   240,   249,  1519,   221,   237,  1909])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5043 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   820,  3456,   216,  6849, 16632,   249, 15504,  6441,   190,\n",
      "          105,  2644,   636,   189,   105,   236,  1338,  5075,   189,   115,\n",
      "        10096,   547,   115,   215,  1882,  6272,   210,   718,  5960,   117,\n",
      "          330,   223,   115,   390,   115,   230,   460,   216,   228,   971,\n",
      "          189,  1325,  1355,   210,   444,  1472,   217,  5859,   504,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3736 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6158,   292,  8539,   174,   225,   237,  5210,  6216,  1455,\n",
      "          444,   327,   217,  5859,   504,   117,   199,   211,   207,   464,\n",
      "          216,  6849, 16632,  2022,   222,   327,  1261,   946,   218,   894,\n",
      "          215,   768,   218,   207,   894,   948,   270, 10756,   189,   210,\n",
      "         1016,   115,   327,   115,   212,   207,  1486,   115,   330,   223])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1285 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2321,   275,  6102,   117,   392,  1952,   189,  3190,   700,\n",
      "         2024,   242,  1954,   117,   162,   117, 12548,   117,  7627,   211,\n",
      "         1115,   528,   460,   222,   336,   365,  1000,   210,  5935,   316,\n",
      "          115,   283,   726,   252,  2606,   117,   207,   894,  4000,   270,\n",
      "         2533,   222,  6849, 16632,   110,   163,  4354,   213,  5169,   275])\n",
      "Original length: 118 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  528, 2231,  212,  308,  665,  119, 1687,  116, 3883,  217, 1152,\n",
      "         371,  111,  332,  428,  119,  665,  119,  142,  198,  558, 1131,  109,\n",
      "        4805,  116, 3212,  548,  112,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 199, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4232 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  153,  117, 2500,  226,  691,  223,  373,  207,  240,  222,  207,\n",
      "         276,  110, 1687,  116, 3883,  217, 1152,  371,  117,  207, 3883, 3296,\n",
      "         222, 4024,  217,  886,  222,  463,  258,  115,  689,  117,  222,  245,\n",
      "         441,  115,  634,  115,  532,  561,  237,  308,  225,  267,  211,  820,\n",
      "         110, 1566])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 200, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4596 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2080,   115,   110,   310,   335,   244,   227,   558,  1131,\n",
      "          213,   683,   210,   207, 10104,   325,   117,   105,   447,   117,\n",
      "         3420,   857,  5975,  3275,  5540,   608,   222,   907,   126,   113,\n",
      "          203,   128,   212,  3499,   221,   705,   221,  1265,   222,  1016,\n",
      "          115,  1951,   465,   227,  2488,   207,  2323,   351,   405,  2080])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3965 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5483,  9043,   247,  2747,   270,  3990,   110,   763,  5219,\n",
      "          117,   222,   239,  2535,   207,   184,   115,   207,  5220,   460,\n",
      "         2777,  1498,   212,  5477, 24621,   547,  3373,   281,  2006,   212,\n",
      "         2755,   174,  3034,   117,   820,  4293,   216,  1425,  1614,   115,\n",
      "          331,  3420,   215,  1395, 13408,   115,  8825,   189,   828,  3576])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3689 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  284,  907,  972,  301,  117, 1951,  335,  212, 1425,  231,  857,\n",
      "        2168,  311,  115,  236,  805,  564,  115,  284,  907,  217,  207,  338,\n",
      "         335, 1240,  117,  321,  146, 3905,  115, 7212,  165,  117,  163,  117,\n",
      "         236,  206,  117,  552, 3677,  220,  405, 1946, 1539,  405, 2080,  242,\n",
      "         820,  110])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4350 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   829,   402,   477,   174,   391,  2399,   217,   389,  7569,\n",
      "          189,   117,   447,   117,   160,  5531,   117,   809,   820,  3194,\n",
      "          894,   311,   247,   263,   237,  1395, 13408,   592,   117,   467,\n",
      "          894,   110,  1964,   115,   207,   817,   210,   207,  1129,  1104,\n",
      "          115,   212,   207, 12682,  8876,   210,   207,   845,  5975,   115])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4118 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 8088,  110,  163, 2053,  211, 4310,  351, 7705,  182,  383,  626,\n",
      "        8088,  545,  230,  365, 5021,  213, 7705,  182,  383,  117,  467,  207,\n",
      "        1345,  210,  220,  365,  445,  317, 8088,  212, 7705,  182,  383,  115,\n",
      "         237,  232, 2876,  235, 8088,  110,  163, 2053,  211, 4310,  213,  207,\n",
      "        3542,  354])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3857 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  231,  268,  218, 1826,  582,  115, 7705,  182,  383,  212, 5483,\n",
      "        9043,  356,  307,  219, 7483,  293,  947,  637,  115,  440,  428,  637,\n",
      "         293,  326,  117,  447,  117,  160, 6634,  117,  145,  947,  116,  398,\n",
      "         998,  225,  428,  116,  637,  326,  297, 1451,  894,  211, 3620, 1613,\n",
      "         225, 7705])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3853 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  160, 4605,  115,  820,  462,  230,  460,  216,  207,  418,  210,\n",
      "        3669,  207, 6572,  115,  331,  497,  211, 1865,  664,  115, 1110,  115,\n",
      "         215, 2796,  115,  297, 1761,  237, 3420, 2168,  211,  284,  870,  907,\n",
      "         217, 4143,  212, 5567,  171, 1517,  126,  113,  508,  128, 9203, 8954,\n",
      "         117,  426])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3391 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1190,   120,   126,   233,   128,  1640,   217,   405,  2080,\n",
      "          221,   145,   358,   210,   105,   405, 21096,   860,   105,   120,\n",
      "          212,   126,   233,   128,   246,  4024,  2950,   225,   207,   749,\n",
      "         6515,   210,   606,  7705,   182,   383,   212,  5483,  9043,   117,\n",
      "         2945,   189,   117,   110,   889,   160,  9942,   117,   820,  2664])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4454 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   293,  5334,  4624,   212,  2793,   820,   110,   460,   212,\n",
      "         1689,   115,   212,  2597,   241,   480,   126,   113,   774,   128,\n",
      "         6295,   213,   820,   110,  2009,   115,   532,  5468,  2283,   145,\n",
      "          480, 11003,   210,   482,   481,   227,  1800,   216,   894,  3506,\n",
      "         7705,   182,   383,   212,  5483,  9043,  2001,   211,  2574,   237])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2140 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   211,   207,   464,   231,   578,  3606,   245,  2086,   317,\n",
      "          207,  2168,   213,  4287,   212,  7705,   182,   383,   212,  5483,\n",
      "         9043,   115,   335,   465,   227,  2636,   558,  1131,  5408,   117,\n",
      "          126,   113,   909,   128,  4287,   503,   227, 14934,   222,   239,\n",
      "          142,   198,   558,  1131,  1355,   115,   212,   820,  5343,   230])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1270 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   226,   691,   223,   373,   207,   240,   222,   595,\n",
      "        11750,   195,   469,   110,   163,   867,   211,  2310,   894,   110,\n",
      "         5060,   346,   266,  3427,  1016,  3757,   294,   211,   525,   279,\n",
      "          111,   146,   112,   111,   203,   112,   212,   217,  1182,  6169,\n",
      "          212,  2549,   210,   411,  2779,   294,   211,  1954,   117,   162])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2900 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 23949,  1484,   115, 10269,   117,   115,  3816,   150,   117,\n",
      "          200,   174,  5802,   115,  7673,   111,   203,   475,   682,   117,\n",
      "          722,   112,   126,   113,   201,   128,   111,  5453,  1396,   112,\n",
      "          117,   207,   409,  1309,   960,   307,   145,   105,  1739,   212,\n",
      "         3647,   513,   210,   207,   388,   105,   212,   227,  1369,  2148])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4153 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   347,   120,   111,   200,   112, 27538,   174,   207,  1282,\n",
      "          347,   211,   162,   109,   163,  2977,  3009,   110,   163,  1035,\n",
      "          212,  1498,  1636,   120,   212,   111,   201,   112,  3446,   162,\n",
      "          109,   163,  2977,  3009,   110,   163,  1035,   212,  1498,  1636,\n",
      "          216,   162,   109,   163,  2977,  3009,   105,   261,   219,  2159])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4983 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2834,   221,   218,  5867, 30518,   275,   737,   221,   324,\n",
      "          210,   789,   100,   117,   330,   245,   219,   323,  2295,   218,\n",
      "         3625,   145, 25466,   195,   210,  3728,   117,   207,  3625,   210,\n",
      "          145, 25466,   195,   210,  3728,   115,  4305,   227,   213,   667,\n",
      "         1270,   115,   310,   217,   207,   592,   210, 19670,   235,   207])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3658 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   322,   211,  4317,   207,   316,   210,   789,   218,   220,\n",
      "          358,   216,  3684,  2067,   215, 19021,   115,   145,   240,   210,\n",
      "          800,   261,  1839,   237, 10348,  3839,   212,  7349,   145,  9842,\n",
      "          289,   238,  2325,   303,   275,  1286,   117, 24481,   577,  8337,\n",
      "          115,  5003,   158,   117,   167,   117,   236,  3434,   117,   204])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4488 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   162,   109,   163,  2977,  3009,   110,   163,  5859,  3757,\n",
      "          189,   244,  1280, 14239,   222,   145,  1408,   216, 11750,   195,\n",
      "          110,   163,  2038,   213,   461,   244,  2151,   212,  2491,   626,\n",
      "          105,  5859,   534,   465,   227, 12942,   145, 11831,   110,   163,\n",
      "          319,   211,  2990,  1285,   238,  1023,   359,   117,   105,   213])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4057 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1063, 18826,   207,   408,   663,  2189,  2038,   117,   447,\n",
      "          117,   111,  3284, 11763,   383,  3181,   535,   117,   166,   117,\n",
      "          151,   117,   163,   117,  1772, 13175,   535,   117,   115,  5440,\n",
      "          165,   117,   163,   117,  7634,   115,  8014,   116,   962,   115,\n",
      "         1288,   156,   117,   149,   174,   117,  5699,   115,  1342,   163])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3393 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  207,  240, 3448,  216,  228,  145,  971,  223, 1432,  213,  226,\n",
      "         691,  217, 1298,  853,  117,  217,  289,  115,  233,  261, 4224, 6480,\n",
      "         317,  207,  276,  802, 1067,  223,  212,  223,  227,  497,  211,  229,\n",
      "         470,  212,  809, 1067, 2779,  223,  702,  936,  212, 1067, 2779,  249,\n",
      "         255, 5589])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 269 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   233,   223,   435,  1677,   216,   207,   276,   126,   113,\n",
      "          345,   128,   244,   211,  1919,  4515,   865, 17317,   163,   117,\n",
      "         1645,  5437,   174,   195,   281,   201,   437,   210,   207,   254,\n",
      "          210,   226,   308,   211,   610,   145,  1304,  3163,   213,   226,\n",
      "          691,   117,   777,   213, 27586, 18251,   115,  3929,   119,  6391])\n",
      "Original length: 2472 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  9208,   128,  2231,   528,   207,   820,\n",
      "          213,   226,   256,   678,   145,  3458,   116,  2481,   759,   115,\n",
      "         3187,  2581,   210,   207, 10104,   325,   115,   207,  8913,  1386,\n",
      "          184,   325,   115,   207,  2467,   325,   115,   212,   207,  4591,\n",
      "         5859,   325,   115,   212,   264,   266,   470,   217,  3427,  1016])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5119 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  9714,  5183, 20234,   126,   113,   113,   202,   128,  2882,\n",
      "         1410,   115,   521,   117,   111,  2882,  1410,   215,   207,  1410,\n",
      "          112,   115,   223,   145,  1278,   469,   115,  3568,  1796,   210,\n",
      "          229,   223,  1056,   218,  8938,   117,  2882,  1410,  3704,  1166,\n",
      "          212,   380, 15889,   189,   145,   105,   289,  1919,   118,   289])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4707 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   462,   145,  2376,   115,  2882,  1410,   804, 16764,   189,\n",
      "         3252,  8938,   110,   163,  1279,   189,   117,   623,   211,   207,\n",
      "          894,   115, 15889,   189,  1155,   392,  1279,   189,   217,   270,\n",
      "         3638,  3512,   115, 13076,   115,  5630,   115,  5098,  1050,   115,\n",
      "          212,  1974,  2796,   453,   117,   207,   820,  1115,   145,   870])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5237 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   198,   117,   424,   327,   213,   260,   210,   270,  5859,\n",
      "          470,   115,   207,   152,   184,   973,   126,   128,   820,  1769,\n",
      "          207,  1874,   210,  5775,   444,   327,   117,  5575,  3332,   109,\n",
      "        11835,   117,  3520,   117,   115,   521,   117,   166,   117,  6851,\n",
      "          110,   156,  3230,  5938,   210,  3322,   117,   115,   126,   113])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3690 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   487,   222,   599,   889,  3442,   211,   207,   954,   327,\n",
      "          117,   200,   207,   820,   110,   954,  1909,   213,   226,   256,\n",
      "          115,  3097,   146,   117,  6849, 16632,   115,   223,   227,  1519,\n",
      "          211,  1115,   275,   528,   221,   211,   207,   444,   424,   215,\n",
      "         3956,  1472,   115,   696,   223,   300,  1519,   211,  5312,   221])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4685 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   820,   323,  5343,  1298,   649,   212,   636,   727,\n",
      "          216, 10019,  2238,  1601,   221,   211,   207,   464,   211,   229,\n",
      "         8282,  4310,   225,   231,  2644,  2691,   217,  2796,  1046,   189,\n",
      "          117,   392,  1601,   244,   323,  4548,   117,   330,   223,   230,\n",
      "          460,   216,   220,   210,   392,   649,   215,   727,  4543,   225])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4861 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   238,   228,  1410,   218,   207,  1329,  2370,   182,   212,\n",
      "          231,   972,  8282,   115,   340,   221,   211,  5913,   459,  8938,\n",
      "          110,   163,   327,   643,   218,  7372,   235,   211,   233,  2796,\n",
      "         1198,   212,  1046,   189,   207,   972,  1513,   189,   263,   373,\n",
      "          207, 22055,   547,   117,   117,   117,   292,  2534,   117,   105])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5008 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  221,  460,  216, 2882, 1410, 3350,  351,  239,  639,  695,  115,\n",
      "         207,  820,  564,  303,  216,  213, 1094,  115, 2882, 1410,  110,  163,\n",
      "        1320, 1254, 2629,  239, 2438,  115, 8938,  212, 4586, 9642,  175,  182,\n",
      "         115,  145,  202,  108, 2632,  222,  250,  189,  115, 1318,  213,  145,\n",
      "         105,  826])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4297 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   534,   292,  3740,   217,   207,   644,   210,  1016,   115,\n",
      "          227,  2381,   189,   117,   110,   105,  8313,   175,   540,  1392,\n",
      "          117,   166,   117,  3501,   109, 18153,  3706,  1610,   117,   115,\n",
      "         7107,   165,   117,   163,   117,  4494,   115,  4692,   115,  2531,\n",
      "          156,   117,   149,   174,   117,   199,   174,  4076,   115,  2623])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5470 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   820,   356, 10899,  2617,   211,   729,   222,   392,\n",
      "          470,   115,   552,   152,   184,  4564,   126,   128,   145,  2695,\n",
      "         1121,   217,   548,   242,   248,   199,   223,   216,   207,  5859,\n",
      "          595,   311,   219,  4589,   175,   211,  2587,   145,   444,   327,\n",
      "          216,   207,   588,  3750,   211, 24369,   117, 21265,   115,  1392])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5332 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  216,  126,  145,  588,  128,  223,  227, 1922,  239,  639,  710,\n",
      "         297,  635,  237, 5827,  216,  894,  110, 2960,  235, 1302,  292, 1733,\n",
      "         211, 3414,  126, 2381,  189,  128,  303,  210,  207,  327,  117,  105,\n",
      "         447,  117, 2641, 4714,  235, 1364,  223,  227,  145,  105, 2960,  235,\n",
      "         663,  117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4221 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5343,   207,  4972,   210,  6424, 10384,   115,   145,  1771,\n",
      "          707,   210,   804,   212,  1110,   217,   207,  1273,   117, 10384,\n",
      "         2522,   216,   300,  2982,   449,  3280,   217,  2796,   216,  2147,\n",
      "          145,   828,  1527,   115,   211,   219,   484,   288,   366,   372,\n",
      "         1946,   237,  2796,   449,   117,   623,   211,  4812,   115,   221])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5141 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   293,  2779,   115,   207,   820, 14245,   216,   270,   405,\n",
      "         1669,   388,  2448,   211,  5935,  2796,   115,   212,   216,   220,\n",
      "         1249,   217,   820,   222,   226,  2481,   311,  2326,   242,   152,\n",
      "          184,  3875,   126,   128,  4591,   266,   229,   386,   227,  4460,\n",
      "          317,   105,  4672,   105,   212,   105,   446,   105,   225,   374])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 516, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4438 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  592,  117,  226,  223,  227, 3985,  211, 2617,  211,  729,  222,\n",
      "        3427, 1016, 1030,  115, 2872,  213,  901,  210,  207,  240,  110,  163,\n",
      "        2957,  210,  241,  210,  207,  820,  110, 5859,  470,  115,  126,  113,\n",
      "         113, 1291,  128,  212,  207, 5691, 1382, 4998,  352,  210,  226,  602,\n",
      "         210,  347])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 498, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5053 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   210,   347,  4097,   189,   117,   157,   174,   117,   473,\n",
      "          237,   184,   117,   115,   756,   189,   117,   109,  9416,   117,\n",
      "         9597,   117,   142,   202,   116,  1973,   111,  1020,   112,   117,\n",
      "          145,   681,   210,   438,   388,  4097,   189,   372,   207,   438,\n",
      "          223,  4414,   215, 18895,  1242,  8595,  4414,   117,   207, 15849])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 442, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2867 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   227,   294,   211,   894,   110,  2542,   316,   504,   117,\n",
      "          207,   820,   356, 10899,  1267,  3593,   548,  2641,   222,   207,\n",
      "         1300,   216,   207,  1273,   215,  8938,  1351,   603,  4397,   854,\n",
      "          126,   113,   113,  1342,   128,   218,  7805,   235,  4238,   411,\n",
      "        15889,   189,   117,   213,  1739,   115,   207,   820,   110,   336])\n",
      "Original length: 89 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   218,   119,  3125,   162,   117,  7065,   528,  4244,\n",
      "          150,   117,   200,   174,  9545,   115,   113,  9545,   120,   634,\n",
      "          165,   117,   163,   117,  2153,   117, 13648,   189,  3792,  6897,\n",
      "          115,   113,   113,   198,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4139 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113, 10604,   128,  7065,   115,   881,   865,   117,\n",
      "          820, 27321,   185,  3198,   189,   212,   239,   706,  1872,  9748,\n",
      "        23443,   111,  1462,   105, 27321,   185,   105,   112,  1125,  1333,\n",
      "          213,   207,  3087,   483,   210,  2015,   351,   207,   354,   265,\n",
      "         4138,   446,   117, 27321,   185,  2621,   145,   394,   210,   522])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4679 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   202,   128,   126,   199,   128,   213,\n",
      "         6938,   166,   117, 10824,   115,  5351,   165,   117,   163,   117,\n",
      "         6631,   115,  7352,   115,  3340,   156,   117,   149,   174,   117,\n",
      "          199,   174,  5209,   115,  2938,   163,   117,   756,   117, 10834,\n",
      "          111,  1077,   112,   115,   152,   184,   973,   126,   128,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4905 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,  1077,   115,   207,   240,  2591,   211,   207,  4691,\n",
      "          116,   212,   116,   219,   116,  6047,   550,   213, 10824,   115,\n",
      "         5351,   165,   117,   163,   117,  6631,   115,  3340,   156,   117,\n",
      "          149,   174,   117,   199,   174,  5209,   115,  2938,   163,   117,\n",
      "          756,   117, 10834,   117,   330,   115,   207,   595,  2774,   252])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4209 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3261,   165,   117,   163,   117,  2153,   117,   148,   117,\n",
      "          147,   117,  5125,   115,  6218,   150,   117,   199,   174,  5363,\n",
      "          115,  5611,   111,   148,   117,   147,   117,   682,   117,  1755,\n",
      "          112,   115,   207,   184,   116,   865, 19647,  5673,  6175,   217,\n",
      "          207,   240,   213,  1108,   216,   105,  2353,   503,   227,   669])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3853 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  447,  117,  236, 4587,  120,  201, 5208, 1954,  117, 1618, 1610,\n",
      "         117,  166,  117,  354,  265, 4138, 6733,  117,  115, 3923,  150,  117,\n",
      "         200,  174, 7790,  115, 7241,  116, 1100,  111,  203,  475,  682,  117,\n",
      "        1020,  112,  120,  354,  265,  166,  117,  161,  213,  190,  110,  156,\n",
      "        3924,  115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3930 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   508,   165,   117,   163,   117,   147,   117,   142, 20442,\n",
      "          111,   146,   112,   111,   198,   112,   117,   145,  3347,  7753,\n",
      "          454,   213,   207,  7723,   171,  1454,   483,   240,   585,   242,\n",
      "          142, 20442,   111,   146,   112,   111,   198,   112,   222,   609,\n",
      "          198,   115,   689,   120,   390,   115,   145,  3576,   454,   265])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4592 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  207, 2015,  316,  109, 4866,  189,  473,  142, 3454, 3384,  388,\n",
      "         207, 4138,  446, 2042,  216,  207,  483,  240, 6357, 2333,  585,  569,\n",
      "         126,  113,  113,  442,  128,  207, 2015,  316,  109, 4866,  189,  473,\n",
      "         142, 3454, 3384,  388,  212,  115, 1623,  301,  115,  216,  207,  388,\n",
      "         223, 7542])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3620 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3196,  7869,   109,   145, 14322,  6018,  8203,   189,   115,\n",
      "          521,   117,   166,   117,  4104,   115,  6353,   165,   117,   163,\n",
      "          117,  3725,   115,  3794,   116,  1027,   115,   258,   156,   117,\n",
      "          149,   174,   117,   199,   174,  4572,   115,  1348,   163,   117,\n",
      "          756,   117, 14254,   111,  4734,   112,   115,   212, 18664,   166])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1329 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   552,  3605,   223,  1549,   213,   207,   126,   113,   113,\n",
      "          846,   128,  3087,   483,   210,  2015,   217,   207,  5859,   470,\n",
      "          115,   212,   552,   207,  3607,  2976,   115,   142,  3454,  3384,\n",
      "          115,   212,   142,  1577,   470,  2140,   303,   210,   207,   376,\n",
      "          346, 14509,  1264,   210,   889,   115,  3605,   213,   207,  3087])\n",
      "Original length: 1647 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  528,  126,  113, 9464,  128,  308,  212,  371,  113,  226,  347,\n",
      "        5185,  303,  210,  207,  474,  210,  428, 1182,  952,  119,  145,  804,\n",
      "        3149,  126,  113, 9736,  128,  232,  317,  145, 1547,  210, 2656,  235,\n",
      "         212, 1180, 6775,  877,  115, 1985,  847,  115,  521,  117,  115,  148,\n",
      "         118,  146])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4486 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 20804,   232,   246, 13520,   452,   105,   218,   591,   257,\n",
      "          115,   225,   215,   314,   602,   115,   306,   456,   437,   436,\n",
      "          326,   117,   105,   242, 15373,   175,   110,   163, 20804,   608,\n",
      "          115,   283,   324,   225,   152,   190,   189,   171,   115,  4850,\n",
      "         6011,   981,  1668,   338,   238, 15373,   175,   236,   145,   284])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4337 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   388,   217,   681,   210,   239, 20804,   232,   120,   111,\n",
      "          200,   112,  3738,   207,   371,   217,   152,   190,   189,   171,\n",
      "          222, 15373,   175,   110,   163,  2067,  3757,   120,   212,   111,\n",
      "          201,   112,  3738,   207,  2957,   210, 15373,   175,   110,   163,\n",
      "         3757,   189,   217,  8437,  6750,  1346,   212,   237,  1320,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4351 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   221,   213,   231,   265,   115,   446,   952,   245,   219,\n",
      "        13520,   452,   236,   261,   621,   207,   232,  6539,   189,   378,\n",
      "          215,   621,   805,   599,  1119,  1378,   216,  2791,   525,   117,\n",
      "          105,   112,   117, 17498,  9959,   189,  1143,   211,  1915,   237,\n",
      "         1887,   211,   226,   525,   217,  3149,   608,   117,   213,   635])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4255 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 4323, 7573,  117,  199,  174, 6118,  115, 7178,  158,  117,  167,\n",
      "         117,  199,  174,  912,  115, 1027,  111, 7573,  117,  756,  117, 2153,\n",
      "         117, 1077,  112,  111,  105,  126,  588,  128,  481,  227,  247, 4414,\n",
      "         207, 2666, 1999,  210,  667, 1270,  218, 5086,  235,  275,  438,  552,\n",
      "         207,  438])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3484 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   128,   242,   207,  3497,   210,  2125,  4706,   115, 15373,\n",
      "          175,   481,   219, 14013,   174,   238,  4197,   216,   207,   804,\n",
      "         3149,   232,   263,   255,  1453,   211,   960,   667,   602,   217,\n",
      "          474,   253, 15373,   175,  2104,   213,   347,   215,  7716,   229,\n",
      "         4750,   174,  7404,  2664,   218, 17498,   117,   321, 15396,   166])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4808 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,  1336,   765,   351, 15373,   175,   222,   239,  3515,\n",
      "         1372,   212,   217, 15373,   175,   222,   239,  3757,   117,   228,\n",
      "         1601,   244,   126,   113,   113,   516,   128,  2971,   242,   207,\n",
      "         3262,   467,   117,   152,   190,   189,   171,  2042,   216,   207,\n",
      "         3925,   189,   292,   227,  3028,   115,   552,  2067,   223,   227])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4084 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   728,   253, 15373,   175,   110,   163,  1328,   189,   292,\n",
      "          227,   969,   238,   207,   879,   115,   152,   184,  5443,   126,\n",
      "          128,   226,   240,   297,   542,   239,  1045,   211,   709,   207,\n",
      "         1927,   210,   152,   190,   189,   171,   110,   163,   949,   388,\n",
      "          211,   207,  1336,   217,  3647,  1675,   117,   321, 20737,  3831])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3713 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1327,   210,   525,   541,   115,   207,   460,   386,   227,\n",
      "          635,   207,  3757,   210,  2067,   212,   115,   420,   115,   153,\n",
      "          110,   157,  6214,   233,   117,   105,   111,  2153,   117,  6899,\n",
      "         2311,   112,   117,   202,   221,  1571,  2494,   115, 15373,   175,\n",
      "          110,   163,  3757,   217,  2067,   311,   219, 15255,   179,   252])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3876 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   460,   222,   207,  2972,  3855,   693,   117,   307,  6434,\n",
      "        20592,   246,   488,   213,   502,  1743,   211,  3508,   235,   207,\n",
      "          693,   216,   741,  2230,   174,   117,   532,   244,   227,  9146,\n",
      "          216,   231,  2229,   305,   247,   255,  2003,   238,  3508,   235,\n",
      "          388,  4574,   693,   229,   292,  1385,   207,  2972,  3408,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4346 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 18917,   115, 25717,   111,  9986,   117,   756,   117,  2153,\n",
      "          117,  1138,   112,   117,   532,  1231,   216,   207,   483,   240,\n",
      "         4342,   213,  1408,   216, 15373,   175,  2158,   239,  3757,   189,\n",
      "          217,  8437,  6750,  1346,   212,   237,  1320,   117,   205,   126,\n",
      "          113,   113,   984,   128,   152,   184,  6336,   126,   128,   606])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4978 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1000,   281,   207,  1064,   210,   207,  1213,   117,   110,\n",
      "          105,   447,   117,   111,  3284,  4664,   111,   450,   112,   210,\n",
      "        12931,   142,  9713, 13515,   117,   145,   111,  1851,   112,   112,\n",
      "          117,   228,   145,   838,   245,  2140,   372,   289,   257,   223,\n",
      "          213,   145,  3638,   811,  2515,   211,   207,   231,   257,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3823 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   962,   156,   117,   149,   174,   117,  5241,   115,  1299,\n",
      "          126,   113,  8639,   128,   163,   117,   756,   117,  5366,   111,\n",
      "         7136,   112,   115,  9150,   222,   231,  1030,   218,  5119, 20137,\n",
      "          174,  1610,   117,   166,   117,  4265,  3574,  1610,   117,   115,\n",
      "         6803,   165,   117,   163,   117,  8167,   115,  1293,   156,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 31 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   634,   165,   117,   163,   117,  6394,   117, 13648,   189,\n",
      "        20481,  2450,   115,   113,   199,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3988 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,  2231,   212,   308,  1282,   244,   207,   867,   210,\n",
      "          207, 24514,  7936,   383,  1145,   109, 23127,   189,  1850,   529,\n",
      "          212,   207,  5733,  1850,   823,   217,   207,  1933,   221,  1761,\n",
      "          595,   212,   672,   210,   270,  2541,   210,   535,   116,  1761,\n",
      "          212,  9314,   661,   111,   651,   230,   117,   279,   112,   115])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4676 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   822,   537,   212,  5803,   340,   211,  2433,   213,   145,\n",
      "         1206,  4808,   334,   687,   347,   117,   126,   113,   206,   128,\n",
      "          105, 12475,   166,   117,  3755,   171,   187,  2608,  1610,   117,\n",
      "          115,  5968,   150,   117,   200,   174,  5279,   115,  5279,   111,\n",
      "          202,   475,   682,   117,   634,   112,   111,  4197,  7458,   769])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4508 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  8566,   252,  2070,   212,  1365,   115, 17726,  4476,   210,\n",
      "          334,  2067,   115,  2078,  2067,   212,  5859,   266,  2581,   117,\n",
      "          207,  1685,   249,  4023,   126,   113,   337,   128,   569,   107,\n",
      "          198,  4181,  1628,   189,   217,  4660,   212,  2747,  1106,  1378,\n",
      "          213,  1215,  4855,   117,   207,   219, 13166,   184, 22741,  2864])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1919 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   217,   207,   788,   853,   115,   233,   223,  1677,   216,\n",
      "          894,   110,   867,   211,  6804,   497,   649,   111,   651,   230,\n",
      "          117,   478,   112,   223,   614,   115,   212,   966,   602,   230,\n",
      "          117,   152,   116,   662,   116, 19882,   115,   160,   176,   171,\n",
      "          191,   166,   117, 14414,   190,  1197,   115,   521,   117,   115])\n",
      "Original length: 714 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  4844,   128,   777,   119,   979,   345,\n",
      "          115,   634,  6322,  3155,   115,  1278,  7465,   115,  1509,   865,\n",
      "          153,   117,  2500,   222,   624,   345,   115,   689,   115,  2765,\n",
      "          212,   393,   116,   257, 11710,   189,   111,   105,   164,  2232,\n",
      "          189,   105,   112,   443,   493,   241,   215,   332,   210,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  404,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4015 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5859,  3031,   115,   207,  1278,  2078,  2067,   325,   111,\n",
      "          203,  2383,   117,   147,   117,   142,  4997,  1570,   112,   115,\n",
      "          212,   207,  2078,  2067,   212, 12497,  1275,   212,  1748,  3031,\n",
      "          210,   207,  3568,   265,   212,   207,   483,   210,  3634,   117,\n",
      "          820,   323,  1430, 11628,  5827,   225,   438,   222,   767,   210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  416,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4384 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,   463,  1020,   115,   207,   240,   614,   588,   110,\n",
      "          163,   867,   211,  2310,   207,  6582,   189,  3298,   179,   181,\n",
      "          115,  9384,  1522,   115,  3457,  6885,   115,   212, 15998, 10172,\n",
      "          872,   117,   111,   148,   117,   153,   117,  1164,   112,   820,\n",
      "         3451,   207,   850,   210,   207,   240,   110,   163,   371,   229])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  422,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4072 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   117, 19134,  2094, 13378,   189,   677,  1522,   115,  8386,\n",
      "          162,   117, 20347,   212,  7028,   148,   117, 13759, 16601,   166,\n",
      "          117,  2019, 11112,  5608,   181,  3447,   243,   115,  4984,   116,\n",
      "         1164,   116,  1306,   115,   881,   240,   210, 18863,  7187,  1484,\n",
      "          115,  6754,   111,   105, 13759, 16601,   347,   105,   112,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  433,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3556 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   342,   128,   111,   447,   117,   236,\n",
      "          160,   186, 10873,   116,   924,   112,   207,  3572,   171,   178,\n",
      "          183,   820,  1125,   145,   578,   687,   347,   222,   767,   210,\n",
      "         2765,   213,   337,  4941,   115,   283,  2015,   117,   111,   447,\n",
      "          117,   236,   160,   186, 10946,   116,  1076,   112,   207,  3572])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  339,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 2965 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   142,   199,   117,   111,   447,   117,   236,   160,  1570,\n",
      "          112,   222,   767,   210,   241,   126,   113,   113,   422,   128,\n",
      "          164,  2232,   189,   115,   204,   820,  1637, 20440,   949,   294,\n",
      "          211,   248,   201,   210,   207,  8913,  1386,   184,   325,   115,\n",
      "          342,   165,   117,   163,   117,   147,   117,   142,   342,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  441,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4905 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   221,   145,   599,  1363,   115,   213,   207,   437,   212,\n",
      "          720,   373,   212,   293,  2500,   210, 11379,   110,   163,  5609,\n",
      "         3221,  9535,   890,   340,  3537,   115,   588,  5617,  1430,   213,\n",
      "         2882,  4146,   212,  5765, 11840,   189,  1356,   252,   236,  6000,\n",
      "          216,   536,  4484,  2359,   212,  1718,   263,   211,   219,   946])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  478,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4675 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   906,  2399,   213,   207,   522,   872,  4051,   213,   624,\n",
      "          722,   212,  1795,   555,   207,  1595,   398,   117,   111,   148,\n",
      "          117,   153,   117,  5722,   236,   160,   186, 11687,   115,  3532,\n",
      "          112,   213,   207,   971,   210,  2399,   115,   820,  1738,   252,\n",
      "          225,   237,   636,  1909,   211,  2771,   145,   949,  1355,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  516,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3204 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113, 5611,  128,  148,  117, 1366,  672,  222,  979,  198,\n",
      "         115,  689,  115,  207,  240,  614, 1366,  672,  210,  207,  906,  212,\n",
      "        4124,  301, 1853,  207,  906,  687,  117,  111,  148,  117,  153,  117,\n",
      "        3164,  112,  207,  308,  957,  207,  399,  217, 1283,  326,  211,  207,\n",
      "         687,  749])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  498,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3599 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1182,   326, 12504,   189,   292,  2268,   217,  2765,   212,\n",
      "          164,  2232,   189,   117,   111,   447,   117,   115,   676,   117,\n",
      "          147,   236,   160,   973,   112,   207,   326, 12504,   189,   950,\n",
      "          145,   576,   436,   326,   210,   207,   954,   906,   115,   145,\n",
      "          326,   210,  2819,   432,   115,   212,   145,  2078,   215,   164])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  442,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 2012 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   150,   117,  1328,   189,  8390,  1328,   189,   211,   207,\n",
      "          906,   292,   678,   218,   207,   463,   416,   115,   689,  3357,\n",
      "          115,   283,  7374,   238,   750,  2765,   215,  2078,  2215,   282,\n",
      "          126,   113,   113,   984,   128,   212,   428,   238,   164,  2232,\n",
      "          189,   117,   279,   207,  1328,   189,   356,  1309,   219, 26394])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  556,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4637 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   909,   128,   152,   184,  1570,   126,\n",
      "          128,   207,   240,   323,  1723,   216,   233,  1013,  1038,   585,\n",
      "          569, 25780,   687,   749,   218,  1283,  1549,   326,   210,   207,\n",
      "        21400, 12160,  2759,   687,   347,   211,   687,   749,   212,   218,\n",
      "         1283, 25780,   189,   225,   207,  1588,   211,   219,  2752,   212])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  554,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3411 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  4214,   126,   128,   207,   240,   386,   227,\n",
      "         7814,   288,   207,  1823,   210,   145,  4179,   934,  1455,   331,\n",
      "          233,   245,   219,  1589,   221,   145,   687,   347,   117,   321,\n",
      "        14854,   166,   117,  9292, 19742,   212, 29157,   115,  6652,   165,\n",
      "          117,   163,   117,  3668,   115,  3164,   115,   673,   156,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  508,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3254 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   899,   128,   226,   256,   223,  1528,\n",
      "         7804,   222,   207,  2965, 12497,   818,   210,   588,   212,   207,\n",
      "          363,   216,   818,   263,   222,   327, 13076,   218,   207,  5609,\n",
      "         2291,   212,   207,   907,   493,   217,  3221,  9535,   890,   340,\n",
      "         3537,   120,   233,   223,   227,  7804,   222,   207,   818,   210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  509,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5121 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,  1036,   128,   207,   256,   236,  2061,\n",
      "          356,   323,   219,  6171,   238,   649,   343,   548,  4678,   222,\n",
      "          599, 12497,  1798,   280,   211,   750,   687,   749,   212,   749,\n",
      "          110,  2664,   222,   324,  1798,   117,   321,   213,   665,  1111,\n",
      "         9309,  1108,   115,   521,   117,   115,  5073,   150,   117,   200])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  345,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4518 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3196,   112,   115,   340,  5010, 17126,   695,   222,   949,\n",
      "          244,   323,  1332,   218,  2615,   820,   115,  1917,   986,   247,\n",
      "          765,   216,   152,   184,  3875,   126,   128,  1477,   189,   213,\n",
      "          207,   291,   210,   949,  2150,   317,   687,  1276,   212,   687,\n",
      "          749,   386,   227,  3190,   207,  2615,   820,   110,   470, 21996])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  415,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3794 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 3195,  117,  160,  117,  516,  111,  146,  112,  111,  200,  112,\n",
      "         111,  147,  112,  117, 1489,  115,  207, 2312,  210,  588,  110,  163,\n",
      "         669,  210,  316,  213, 1278,  126,  113,  113, 1323,  128,  212,  207,\n",
      "         914,  995,  210, 1298, 1182,  687,  347, 4179,  189,  213, 1278,  468,\n",
      "         233,  145])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  846,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5240 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   326,  2461,   242,   207,   620,   115,   283,   750,   326,\n",
      "          211,   241,   749,   443,   356,   219,  1541,   555,   480,  3386,\n",
      "          117,   110,   105, 19131, 11268,   115,  7523,   165,   117,   163,\n",
      "          117,   236,  7463,   111,  3284,  1954,   117,   162,   117,  3195,\n",
      "          117,   160,   117,   516,   111,   147,   112,   111,   199,   112])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  875,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3871 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1387,   211,  2357,   306,   207,  1469,   221,   145,  1141,\n",
      "         1959,   210,   906,   286,   117,   105, 19329,   890,   166,   117,\n",
      "          793,  2197,   210,  4859, 27254,   189,   115,  7264,   150,   117,\n",
      "          199,   174,  2938,   115,  3266,   111,   205,   475,   682,   117,\n",
      "         2386,   112,   117,   487,   222,   207,   348,   115,   207,   240])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  912,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4019 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  7190,   126,   128,   105,   226,  2025,  7721,\n",
      "          189,   110,   207,  4631,   418,   115,   213,   606,   259,   212,\n",
      "         1364,   115,   210,  1795,  1641,   117,   110,   105,   213,   665,\n",
      "        12607, 16206,   115,  5727,   150,   117,   200,   174,   236,  5284,\n",
      "          117,   906,   223,  1750, 17511,   213,   145,  3197,   687,   347])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  774,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4029 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   548,   213,   226,   256,   115,   606,   555,  1961,  6114,\n",
      "          211, 10161,   235,   212,  3331,   145,  5052,  3409,   687,   347,\n",
      "          212,   555,  6114,   211,  5775,   548,   117,   820,  3372,  1298,\n",
      "          207,  7791,   210,   548,   242,   522,   212,   264,  5859,   534,\n",
      "          115,   264,  2078,   644,   534,   115,   212,   264,   346,   266])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  910,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3482 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3178,   207,   394,   210,  2765,   443,   297,   227,   247,\n",
      "        12815,   211,   207,  5609,   120,   212,   216,   239,  2965, 12497,\n",
      "          818,  3307,   597,   268,   289,   398,   293,   207,  2728,   210,\n",
      "          207,   854,   287,   115,   809,  3622,   235,   949,   117,   111,\n",
      "          447,   117,   236,   160,  4019,   112,   207,   240,  1723,   216])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  984,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4130 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1016,   297,   247,   280,   588,  1136,   199,   117,   202,\n",
      "          108,   597,   126,   113,   113,  1368,   128,   217, 15704,  3621,\n",
      "          174,   890,   117,   111,   447,   117,   236,   160,  4214,   112,\n",
      "          552,   588,   297,   247, 16119,  3648,   207,   421,   217,   820,\n",
      "          110,   949,  3283,   236,   729,   115,  1704,   117,  1441,   323])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1043,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4374 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   240,  2635,   216,   805,   210,   392,  1328,   189,\n",
      "         2140,   238,   145,  4790, 12103, 16863,   210,   207,   906,   115,\n",
      "          225,   805, 18434,   189,  9710,  2765,   481,   307,  2443,   238,\n",
      "          422,   108,   210,   207,   906,   126,   113,   113,  1371,   128,\n",
      "          831,   117,   111,   148,   117,   153,   117,  4801,   115, 20858])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  909,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4381 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   535,   116,   490,  2765, 11373,   195,   247,   470,   217,\n",
      "         4162,   215,   231,  2125,  1322,   115,   253,   227,   949,   115,\n",
      "          212,   809,   356,  4908,   805,   748,   217,  7763,   235,   270,\n",
      "          470,   351,   588,   117,   253,   975,   535,   116,   490,   126,\n",
      "          113,   113,  1288,   128,  2765,   292,  1881, 10972, 11559,  5550])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  673,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3977 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   215,  1689,  1564,   218,   687,   661,   213,   635,   210,\n",
      "          207,   906,   672,   117,   207,   482,   216,   145,   394,   210,\n",
      "         1369,  1328,   189,   292,   678,   218,   207,   489,  3357,  2777,\n",
      "          216,  1667,   326,   212,   286,   246,   537,   211,   687,   749,\n",
      "          212,  1498, 18434,   189,   117,   213,  1113,   115,   293,  2793])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  895,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4401 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 4758,  207, 2529, 2124, 2899,  218,  661,  340,  221,  211,  690,\n",
      "         216,  105,  791,  661, 1088,  211,  219, 5803,  211, 2670,  803,  195,\n",
      "         115, 3197,  115,  212, 7665, 1641,  117,  105, 2976,  217, 3197, 1641,\n",
      "         200,  174,  115,  142,  498,  117, 3112,  115,  236, 4685,  117,  687,\n",
      "         661,  323])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  925,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3768 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   217,   107,  1039,   116,  1033,  1105,  2443,   547,   212,\n",
      "          145,   203,   116,   258,   108,  1832,   111,   215,  1812,   112,\n",
      "          217,   105, 15224, 15603,   105,  2443,   547,   210,   107,  1033,\n",
      "          116,  1586,  1105,   117,   213,   665,   790,  1180, 17140,   117,\n",
      "         5859, 14150,   117,   115,  3486,   150,   117,   162,   117,   148])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1027,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3947 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   906,  1450,  4129,   189,   216,   107,   199,   115,\n",
      "        12175,   115,  5631,   117,   743,   213,   906,  1269,   418,   263,\n",
      "          255,   796,   555,   245,   415,   115,   634,   115,   283,   107,\n",
      "          198,   115,  7833,   115,  2220,   217,  1279,   210,   326,   211,\n",
      "          207,   687,   117,   111,   148,   117,   153,   117,  5164,   236])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1007,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 2352 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  207,  276,  244,  457, 2296,  126,  113,  113, 2206,  128,  211,\n",
      "        2397,  212, 2788,  207, 4618,  210,  906,  111,  148,  117,  153,  117,\n",
      "        3454,  112,  623,  211,  239,  333,  212,  318,  117,  207,  276,  244,\n",
      "         755,  211,  753,  211,  212, 1643,  228, 1426,  212, 3113,  211,  207,\n",
      "        4618,  221])\n",
      "Original length: 2386 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  2567,   126,   128,  1743,   115,  1909,  2229,\n",
      "         1954,   117,   162,   117, 12548,   117,  7596,   246,   396,   213,\n",
      "          722,   211,  3430,  1967,   528,  1743,   216,   246,   487,   222,\n",
      "         2137,   115,   927,   215,   231, 12516,   174,  1002,   281,   207,\n",
      "         1064,   210,  1954,   117,   162,   117, 12548,   117,  7627,   117])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2944 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,  2242,   616,  3899,   325,   115,   221,  4042,   252,\n",
      "          218,  3422,   115,  3530,   119,   419,   211,   207,   464,   216,\n",
      "          145,   397,   212,  7222,   581,   210,   811,   367,   211,  3015,\n",
      "         1002,   215,   744,   211,  1996,   210, 24590,  1485,  8366,   145,\n",
      "         2538,  1762,   213, 14878,   115,   145,  4167,   213,   145,   966])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2147 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   986,  2041,   211,  1204,   145,   480,  2662,  1008,   207,\n",
      "          453,   210,   207,  3569,   211,   207,   588,   236,   207,   259,\n",
      "          216,   233,   246, 14300,   174,   115,  2248,   210,   207,   993,\n",
      "         4486,   210,   207,  3047,   117,  2257,   109,   551,   266,   123,\n",
      "          117,   117,   117,   123,   364,   109,   333,   123,   616,  3899])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2784 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   966,   492,   123,   117,   117,   117,   123,  1322,   238,\n",
      "         1969,   123,  2704, 18323,   188,   109,  5646, 17825,   188,   123,\n",
      "          336,  9283,   966,   492,   123,   117,   117,   117,   123,  1322,\n",
      "          238,  1969,   123,  2704, 18323,   188,   109,  5646, 17825,   188,\n",
      "          123,  5646, 17825,   188,   189,   152,   184,  5531,   126,   128])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3910 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   217,   160,   117,   169,   117,  3289,   115,   588,   116,\n",
      "         1421,   111,   362,   116,  6374,  1570,   112,   119, 11534,   154,\n",
      "          117,   256,   195,   115, 14166,   195,  4156,  5160,   115,  8889,\n",
      "          115,  2838,   115, 10020,   109, 19422,   171,   195,   115,  3021,\n",
      "          115,   148,   173,   117,  1716,   119,   373,   119,  6278,   212])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4759 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 18401,   235,   239,  1261,   212,  2198,  1781,   117,   150,\n",
      "          186,   503,   227,  2394,  4335,  1617,   210,  7612,  1181,   338,\n",
      "          115,   696,   503,   233,  1979,   656,  4320,   338,   211,   327,\n",
      "          120,  1625,   115,   221,  3289,   763,   222,   207,  9807, 19553,\n",
      "          115,   105,   221,   145,  1261,  3070,   115,   532,   465,   227])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5066 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  2138,   126,   128, 18467,   190,   545,   216,\n",
      "          207,   105,  4000,   115,   210,   971,   115,   311,   219,  1199,\n",
      "          222,  1075,   212,  4354,   115,   227,   222,   207,  2314,   335,\n",
      "         3127,   117,   105, 18467,   190,   115,  7107,   165,   117,   163,\n",
      "          117,   236,  7757,   117,   390,   115,   207,   428,   244,   227])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4466 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2533,  1909,  1480,  1743,   216,   223,  8915,   115,  1625,\n",
      "          268, 18644,   301, 16318,   215,  4548,   117, 18467,   190,   115,\n",
      "         7107,   165,   117,   163,   117,   236,  8302,   117,   532,  1800,\n",
      "        24744,   110,   163,  1743,   606,   444,   212,  4575,   117, 24744,\n",
      "          110,   163,  1743,   246,   656,   444,   211,   207,   126,   113])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4339 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   422,   128,   150,   186,  3754,   213,\n",
      "        16587,   175,   211,  2990, 13234,   182,   189,   110,   163,  1743,\n",
      "          115,   487,   222,   239,  3868,   210, 18467,   190,   117,   207,\n",
      "          240,   633, 13234,   182,   189,   110,   163,  1743,   213,   901,\n",
      "          210, 18467,   190,   115,   212,   657,   216, 13234,   182,   189])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4453 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113,  113,  478,  128,  201,  117, 5372,  212,  264,  116,\n",
      "         266,  949,  117,  150,  186, 1409,  207, 3053,  210,  239,  867,  211,\n",
      "        1836,  207,  371,  115,  487,  222,  207, 1151,  216, 7612, 1181,  481,\n",
      "         227, 2443,  217,  606, 5372,  212,  264,  116,  266,  470,  117,  207,\n",
      "         483,  240])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3156 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  5531,   126,   128,   532,   709,   207,   483,\n",
      "          240,   110,   163,  3053,   210,  5646, 17825,   188,   217,  2287,\n",
      "          210,  1045,   117,  8922,   166,   117, 23949,  1484,   115,  3816,\n",
      "          150,   117,   200,   174,  5802,   115,  6059,   111,   203,   475,\n",
      "          682,   117,   722,   112,   117,   150,   186,  2042,   216,   207])\n",
      "Original length: 1224 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   218,   119,   157,   117, 16746, 11383, 16945,   528,\n",
      "          126,   113, 20589,   128, 11383, 16945,   115,   881,   865,   119,\n",
      "          226,   256,  3335,   238,   470,   216,   428,  1277,   918,  6887,\n",
      "          174,   207,  6044,   235,   210, 29144, 22983,   213,   308,   211,\n",
      "        15677,   207,  4405,   210,   270,  2358,  5731,   795,   117,   532])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3496 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3021,   115,   207,  6599,   210,  3021,   110,   163,  1698,\n",
      "         1104,   117,   623,   211,   207,   759,   115,   198,   207,   795,\n",
      "          244,   105,   652,  2358,   755,   211,   219,  1593,   213,   207,\n",
      "          354,   265,   117,   105,   335,  3222,   217,   207,  8203,   189,\n",
      "          105,   236,  4405,   216,   244,  1305, 15677,   252,   552,   210])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4332 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   795,  3754,   217,  7005,   115, 15476,   235,   145,\n",
      "          954,   396,   759,   216,   832,   145,  3854,  2499,   577,   268,\n",
      "          207,  2615,  8203,   189,   212,   950,   353,   599,  8098,   383,\n",
      "         2148,   117,   207,   396,   759,   265,   216,   207,  8203,   189,\n",
      "          212, 23295, 10815,   189,   105,  4234,   145,  1794,  1224,   210])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4164 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  795, 4327,  237, 1563,  211,  270,  359,  213,  207,  432,  210,\n",
      "        2852, 4405,  117,  201,  207, 2129, 3346,  223,  211, 1025,  126,  113,\n",
      "         113,  258,  128,  331,  226, 1563,  246,  105,  218,  744,  210,  105,\n",
      "         207, 8203,  189,  110,  832, 2581,  115,  145, 1121,  207, 1313,  240,\n",
      "         249, 1950])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4932 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   795,  4327,   145,   830,  1576,   222,   207,  2257,   327,\n",
      "          115,   227,   207,   353, 14164,   252,   388,   210,   126,   113,\n",
      "        22583,   128,   237,  1576,   222,   207,  4506,   175,   327,   115,\n",
      "          229,   213,  1736,  1423,   207,  1407,   907,   117,   207,   450,\n",
      "          881,   115,   207,   307,   881,   211,   247,   633,  2148,   210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4570 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   483,   240,   110,   163,  1355,  7804,  3093,   222,\n",
      "          602,   116,   213,   116,   482,   115,   227, 12297,   602,   115,\n",
      "          212,   233,   223,  5545,   236,   226,  1925,   211,  2291, 10837,\n",
      "          217,   207,   759,   110,   163,  2148,   210,  8098,   383,   117,\n",
      "          221,   532,  2494,   213,  6870, 14459, 19854,  3455, 21142,   547])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4387 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   795,  6047,  6070,   551,  1199,   242,   264,   266,\n",
      "          115,  9829,   235,   522,   461,   585,   115,   212,   241,   276,\n",
      "          244,  3021,  2917,   115,  9829,   235,  5642,   585,   117,   321,\n",
      "          508,   165,   117,   163,   117,   147,   117,   142,   142, 12771,\n",
      "          109, 14574,   117,   213,   228,   145,   978,   115,  3803,   237])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4774 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1301,  1333,   115,   310,   227,   235,   216,   105,  2353,\n",
      "          249,   936,  4299,   755, 18379,   257,   585,   105,   111,  3189,\n",
      "          508,   165,   117,   163,   117,   147,   117,   142, 15579,   112,\n",
      "          112,   117,   146,   117,  1515,   658,   210,  2333,   585,   242,\n",
      "          142, 15579,   220,  4473, 21958,   235,  1866,   216,   145, 21794])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 368, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 174 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2333,   585,   115,   212,   211,   542,  1045,   569,   331,\n",
      "          228,   126,   113, 23663,   128,   585,   297,   219,   489,   213,\n",
      "          207,  1158,   210,   226,  1641,   117,  5133,   212,  5661,   117,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 1131 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  6770,   128, 12381, 20420,   145,   117,\n",
      "         5676,   190,   189,   115,   354,   265,   483,   865,   117,   595,\n",
      "        20946,   395,  4656,  1346,   212,  6206,  1410,   115,   884,   111,\n",
      "         1404,   105,  2958,   105,   112,  1979,   189,   226,  5859,   347,\n",
      "          294,   211,   248,   199,   210,   207, 10104,   325,   115,   342])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4069 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   588,  3230,  5938,   212,   239,  2404,  1056,   734, 12092,\n",
      "         1554,   115,   126,   113,   113,   200,   128,   521,   117,   115,\n",
      "          244,  1278,  4271,   229,   656,   215,  1419,   639,   212,  2075,\n",
      "         3230,  3332,  1190,   189,   213,  1338,  1340,   210,   350,   871,\n",
      "          115,   350,  4403,   115,   212,  7123,   117,   111,  7233,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3920 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  207,  393,  115,  207,  327,  210,  207,  708,  210, 2186,  116,\n",
      "        5031, 3893,  235,  115, 3684,  207,  427,  212,  496,  210,  490, 3332,\n",
      "         410,  317, 4320, 7800,  186,  174,  189,  212,  207, 4849,  212,  316,\n",
      "         395,  213,  270,  841,  446,  839,  117,  111, 7233,  117,  160,  186,\n",
      "        9017,  115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4261 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,  7727,   128,   198,   117,  2958,   386,   227,\n",
      "          247,  5859,  1566,   211,  2962,   145,   388,   210,  3750,  5508,\n",
      "         3064,   210,   207,   832,   105,   958,  5616,   277,   105,   327,\n",
      "          552,   230,  8098,   383,  2431,   217,  2958,   110,   832,  1563,\n",
      "          212,  1623,   301,   115,   207,  1563,   223,   227, 14299,   221])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4068 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  152,  184, 2837,  126,  128,  207,  325,  210, 5508, 3064, 2003,\n",
      "         218,  342,  165,  117,  163,  117,  147,  117,  142,  199,  126,  113,\n",
      "         113,  368,  128, 2256,  189,  210,  428, 1877,  119,  105,  207, 1470,\n",
      "         210, 5508,  195,  643,  213,  207,  444,  327,  204,  212,  100,  207,\n",
      "        3007, 1128])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5220 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1280,   115,   152,   184,  4290,   126,   128,   237,  5859,\n",
      "         1563,   223,   105,   210,   207,   816,   207,  5859,   534,   292,\n",
      "          752,   211,  1403,   212,   216,  4236,   238,   216,   229,  1907,\n",
      "          894,   110,  1275,  2093,   117,   207,  1563,   305,  2389,   207,\n",
      "         1395, 13408,   363,   126,   113,   113,   422,   128,   591,   210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4261 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5616,   277,   212, 15889,   189,   244,   237,   105,  7570,\n",
      "          175,   687,   210,   652,  1234,  2212,   116,   302,   297,  2995,\n",
      "        27755,   603,   211, 21135,   207,   408,   302,   213,  5859,  1177,\n",
      "          117,   105,   321,  4539,   117,  5918,   117,   266,   236,   422,\n",
      "          116,   433,   117,  2958,  3618,   189,   216,   233,   249,  1566])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5386 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1563,   211,  2765,   115,   310,  3005,   218,   207,   826,\n",
      "          210,  2606,   126,   595,   128,   297,   247,   378,   280,   263,\n",
      "          233,   227,   255,  1881,   238,   207,   327,   112,   120,  6744,\n",
      "         4417,   828,  4280,   115,   521,   117,   166,   117,   151,   190,\n",
      "          175,   921,   547,   115,  8277,   150,   117,   199,   174, 19925])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5203 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2381,  3187,   145,  4132,  1563,   211,  1016,   212,   211,\n",
      "         1092,   213,   145,   512,   327,   115,  8093,  1566,   117,   207,\n",
      "          450,   881,   249,   765,   115,   213, 14288,   115,   216,   152,\n",
      "          184,  6336,   126,   128,   105,   233,   223,  4462,   115,   213,\n",
      "          207,  1345,   210,   145,  1179,   316,   663,   115,   217,   145])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3510 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1015,  115,  588,  110,  163, 3883,  211, 2310,  595,  110,  163,\n",
      "         142,  199,  470,  213,  207, 5616,  277,  212, 2796, 1472,  126,  113,\n",
      "         113,  774,  128,  126,  113, 9008,  128,  222,  207,  421,  210, 1566,\n",
      "         244, 1463,  117,  279,  126,  113,  113,  910,  128,  199,  117, 5859,\n",
      "        1563,  212])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3816 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1798,  1610,   117,   166,   117,  1985,  3834,   117,   109,\n",
      "         3834,   117,   535,   117,   115,  9220,   150,   117,   199,   174,\n",
      "        17846,   115, 12786,   116,   875,   111,   204,   475,   682,   117,\n",
      "          112,   111,   105, 28840,   110,   163,   494,   210,   237,  1754,\n",
      "         1068,   111,  5026,  1640,   145,   110,  5917, 18556,   110,   112])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2723 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 17846,   115, 18596,   111,   204,   475,   682,   117,  1577,\n",
      "          112,   117,   226,   240,   115,   390,   115,   356, 10899,  3248,\n",
      "          226,   550,   222,   207,  1823,   236,   226,  1925,   210,   207,\n",
      "         1641,   117,   321,   115,   149,   117,   151,   117,   115,   266,\n",
      "         2813,   210, 12394,   166,   117,  2639,   184,  4294,   115,  5631])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 516, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3969 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 4539,  117, 5918,  117,  266,  236,  345,  117, 2958, 3618,  189,\n",
      "         216, 2958,  223,  227, 3625,  145,  105,  164,  195,  235,  388,  115,\n",
      "         105,  215,  145,  388,  211, 1960, 7619,  697,  111, 2945,  117,  110,\n",
      "         163, 5918,  117,  266,  236,  875,  112,  212, 1015,  115,  233, 1763,\n",
      "         216,  207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 498, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4424 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1692,  210,  207, 5271, 3854,  115,  111,  200,  112, 4327, 1459,\n",
      "         207,  636, 1576,  210,  216, 3854,  223,  211, 7349,  616,  213,  207,\n",
      "         327,  213,  461,  115,  212,  111,  201,  112, 1596,  145, 3854,  215,\n",
      "        7740, 1389,  317,  428,  215,  353, 1499,  117, 2223, 4613,  109, 4423,\n",
      "        8133,  535])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 442, 210, 442, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 456 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   426,   117,  1113,   217,   207,   853,   763,   348,   115,\n",
      "          207,   240,  6786,   189,   588,   110,   163,   867,   211,  2310,\n",
      "          225,   207,  1887,   210,   588,   110,   163,   867,   221,   211,\n",
      "         2481,  2014,   115,   207, 11750,   195,   325,   388,   115,   229,\n",
      "          223,   614,   117,   595,   224,   938,   237,   396,   759,   213])\n",
      "Original length: 113 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  3486,   128,   528,   210,   207,   240,\n",
      "         1742,  9234,  6377,   115,   881,   865,   119,  4989,   150,   117,\n",
      "          200,   174,  3472,   115,   113,  3472,   120,   634,   165,   117,\n",
      "          163,   117,  2153,   117, 13648,   189, 12766,  2837,   115,   113,\n",
      "          113,   198,   102,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2786 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   226,   615,   218,  1547,   189,   210, 10395,  3703,   198,\n",
      "          126,   113,   113,   202,   128,  1379,  1143,   211,  1988,   253,\n",
      "          207,   483,   240,  4342,   213,  1848,   428,  3883,   217,   687,\n",
      "         2122,   218,  2215,   210,   820,   443,  1125,  5859,   266,  3728,\n",
      "         3187,   216,   207, 10395,  3703,  1547,   189,  2104,   213,   145])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3981 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   447,   117,   236,  4602,   117,   207,   240,   207,   184,\n",
      "         1287,   216,   207,  1206,  4808,  3610,   263,  1624,   207,   506,\n",
      "          210,   525,   516,   111,   146,   112,   117,   207,   240,   657,\n",
      "          296,   115,   216,   820,  1564,   885,   460,   211,   635,   470,\n",
      "          216,   207,  3854,   211,  2807,   207,   405,   210, 10395,  3703])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4510 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  7285,   213,  2798,  3703,  2960,   235,   391,   207,   296,\n",
      "         2512,   210,  1154,   246,   353,  1010,   116,   497,   268,  1222,\n",
      "          116,   497,   117,  1435,   226,   398,   115,  1104,  8826,   189,\n",
      "          210,  2798,  3703, 27593,  7889,   348,   199,   117,   206,   126,\n",
      "         1105,   128,   211,   184,   189,   212,  5691,   126,  1310,   128])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4123 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  870, 2205,  115,  246, 1835,  213,  241, 2205,  268,  207, 1832,\n",
      "         229,  297,  247, 3733,  213,  241, 2205,  242, 2010,  364,  115,  233,\n",
      "         297,  219,  969,  216,  241,  749,  210,  207,  687, 2150,  805,  854,\n",
      "         115,  911,  216,  330,  297,  219, 4275,  189,  967,  241, 3942,  221,\n",
      "         211,  207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4464 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   692,  1138,   117,  1532,   211,   239,  4203,   115,  1067,\n",
      "         3128,   126,   113,   113,   422,   128,  2199,   223,   216,   330,\n",
      "          223,   230,  8176,   317,   207,  1252,   213,  1010,   210, 10395,\n",
      "         3703,   212,   207,  1230,   126,   113,  4153,   128,   405,  3732,\n",
      "          117,  1067,   335,  8001,  4293,   223,   216,   820,   110,  1151])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4468 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   820,   323,  1564,   237,  3629,   210,  1704,   117,\n",
      "        18278,   147,   117, 29065,   443,   763,   216,   105,   487,   222,\n",
      "         1357,  1355,   210,   207,  2960,   235,   586,   212,   243,  1083,\n",
      "          115,   153,  2283,   216,   207,   832,  2093,   818,   211,  2807,\n",
      "        10395,  3703,   907,   297,   247,  1576,   252,   241,   749,   210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4227 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 14150,   117,   115,   896,   150,   117,   162,   117,   148,\n",
      "          117,  5611,   111,   163,   117,   148,   117,  5567,   117,  2142,\n",
      "          112,   115,   207,  2797,   978,   246,  1564,   211,   216,   240,\n",
      "          343,   595,   263,  1541,   428,  1309,  1428,  9531,   395,   115,\n",
      "          229,   300,  3450,   222,  1260,   211,  1025,  1576,   212,   949])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4588 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   532,  1262,   233,  4013,   211,  2600,   207,  3543,  9237,\n",
      "         6956,   210,  4255,   236,  5021,   213, 15037,   117,  7935,  1517,\n",
      "          175,  9662, 15504,  6441,   190,  1143,   216,   152,   184,  2138,\n",
      "          126,   128,   207,  1263,   736, 14052,   189, 13099,  2570,   213,\n",
      "          207,  3517,   273,   757,   736,   119,   111,   198,   112,  1408])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4331 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1105,   189,   210,   320,   693,  1325,   115,   809,  1017,\n",
      "          207,  1206,  4808,   687,  5970,   668,  1522,  2342,   452,   120,\n",
      "         1438,   115,   237,   221,  9931, 21834,   757,   394,   210,   693,\n",
      "          223,   227,   530,   117,   207,  3610,   244,  4212,   452,   117,\n",
      "          532,  5133,   207,   483,   240,   110,   163,   904,   210,   687])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4393 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2142,   112,   115,   126,   113,   113,   910,   128,   213,\n",
      "          229,   532,   545,   216,   207,   518,   189,   210,   356,   174,\n",
      "          195,   280,   218,   207,  1508,  1547,   189,   292,   227,  4570,\n",
      "          238,  3625,  1333,   242,   207,   142,   198,   210,   207, 10104,\n",
      "          325,   115,   342,   165,   117,   163,   117,   147,   117,   142])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3634 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 3497,  210, 3399, 6109, 1346,  552,  335,  263,  230, 1002,  210,\n",
      "         207,  832, 3854,  215,  210,  220,  889,  216, 1219,  247, 2904,  211,\n",
      "         207, 2779,  375,  213,  207,  542,  210,  480, 3168,  117,  335, 4293,\n",
      "         216,  233,  246,  227,  598, 2064,  760,  442,  115, 1020,  115,  372,\n",
      "         207,  522])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5288 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1779,   112,   245,  4654,   222,   889,  9517,   211,   260,\n",
      "          595,   110,   163,   256,   115,   687,  2122,   223,  4337,   117,\n",
      "          105, 25724,   189, 11700,   174,   166,   117,  1561,  2370,  5907,\n",
      "         7020,   117, 26807,   188, 10671,   115,   521,   117,   115,  3978,\n",
      "          150,   117,   200,   174,  5893,   115,  6525,   111,   201,   475])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3811 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2974,  7630,   188,   117,   213,   231,  1695,   115,   207,\n",
      "         3036,   482,   216,   228,  2044,   245,  2140,   212,   245,  1049,\n",
      "          870,   687,   749,  6743,   386,   227,  5371,   145,  1408,   216,\n",
      "          750,  1540, 15835,   569,   346,   289,   189,   117,   221,   915,\n",
      "          221,   145,   885, 14061,   860,   210,   346,  1540, 19819,   687])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 339, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3519 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,   113,  6628,   128,   528,   218, 17798,\n",
      "          157,   117, 12989,   115, 10615,   865,   119,   126,   113,   160,\n",
      "          866,   128,   595,   115, 11702,   905,   147,   117,  2110,   115,\n",
      "         1536,   145,  7978,  1164,  1029,   529,   238,   648, 10748,   213,\n",
      "        14697,   171,  1484,   117,   306,  7667,   235,   226,   529,   222])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3827 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113,  160, 2837,  128, 1556,  637,  983,  207,  354,  265,\n",
      "        1313,  240, 7415,  239, 1531,  213, 3465, 9776,  213, 2015,  166,  117,\n",
      "        6222, 1824,  117, 1610,  117,  115, 6996,  165,  117,  163,  117,  951,\n",
      "         115, 2720,  163,  117,  756,  117, 4369,  866,  115, 2206,  156,  117,\n",
      "         149,  174])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3918 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   264,   210,  6411, 11702,   905,   147,   117,  2110,   115,\n",
      "          222,   275,   639,   767,   212,   222,   767,   210,   324,  2948,\n",
      "         3278,   115,   595,   115,   166,   189,   117,  2810,  7534,   469,\n",
      "          115,   145,  3021,   469,   115,   588,   117,   256,   230,   117,\n",
      "          147,   180,   116,   722,   116, 15883,   245,   498,   115,   634])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3752 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6411,   249,  3918,   227,   211,  2191,   301, 15599,   207,\n",
      "          830,   518,   525,   117,   420,   115,   226,   240,  1266,   116,\n",
      "          212,  2096,  3119,   116,   216,   207,  3465,  9776,   830,   518,\n",
      "          525,   223,  2681,   266,   213,  6411,   117,   198,   126,   113,\n",
      "          113,   113,   279,   128,   126,   113,   113,  6871,   128,   207])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5159 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  199,  117,  207, 1210,  215,  494, 1887,  117,  117,  207,  240,\n",
      "         213, 3465, 9776, 1468,  115,  213,  145, 2561,  115,  216,  213,  805,\n",
      "        3038,  237, 1977,  518, 1219,  219,  702,  211, 1069,  237, 5859,  347,\n",
      "         105,  343,  207,  830,  518,  126,  213,  226,  256,  207, 9178,  189,\n",
      "         212,  118])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4463 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   481,  2636,   145,   828,   116,  1440,   438,  1887,   117,\n",
      "          105,   253,   207,  2358,   329,  2898,   116,   222,   213, 25627,\n",
      "         7438,   188,   186,   503,   227,  2942,   221,   145,   828,   116,\n",
      "         1440,   438,   115,  2110,   110,   163, 13283,  2148,   210,  2898,\n",
      "          116,   222,   356, 10899,   465,   340,   117,   201,   117,   595])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 14 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 483, 865, 102,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1712 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  5520,   128,   528,   210,   207,   240,\n",
      "         7945,  8628,   115,   881,   865,   117,   236,   550,   213,   226,\n",
      "          954,   687,   347,   223,   207,  4139,  3729, 20717, 23199,  1064,\n",
      "          210,   207, 10104,  5859,   325,   212,   239,   352,   213,   226,\n",
      "          256,   117,   207,  1206,  4808,   595,   687,  4234,   189,   411])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4520 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1175,   210,   354,   265,   116,   487,  3603,   189,   117,\n",
      "          213,   308,   211,   468,   145,  1390,  4169,   115,   145,  2251,\n",
      "          313,   356,   307,   824,   207,  3603,   110,   163,  1394,   529,\n",
      "          225,   237, 19918,   394,   117,   207,  2251,   313,   110,   163,\n",
      "          250,   223,  2527,   126,   113,   113,   200,   128,  5157,   238])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3994 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,   866,   126,   128,   522,   585,  1150,   189,\n",
      "          217,   105,   220,   966,   347,   215,   814,   861,   242,   220,\n",
      "          325,   210,  2353,  7367,  2525,   215,  4648,   616,   212,  2525,\n",
      "          351, 12498,   212,  5508,   547,   117,   105,   508,   165,   117,\n",
      "          163,   117,   147,   117,   142, 22150,   111,   145,   112,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4622 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1130,  3740,   428, 16571,  9920,   115,  3425,   522,   986,\n",
      "          247,   263,  4082,   211,   549,   207,  1093,   616,  5859,  2532,\n",
      "          325,   117,   213,   289,   228,   256,   115,   532,   545,   207,\n",
      "          325,  3690,  2353,   110,   163,  1870,   211,  2700,   238,   207,\n",
      "        10104,   325,   670,   693,   227,   213, 14551,   235,   207,   354])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3675 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   111,   146,   112,   222,   670,   616,   215,   670,  2525,\n",
      "          225,  1093,  3932,   115,   210,   145,   322,  2104,   213,   228,\n",
      "          616,   215,  2525,   213,   207,   354,   265,   120,   212,   111,\n",
      "          199,   112,   228,   363,  2724,  1688,   211,   145,   388,   242,\n",
      "          207,   318,   210,   126,   207, 10104,   325,   128,   231,   268])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3470 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3932,   105,   215,   105,   573,   616,   215,  2525,   225,\n",
      "         1093,  3932,   117,   105,   204,   763,  6743,   115,   242,   207,\n",
      "         1093,   616,  5859,  2532,   325,   115,   207, 10104,  5859,   325,\n",
      "         1585,   211,   818,   105,  1918,   105,   573,   616,   215,   573,\n",
      "         2525,   225,  1093,  3932,   115,   271,   231,  6380, 20621,   189])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4670 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   478,   128,   152,   184,  5045,   126,\n",
      "          128,   207, 11114,  2613,   223,   331,   207,   818,   210,   894,\n",
      "          115,   227,   820,   115,  3684,   105,   573,   616,   215,  2525,\n",
      "          117,   105,   447,   117,   236,  1299,   116,  1400,   117,   207,\n",
      "         1093,   616,  5859,  2532,   325,   386,   227,  3822,   207,   425])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3697 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1130,  894,  493,  250,  189,  213,  354,  265, 1628,  189,  115,\n",
      "        1073,  207,  593,  696,  270, 3965,  222, 8431,  487,  213,  207,  354,\n",
      "         265,  244, 1758,  633,  105,  573,  189,  117,  105,  230, 1503,  215,\n",
      "         410,  126,  113,  113,  556,  128,  292, 1125,  288,  207,  354,  265,\n",
      "         218,  207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4016 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   820,   388,   216, 10520,   218,   354,   265,  1180,  2282,\n",
      "          189,   211,  4034,   250,   189,   493,   211,  1093,  2251,  1157,\n",
      "         5144,   189,   207,  1701,  1412,   117,   310,   216,  3949,   386,\n",
      "          227,  6632,   237,   105,   363,   105,   222,   354,   265,  2525,\n",
      "          117,   207,   832, 10520,   223,   207,   744,   217,   207,  4179])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4034 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  487,  222,  207, 1412,  210,  248,  199,  210,  207,  150, 8766,\n",
      "         171,  115,  207,  363,  222,  354,  265, 2525,  116,  116,  213,  226,\n",
      "         256,  115,  207, 1835,  907,  493,  218,  354,  265,  918,  217, 4636,\n",
      "         116, 5670,  410,  213,  207, 6980,  210,  126,  113, 5400,  128, 4480,\n",
      "         116,  116])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 419 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  619, 1280,  957,  218,  207,  308,  212,  225,  220,  619, 2575,\n",
      "         845,  218,  207,  308,  117,  105,  447,  117,  142, 6625, 4068,  111,\n",
      "         146,  112,  117,  552,  532,  944,  785,  307,  207, 6380, 1540,  115,\n",
      "         532,  261,  227,  785,  226,  691,  117, 1420,  117,  217,  207,  788,\n",
      "         853,  532])\n",
      "Original length: 91 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113, 21160,   128,   149,  7677,   115,   881,\n",
      "          865,   117,  4989,   150,   117,   200,   174, 22264,   115,   113,\n",
      "        22264,   120,   634,   165,   117,   163,   117,  2153,   117, 13648,\n",
      "          189, 25392,  2138,   115,   113,   113,   198,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3621 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 20653,   184,  4782,  2106,   243,  1409,   238,   145,  1336,\n",
      "         3925,  1408,   233,  2828,   210, 28840,  1161,  6563,   213,   683,\n",
      "          210,  6411,   264,   266,   212,  3356,   949,   210,   569,   107,\n",
      "          204,  1105,   115, 20440,   174,   211,   569,   107,   339,  1105,\n",
      "          117,   236,   550,   223, 20653,   184,  4782,   110,   163, 16254])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4561 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1016, 20653,   184,  4782,  2457,   353,   268,  1074,  1796,\n",
      "          210,   207,   327,   115,   225,   230,  1090,  2381,   348,   199,\n",
      "          117,   201,  1796,   117,   207,   820,   678,  1333,   351, 20653,\n",
      "          184,  4782,  3614,  2581,   210,   522,   212,   264,  5859,   266,\n",
      "          117,   207,   483,   240,   561,  1245,   870,   436,  1041,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4487 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   327,   126,   113,   113,   206,   128,   223,  2780,   210,\n",
      "          338,   216,   247,   480, 11936,  1051,   217,   207,   504,   217,\n",
      "          229,   335,   244,  1514,   116,   116,   405,   115,   355,   212,\n",
      "         8106,   547,   633,   117,   532,   323,  3212,   211,   207,  3956,\n",
      "         2745,   210,   207,   540,   210,   804,   215,  2007,   211,  1025])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4923 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   483,   240,   545,   216,   115,   552,   105,   346,\n",
      "         3466,   105,  4965,   216,   330,  2086,   805,  1808,   111,  2076,\n",
      "          115,  4588,  1808,   443,   355,   490,  4444,   189,   552,   335,\n",
      "          247,   230,  4444,   126,   113, 12786,   128,   446,   236,  1702,\n",
      "          215,   630,   112,   443,   297,   227,  4814,   211,  2414,  4444])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3778 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1975,   117,   105,   447,   117,   236,  5886,   111,   888,\n",
      "         2871,  2023,  1396,   112,   117,   207,   538,   211,  2765,  1907,\n",
      "         3216,  3466,   213,  2019, 12044,   467,   216,   207,   327,   236,\n",
      "          550,   111,   496,   210,  2414, 23265,   175, 21981,   212,   231,\n",
      "         6428,   175, 11609,   235,   189,   112, 15918,   195,   246,   145])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4790 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   588,   110,   163,  1151,   216,  1563,   211,  2007,   314,\n",
      "         1563,   211,   711,   116,  1830,   189,   223,   227, 14299,  5859,\n",
      "         1563,   117,   447,   117,   236, 11344,   120,  3599,   117,   403,\n",
      "         6748,  2156,   109,  3055,  1610,   117,   166,   117,   158, 19742,\n",
      "          195,   115,  2036,   150,   117,   199,   174,  8015,   115,  8525])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5093 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6832,   189,   190,   189,   211,   185,   247,   915,  2255,\n",
      "          216,   327,   213,   175, 16679, 10142,  8801,   395,  1920,   218,\n",
      "         1395, 13408, 12498,   222,  3440,  1472,   356,   219,   221, 19572,\n",
      "          210,   145,   677,   327,  2901,   111,   212,   420,  4048, 11673,\n",
      "          235,   211,  2765,   112,   221, 12498,   222,  3499,  1472,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5150 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   820,  1785,   211,   207,  1336,   236,   729,   216,\n",
      "        20653,   184,  4782,   115,   306, 18531,   235,   216,  1016,   246,\n",
      "         4144,  1893,   115,  2104,   213,   237,   241,   116,   303,  3386,\n",
      "          111,   385,   211,   221,   207,   126,   113,   113,   509,   128,\n",
      "          105,  6668, 11283,   105,   112,   211,  4409,   221,  1970,  1912])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4331 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5859,   534,   115,   105,   310,   216,   105,   799,   245,\n",
      "         1262,   226,   460,   253,   799,  1800,   233,  6368,   189,   211,\n",
      "         1596,   207,   592,   212,  2095,   210,   207,   693,   242,  5165,\n",
      "          117,   105, 20653,   184,  4782,  2042,   216,  2500,   210,   207,\n",
      "          460,   238,   207, 13097,   417,  2863,   207,   230,   577,   188])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4200 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3459,   117,   105,   296,   115,   207,  3648,  6124,   311,\n",
      "          219,   289,  1528, 10329,   212,  3515,   301,  1682,   221,   264,\n",
      "          663,   115,   212,   450,   115,   207,   663,   311,   219,  5557,\n",
      "         6056,   218,   207,   264,   117,   105,  2639,  2113,   115,  5611,\n",
      "          150,   117,   200,   174,   236, 21015,   111,   888,  2871,  2023])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4969 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   588,  6485,   239,  1151,   117,   117,   117,   222,  2016,\n",
      "          189,  1653,   218,   207,  1336,   117,   296,   115,   588,  2042,\n",
      "          216,  1003,   288,   207,   490,  4444,   327,   223,  7389,   117,\n",
      "          221,  1468,   218,   820,   115,   228,   237,  1151, 25905,   189,\n",
      "          213,   207,  2535,   210,   207,  1743,   210,  3865,  2229,   216])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3592 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5165,   117,   216,  1098,   252,  3262,   435,  9960,  2209,\n",
      "          216,   952,   210,   220,  2039,   115,   728,   324,   952,  1438,\n",
      "          216,   481,   219,   765,   211,   219,  3096,  9328,   115,   305,\n",
      "          219,   633,   105,  1783,   105,   212,  1724, 10391,   238,   207,\n",
      "         5859,   266,   117,   809,   115,   207,  1098,   252,  3262,   246])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5563 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  111,  258,  475,  682,  117,  722,  112,  111, 5932,  588,  110,\n",
      "         163, 3780, 1762,  388,  115,  728, 1917,  207,  949,  292, 4471,  207,\n",
      "        1140,  210,  207,  428,  646,  216,  481,  227,  219,  843,  174,  115,\n",
      "         552,  105,  207, 1336,  110,  163,  908,  246,  705,  281,  207, 1832,\n",
      "         210, 1683])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2043 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  221,  207, 1313,  240, 2255,  213,  380, 2392,  210, 1882, 4710,\n",
      "         189,  115,  152,  184, 5349,  126,  128,  105,  207,  847,  351,  229,\n",
      "         207,  126,  483,  240,  110,  163,  128,  308,  311,  219,  865,  174,\n",
      "         223,  331,  207, 1322, 1745,  145,  480, 1243,  210, 8517,  235,  207,\n",
      "        2327,  210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4777 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  327,  120,  216,  223,  115,  207, 2812, 4356,  839, 1325,  120,\n",
      "         212,  111,  199,  112,  145,  444,  424,  327,  116,  207,  816,  215,\n",
      "         839,  210,  737,  215,  410,  213,  229,  207,  424,  229,  223,  309,\n",
      "         211,  207, 6124, 2834, 4310,  189,  117,  105, 2708, 1517,  115,  521,\n",
      "         117,  166])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 516, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5447 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2414, 23265,   175,   110,   163, 11936,  1051,   225,   207,\n",
      "          231,   840,  1411, 14231,   211,   468,   233,   332,   210,   226,\n",
      "         6428,   175,  2228,   235,   327,   117,   105,   447,   117,   236,\n",
      "         2396,   117,   213,   226,   256,   115,   207,   820,  3194,   217,\n",
      "          145,  4533,   577,   327,   115,   289,  8993,   220, 12650,  7078])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 498, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4810 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   460,  8625,   105,  1564,   145,   885,  6480,   211,\n",
      "          960,  1927,   211,   145,  1336,   117,   117,   117,   117,   105,\n",
      "         5127,   166,   117,  2663, 10613,   115,   521,   117,   115,  5457,\n",
      "          165,   117,   163,   117,  5073,   115,  4997,   116,  5268,   115,\n",
      "          917,   156,   117,   149,   174,   117,   199,   174,  4174,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 442, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4356 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1396,   112,   117,   565,   207,   296,   332,   115,   207,\n",
      "          264,   944,   307,  1232,   207,   818,   211,  2008,   116,   672,\n",
      "          223,  4495,   117,  2639,  2113,   115,  5611,   150,   117,   200,\n",
      "          174,   236, 21015,   117,   221,  3124,   348,   115,  6411,  8625,\n",
      "         3984,   207,  1416,   117,   565,   207,   450,   332,   115,   264])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 556, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4825 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   210,   207,  1942,  3212,  1290,   115,   207, 13097,   246,\n",
      "         2993,   239,  1515,   212,  1701,   434,   211, 24384,   207,   490,\n",
      "        13659,   409,   212,   697,   117, 22501,   117,  2704,   117,  6551,\n",
      "          279,   236,   200,   117,   207,   240,   110,   163,  7522,   216,\n",
      "          237,   615,   481,   227,   247,   255,   484,   238,   784,  7716])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 554, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 179 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2153,  117, 9411, 4214,  116,  896,  111,  514, 1151,  112,  117,\n",
      "        1917,  207,  483,  240, 4893,  207, 1336,  211, 4950,  237, 1804, 5292,\n",
      "        2589,  235,  226, 3880,  115,  233, 1118,  211,  207,  944,  217, 8941,\n",
      "         225,  226,  460,  117,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 1183 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  528,  308, 1848,  213,  116,  332,  212, 4197,  213,  116,  332,\n",
      "         894,  110,  867,  211, 2310,  411,  264, 4196,  210,  207, 1977,  518,\n",
      "         820,  110,  393,  396, 6187,  759,  226,  602,  223,  373,  207,  240,\n",
      "         222,  894,  110,  867,  211, 2310,  411,  264,  266, 4196,  210,  207,\n",
      "        1977,  518])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3586 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   153,   117,  2791,   213,  5211,  1020,   115,  9059,   190,\n",
      "          835,   288,  3569, 17285,   225,  5609,  2507, 11034, 10282,   212,\n",
      "          628,   171,   194,   211, 27189,  1016,   213,   239, 27996,  4808,\n",
      "          212,  1382,   790,   327,   217,  5200,  8944,   189,   890,   117,\n",
      "         9059,   190,   110,   163,  2507,   115,   105,  6117, 15416,   115])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3610 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  470,  244, 4570,  221,  145,  691,  210,  266,  120,  111,  200,\n",
      "         112,  820,  110, 8437, 6750, 1346,  470,  242,  207,  534,  210, 6754,\n",
      "         212, 3465,  311,  219, 1629,  221,  145,  691,  210,  266,  120,  111,\n",
      "         201,  112,  820,  110, 4241, 5859,  470,  311,  219, 1629,  221,  145,\n",
      "         691,  210])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3256 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  820, 2148,  225,  267,  211,  165,  193,  189,  179,  244,  885,\n",
      "         211, 1204, 1566,  213,  324, 1556,  265,  236,  550,  117, 2642,  894,\n",
      "         110, 1689,  211,  207, 1004,  115,  165,  193,  189,  179,  223,  227,\n",
      "         488,  211, 5740,  145,  388,  242,  207,  534,  210, 6861,  307,  117,\n",
      "        1625,  115])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4325 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  469, 3580,  215,  854,  174,  218,  237, 2093,  559,  115, 8091,\n",
      "         215, 5508,  195,  215,  239,  363,  115,  830,  215, 1977,  115,  245,\n",
      "         115,  213,  260, 1052,  210,  228, 1563,  215,  854,  115, 2443,  117,\n",
      "         117,  117,  105,  112,  111, 2937, 1061,  112,  117,  221,  228,  115,\n",
      "         330,  223])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2456 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   225,   267,   211,   894,   110,   296,  1151,   115,   216,\n",
      "          820,  6640,  1061,   226,   388,   115,   207,   240, 12259,   216,\n",
      "          233,  1869,   228,  7588,  1779,   222,   207,   812,   213,   229,\n",
      "          820,   110,   245,  1836,   270,   759,   117,   126,   113,   433,\n",
      "          128,  2144,   115,   934,   207,   240,   261,   115,   222,  4082])\n",
      "Original length: 2793 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,   368,   128,  1531,   222,   867,   211,\n",
      "         2310,   207,   820,   115,  1852,  1985,  1197,  1175,   115,   884,\n",
      "          111,   105,  2162,   395,   105,   112,   115,  8072, 11841,   185,\n",
      "          111,   105, 11841,   185,   105,   112,   212,  8922, 10856,   383,\n",
      "          111,   105, 10856,   383,   105,   112,   115,  1979,   226,   347])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4542 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113,  113,  201,  128,  126,  113,  342,  128,  242,  207,\n",
      "         333,  210,  207, 1849,  316, 1190,  115,  207,  588, 4729,  918, 1769,\n",
      "         805,  850,  210,  207, 1636,  110,  418,  212,  323, 1115,  302,  677,\n",
      "         892,  211,  207, 1636,  211, 1294,  207,  418,  210,  207, 1849,  316,\n",
      "        1190,  115])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4442 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1176,   211,   789,   217,   207,   355,   211,   229,   233,\n",
      "          223,  1206,   119,   934,   330,   245,   219,   805,  2030,   210,\n",
      "         2941,   217,   289,   569,   207,   231,   115,   591,   297,   630,\n",
      "         2834,   117,   105,  6303,   116, 24595,  3831,   115,   521,   117,\n",
      "          166,   117,   793,   316, 13796,   117,  1610,   117,   115,   875])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4436 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   389,  1815,   649,  2022,   306,   218,   207,   820,\n",
      "          244,  5134, 10300,   238,   207,   256,   236,  1107,   117,   213,\n",
      "          207,   649,  1918,   207,   389,   115,   126,   113,   113,   279,\n",
      "          128,   207,   389,   246,   227,   207,   712,  7469,   235,   207,\n",
      "         5859,   534,   120,  1625,   115,   207, 24427,   189,   292,   207])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4413 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   145,   117,  2459,  4330,   388,   820, 14245,   216,   552,\n",
      "          207,  2148,  4959,   207,   421,   210,   207,  2459,  4330,   388,\n",
      "          244,   207,   376,   221,   324,  2189,   207,  5859,   470,   115,\n",
      "          207,  2459,  4330,   388,   311,  2827,   253,   207,  5859,   470,\n",
      "         2827,   117,   207,   240,   115,   361,  1629,   207,  5859,   470])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 617 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   750,   820,   323,  2827,   211,  1204,  1566,   211,\n",
      "         1979,   207,  2459,  4330,   470,   117,   220,   798,  1563,  2150,\n",
      "          218,   820,   223,   237,  1977,  1563,   212,   420,   356, 10899,\n",
      "         3193,  1566,   294,   211,  2459,  4330,   117,   321,  8157,   189,\n",
      "          166,   117,  6980,  1129,   535,   117,   115,  7861,   150,   117])\n",
      "Original length: 68 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5337,  8254,   117,  9008,   115,   113,  9008,   120,  5595,\n",
      "          145,   117,   199,   174,  9772,   115,   113,   113,  9772,   120,\n",
      "          634,  8254,   117, 13648,   189,  3864,   115,   113,   113,   113,\n",
      "          198,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4696 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  528,  126,  113, 7252,  128,  126,  113,  113, 9902,  128, 6113,\n",
      "        9010,  115,  147,  117,  154,  117,  207,  595,  115,  718, 6359,  243,\n",
      "         115,  521,  117,  115,  678,  226, 1497,  217,  145, 2112,  210, 2779,\n",
      "         294,  211,  336, 3031,  142, 1133,  116, 3668,  171,  111,  145,  112,\n",
      "         115,  198])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4324 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  111,  148,  112,  212,  263, 2104,  213,  237, 3427, 1243,  210,\n",
      "        1016,  212,  237, 3427,  325,  215,  126,  113, 8965,  128, 1178,  213,\n",
      "         207,  818,  210,  207,  616,  215, 2525,  210, 1809,  215, 4059,  235,\n",
      "        8282,  213,  683,  210,  336, 3031,  142,  925,  116, 2425,  172,  117,\n",
      "         201, 1015])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5181 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   222,  1687,   116,  1163,   115,  9734,  2522,   216, 15082,\n",
      "          109,  7057,  8513,  2127,   145,   916,   210,   207,  5718,  1871,\n",
      "         1161,   189,   216,   244,   537,   217,  1279,   238, 19107,  1242,\n",
      "          189,   117,   216,  8513,  3269,   353,   268,  1586, 10383,   174,\n",
      "         5718,  1871,  1161,   189,   117,   298,   207,   886,   115,   207])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3872 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 28610,   145,  7926,   216,   145,  6595,  6764, 20126,   235,\n",
      "          210,   220,   286,   212,   397,   229,   207,   588,   245,  3557,\n",
      "          297,  1736,   459,   460,  2907,  7286,   211,   126,   275,   128,\n",
      "          256,   117,   117,   117,   117,  1067,   223,   625,   369,   212,\n",
      "         1067,   207,   333,   210,   207,   371,   960,  1842,   217,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4738 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  4631,   602,   211,  2743,   606,   216,   207,   588,   212,\n",
      "          207, 19107,  1242,   189,   263,   228,  1236,   212,   216,   324,\n",
      "         1236,   271,   207,   421,   217,   145,   602,   210,   347,   117,\n",
      "          321, 12475,   166,   117,  5824, 16038,   115,  2559,   115,  3663,\n",
      "         8254,   117,   236,   204,   111,   152,   184,  4214,   126,   128])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4814 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  212,  115,  728,  253,  233,  263,  115,  216,  207,  388,  246,\n",
      "         227, 2536,  218,  207,  460,  117,  207,  595, 9578,  216,  233,  263,\n",
      "         280,  230,  228,  388,  117,  532, 1571,  247, 2255,  216,  152,  184,\n",
      "        2967,  126,  128,  105,  207,  319,  210,  145,  595,  211, 2443,  223,\n",
      "         488,  218])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4946 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5935,   216,   297,  4310,   225,   207,   588,   110,   163,\n",
      "         5718,  5935,   117,   207,   588, 18080,   216,   228,  7012,   318,\n",
      "          244, 15494,   301,   384,   212,  9597, 25284,  7368,  9627,   212,\n",
      "          115,  1015,   115,   216,   213,   207,   126,   113,  8058,   128,\n",
      "         1345,   210,   220,   460,   216,   207,   952,  5141,  1016,   221])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5199 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 16883,   189,   117,   117,   117,   117,   105,   447,   117,\n",
      "          236,   899,   117,   233,  1266,   115,  1625,   115,   216,   105,\n",
      "          226,   256,   386,   227,  3217,   145,  1090,  1068,   216, 24369,\n",
      "          189,   289,   680,   210,   664,   212,  1915,   189,   145,  1498,\n",
      "          211,  2170,   207,  5508,   195,   211,  1285,   117,   532,   247])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4981 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,  7847,   128,   532,   323,  2283,   216,   115,\n",
      "          242,   207,   525,   210,   744,   115,   207,   595,   249,   227,\n",
      "         1624,   239,  1874,   210,   859,  4631,   602,   211,  2743,   216,\n",
      "          105,   207,   832,   126,   113,   113,   113,   912,   128,   117,\n",
      "          117,   117,   232,  1514,  1160,   115,  1395, 13408,  1692,   281])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4440 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   446,   115,  2559,   115,  7695,   150,   117,  1772,   117,\n",
      "          236,  5351,   115,   390,   115,   105,   488,  7012,  2470,   244,\n",
      "          117,   117,   117,   207,  4524,   105,   213,   207,  5935,  1104,\n",
      "          117,  1296,   115,   228,   318,   244,  1309,  9597, 25284,  7368,\n",
      "         9627,   126,   113,   113,   113,  1043,   128,   212,   115,   221])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4901 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,  9118,   128,   207,   371,   223,  5133,   212,\n",
      "          207,   256,   223,  5661,   225,  2317,   211,  3190,   126,   113,\n",
      "          113,  7494,   128,   371,  4197,   207,  1497,   217,   145,  2112,\n",
      "          210,  2779,   117,   213,   226,   528, 13345,   212, 15048,  4488,\n",
      "          115,   154,   189,   117,   115, 19480,   174,   117,  7446,   218])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5435 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 17726,   115,   253,   227,  2404,   115,   211,   851,  1068,\n",
      "          211,  1683,   117,   117, 18299, 10634,  6909,   235,   535,   117,\n",
      "          166, 14019,  3288,   864,   736,   535,   117,   115,  6128,   165,\n",
      "          117,   163,   117,  8058,   115,  8990,   115,  1124,   163,   117,\n",
      "          756,   117,  9604,   115,  1147,   156,   117,   149,   174,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4439 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2859,   353,   268,   145,  3036,  4666,   120,   300,   311,\n",
      "          323,  1596,   216,   330,   223,   805,  2502, 22525,   452,  3466,\n",
      "          210,  4885,   117,   207,   595,   944,   227,   115,   390,   115,\n",
      "          264,   260,   388,   225,   927,   961, 22237,   120,   300,   944,\n",
      "          307,   284,   387,   889,   216,  4285,  2147,   216,   300,   249])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3227 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   308,   211,  1025,   216,   741,   261,   219,  4589,   175,\n",
      "          211,  2587,   216,   347,   117,   226,   879, 20518,  6650,   228,\n",
      "          145, 18455,   212,  6087,   971,   210,   818,   117,   450,   115,\n",
      "          253,   207,   595,  5468,  1979,   189,   145,  2112,   210,  2779,\n",
      "         2642,   207,  1026,  2695,   284,   218,   207,  1375,   213,   207])\n",
      "Original length: 764 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,  1531,   222,   894,   110,   867,   217,  1152,   371,\n",
      "          595, 19531,   195, 27713,   469,   111,   105, 19531,   195,   105,\n",
      "          112,  3907,   212,  5341,   207,   871,  3206, 10235,   111,   105,\n",
      "          871,  3206,   105,   112,   115,   145,  2873,   116,  1964,   115,\n",
      "          584,   116,  6129, 19658, 27713,   213,   476,  6962,   350,   247])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4129 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1178,   115,  1330,   213,   207,   867,  6598,  1104,   221,\n",
      "         1307,   105,  3667,   105,   569,   382, 27713,   189,   115,   223,\n",
      "          145,   105,   480,   115,  3336,   115,  1481,   116,  2010,   316,\n",
      "         1178,   105,   212,   420,   386,   227,  2488,  2459,  4330,   117,\n",
      "         4850,   435,  4293,   216, 19531,   195,   110,   163,   759,   223])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4752 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   233,   223, 26432,   115, 26944,   115, 23869,   115,   215,\n",
      "        13420,  3272, 22099,   120,   111,   200,   112,   331,   233,  3244,\n",
      "         1436,  1563,   211,  2765,   126,  2381,   189,   215,   231,   316,\n",
      "         3277,   128,   117,  8378,   195,   110,   163,  2703,   156,   117,\n",
      "          156,   117,   147,   117,   166,   117,  6206,  1522,   110,   163])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4344 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2945,   117,   163,   676,   117,   416,   117,   213,  3860,\n",
      "          115,   207,  4850,   110,  1115,   460,   216,   207, 27713,   189,\n",
      "          446,   207,   376,  3081,   117, 29302,  1444,   111,   207,   871,\n",
      "         3206,   212,  1596,  8901,  1852,   247,   184,   244,   203,   117,\n",
      "          205,  5837,  4200,   112,   117,   200,   487,   222,   207,   460])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4085 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3667,   189,   223,   237,  3427,  1178,  3820,   236,  4389,\n",
      "          126,   113,   342,   128,  1794,   115,   380,  7037,   890, 19658,\n",
      "        27713,   189,   569,  4807,   215,   972,   301,  1056, 27713,   189,\n",
      "          115,  2248,   210,   331,   207,   972, 27713,   223,   213,  1436,\n",
      "         1016,   225,   207,   380, 27713,   212,  2248,   210,   331,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3140 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   447,   117,   236,  6492,   111,  2937,   213,   856,   112,\n",
      "          111,  3284,  7914,   188, 10456,   117,   166,   117, 14540,   383,\n",
      "          115,  7551,   150,   117,  1772,   117,  2367,   111,   148,   117,\n",
      "         7651,  1755,   112,   112,   117,   126,   113,   433,   128,   152,\n",
      "          184,  4564,   126,   128,   207,  2467,   325,   386,   227,  3430])\n",
      "Original length: 290 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  7681,   128, 20320,   160,   117,  2821,\n",
      "          115,   881,   865,   117,   392,   649,   115,   229,   532,   247,\n",
      "         1609,   217,   504,   210,   226,   528,   115,   241,  2140,   303,\n",
      "          210,   237,   832,  3854,   213,   207,  1301,   189,   211,  4034,\n",
      "          207,   405,   210,  5119,  8056,   236, 11421,  1026,  2005,   222])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4502 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   793,   477,  1472,   117,   226,   327, 10690,  2575,   212,\n",
      "          656, 18107,   174,   207,   405,   210,   207,   338,  1536,   218,\n",
      "          207,   820,   115,   640,   189,   210,  5119, 15157,   175,   115,\n",
      "         5119,  4549,   115,   212,  4810,  5119,   115,   443,   247,  6047,\n",
      "          217,  2581,   210,   207, 10104,   325,   115,  5372,   115,   212])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4797 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1865,  5119, 15157,   175,   211,   237,   156,  4015,   215,\n",
      "         2326,   194,  4457,   120,   253,   145,  3839,   223,   915,   115,\n",
      "          741,   245,  2948,  1842,   213,  1865,  5119, 15157,   175,   238,\n",
      "          145,  4457,   117,   552,   210,   226,   115,   207,   405,   210,\n",
      "         1865,  5119,   115,   283, 15157,   175,   115,  4549,   115,   212])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5154 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   588, 17574,  5437,   469,   223,   145,  6466,  1457,   126,\n",
      "          113,   113,   206,   128,   469,   216,  3750,   211,  4034,   212,\n",
      "         1069,   207,   405,   210,  5119,   236, 11421,  1026,  2005,   238,\n",
      "          732,  1154,   211,   557,  1099,   115,   241,   225,   237,  5210,\n",
      "          211,  6750,   235,  1092,   213,   239,   887,   221,   145,   430])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4994 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1893,  1507,   207,   483,   240,   614,  5807,  5785,  5261,\n",
      "          110,   163,   867,   211,  2310,   241,   470,   216,   207,  4810,\n",
      "         3942,   263,  1125,   351,   233,   222,   207,  1300,   216,   207,\n",
      "          820,   292,   309,   211, 11737,   550, 10820,   383,   222,   207,\n",
      "        18479,   461,   210,   270,  1304,   221,  1977,   518,   189,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5292 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   271,   237,   293,   116,   207,   116,   482,  1588,   211,\n",
      "          207,  4810,  3942,   211,  1979,   536,   840,   211,   239,  1626,\n",
      "          213,   207,  1230,  1641,   351,  5807,  5785,  5261,   117,   321,\n",
      "         6167,  7988,  1610,   117,   115,  1001,   150,   117,   200,   174,\n",
      "          236,  5941,   111, 12203,   235,   343,   595,   263,   230,  1588])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5361 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   145,   525,   297,  4224,   213,   289,  4905,  5776, 22575,\n",
      "          241,  2381,  3728,   487,   222, 17430,  1748,   116,   145,  2393,\n",
      "          216,   805,  5859,   207,  1242, 10863,   247,  8946,   115,   310,\n",
      "          145,  2393,   216,   207,  1313,   240,   249,  2081,   484,   117,\n",
      "          211,   207,  1004,   115,   207,   240,   249,   280,   233,   969])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3743 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 4823,  211, 1240,  270, 8567, 6387,  189,  288,  207,  717,  327,\n",
      "         236, 1602,  301, 1857,  907,  117,  447,  117,  236, 9797,  117,  207,\n",
      "         483,  240,  614,  145,  867,  211,  126,  113, 7666,  128, 2310,  115,\n",
      "        1408,  221,  145,  691,  210,  266,  216,  207, 3317,  189,  110, 3159,\n",
      "         292, 1977])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5451 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1563,   212,   331,   233,   246,   289,  2353,  1637,   211,\n",
      "         4392,   120,   111,   201,   112,   207,   830,  1268,   210,   207,\n",
      "         1563,   120,   111,   202,   112,   207,  8116,   817,   210,   207,\n",
      "          949,   120,   212,   111,   203,   112,   207,   803,   210,  6807,\n",
      "         1762,   215,  3197,   854, 10803,   117,   447,   117,   236,  8316])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5154 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  405,  210, 5119,  246, 3178,  115,  233,  223,  798,  216,  805,\n",
      "         210,  603,  245,  247, 2150,  230, 1377,  636,  826,  236,  241,  117,\n",
      "         213, 1739,  115,  207, 4335,  817,  210,  207,  949,  335,  247, 2150,\n",
      "         223, 8116,  117,  207, 4810,  126,  113,  113,  774,  128, 3942, 2732,\n",
      "         211, 3720])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5300 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   303,   216,   207,   907,   210,  5119, 15157,   175,   222,\n",
      "          207,   156,  4015,   212,  2326,   194,  3275, 23235,   174,   117,\n",
      "          532,  2827,   211,   321,  2715,   226,  1000,   117, 17574,  5437,\n",
      "         1536,  8056,   222,  2326,   194,   211,  3414,   459,   207,   405,\n",
      "          222,   216,   382,   477, 11421,   115,   212,   207,   907,  2278])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4900 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   212,   223,   230,   353,   744,   211,  4074,  2278,  4627,\n",
      "          212,  4416,  9354,  1762,   268,   207,   482,   216,   805,   518,\n",
      "          189,  1219,   247,  6011, 15157,   175,   236,   907,   227,  8746,\n",
      "          211,   324,   222,   591,  2326,   194,   215,   207,   156,  4015,\n",
      "          117,  2948,   115,   532,  4264,   207,   894,   110,  1151,   216])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5514 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2388,   115,  2584,   397,   907,  4168,  7856,   212,  1607,\n",
      "         1338,  2632,   189, 30518,   207,  2326,   194,   405,   211,   366,\n",
      "          217,   228,  1674,   221,   207,  1387,   828,   210,  1011,   288,\n",
      "        15157,   175,   115,   229,   213,  1736, 10507,   189,   487,   222,\n",
      "         1010,   115,  1222,   115,   212,   997,  6909,   235,   212, 19327])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5162 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  327,  126,  113,  113, 1133,  128,  261,  219, 6807,  174,  213,\n",
      "         270, 1427,  218,  949, 2150,  213,  207, 8056,  327,  117,  420,  115,\n",
      "         307, 8056, 3839,  189,  115,  212,  227,  717,  327, 2593,  115,  305,\n",
      "         219,  702,  211, 2443,  117,  207, 4930,  235,  207, 7791, 2378,  174,\n",
      "         218,  207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5125 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   264,   949,   211,  1977,   518,   189,   212,   522, 20440,\n",
      "          949,   211,   830,   518,   189,   112,   117,   253,   207,  1374,\n",
      "          210,   207,   264,   240,   347,  6417,   189,   145,  2906,   236,\n",
      "          241,   211,   392,   820,   115,   233,   297,   219,   213,   207,\n",
      "          817,   210,   388,   215,   550, 10820,   383,   117,   321, 11938])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 516, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4874 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   217,  2984,   189,   216,   297,  6807,   324,   210,  2278,\n",
      "         4627,   112,   115,   207, 22929,  2730,   223,   211,  5102,   289,\n",
      "          116,   310,   307,   289,   116,   680,   210,   518,   189,   126,\n",
      "          113,  8065,   128,   213,   207,  1865,  5119,   327,  2443,   117,\n",
      "          487,   222,   241,   207,   460,   537,   222,  1152,   371,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 498, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4698 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   327,  2593,  8366,   207,  1563,  1977,   215, 12309,  3079,\n",
      "          115,  2559,   236,   912,   116,   910,   117,  1700,   115,   532,\n",
      "          247,   765,   216,   207,   949,  1430,   244,   227,   211,   185,\n",
      "         8116,   215,  3197,   115,  2559,   236,   984,   116,  1043,   117,\n",
      "          236,   226,   564,   532,   356,  3339,   210,   307,   289,   798])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 442, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4763 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   146,   117,   550, 10820,   383,   119,  5807,  5785,  5261,\n",
      "          207,   483,   240,  1629,   207,  4810,  3942,   110,   470,   351,\n",
      "         5807,  5785,  5261,   222,   550, 10820,   383,  1030,   117,   152,\n",
      "          184,  6678,   126,   128,   211,  2587,   216,   550, 10820,   383,\n",
      "         1585,   115,   207,   588,   311,  1204,   216,   111,   198,   112])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 556, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4253 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   351,   233,   552,   233,   307,   520,   252,   212, 27985,\n",
      "          174,   207,  3854,   317, 17574,  5437,   212,  1506,   117,   310,\n",
      "         6590,   631,   223,   227,  7137,   211,   264,   237,   105, 15005,\n",
      "          212,  3701, 21557,   177,   105,   256,   117,   239,  3949,   223,\n",
      "          216,  5807,  5785,  5261,   246,   145,   897,   213,   207,  3854])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 554, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2349 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113, 7850,  128,  226,  223, 2710,  301,  207,  256,  225,\n",
      "         207, 2326,  194,  212,  207,  327,  217, 1865, 5119,  117,  728, 1917,\n",
      "         207, 1375, 5809,  211, 8220,  207, 3694,  189,  238,  145, 1097,  830,\n",
      "        1389,  317,  207, 8056,  212,  207, 1865,  189,  327,  111,  212, 2429,\n",
      "         550,  225])\n",
      "Original length: 1376 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,  1003,   222,   894,   110,  3883,   211,  2310,   393,\n",
      "          396,   759,   595,  4829,   152,   117, 11165,   383,  1240,   189,\n",
      "          212,  2675,   189,   207,   355,   210,  7849,  1175,   211,  3068,\n",
      "         7869,  2884,   238,  1205,   314,  1260,   207,  1821,   189,   216,\n",
      "          244,   407,   213,  4320,  1175,   117, 11165,   383,  2421,   216])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4439 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   153,   117,   207, 17907, 11728,   894,   110,   867,   213,\n",
      "          444,   332,   115,   152,   184,   866,   126,   128,   248,  1027,\n",
      "          111,   145,   112,   210,   207,  6908,  3670,   325,   783,   145,\n",
      "          966,  1249,   351,   145,   322,   105,   443,   115,   222,   215,\n",
      "          213,   445,   225,   220,   737,   215,   410,   115,   100,  3214])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4329 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   199,   117,   105,   319,   936,   115,  7849,  1205,   723,\n",
      "          189,  1175,   244,   242,   724,   212,   261,  1893,   219,   484,\n",
      "        30518,   207,   327,   117,   105,   200,   117,   105,   324, 13188,\n",
      "          918,   244,   105,  3811,   116,   218,   116,  4909,   105,   963,\n",
      "          216,  5679,   110,   164,  6410,   270,  2130,   215,  1089,   117])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3650 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1106,   363,   213,  3961,   215,  1367,   235, 12518,   383,\n",
      "          212,  2884,  3766,   117,   105,   393,  1824,   117, 16846,   190,\n",
      "          117,   160,  1043,   117,  7158, 13556,   905,   987,   237,   105,\n",
      "          221, 11315,   175,   105,   663,   513,   117, 11165,   383,  3503,\n",
      "          207,   513,   223,  3049,   213,   428,  2119,   119,   105,   296])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4758 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3432,   535,   116,  3436,   117,   221,   189,   110,   158,\n",
      "          115,  6757,   150,   117,  1772,   117, 24019,   115, 25641,   111,\n",
      "          163,   117,   148,   117,   213,   174,   117,  1464,   112,   115,\n",
      "         5198,   110,   148,   115,  9863,   150,   117,   199,   174,  1586,\n",
      "          111,   204,   475,   682,   117,  1301,   112,   111,  1848,   588])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 119 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  340, 1677,  117,  254,  119,  732,  339,  115,  634, 3125,  150,\n",
      "         117, 7895, 5548,  115,  865,  354,  265,  483,  240, 3426,  483,  210,\n",
      "        5720,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 2278 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  528, 2231,  212,  308, 3655,  152,  117, 4104,  175,  195,  426,\n",
      "         115,  483,  865,  119,  820,  244, 5871, 1547,  189,  212,  497, 1499,\n",
      "         215,  270,  600,  189,  117,  588, 5637,  540,  118,  993,  410,  115,\n",
      "         521,  117,  111,  105, 5637,  105,  112,  223,  207, 5601,  190,  790,\n",
      "        2025,  217])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4723 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,   207,  5871,  1104,   115,   145,  2025,   110,   163,\n",
      "         4660,   245,   698,   207,   105,  6046,   737,   105,  7149,   216,\n",
      "         1442,  2584,   840,   228,   221,  2888,   115, 11713,   189,   115,\n",
      "        15617,   212,   231,  5573,   395,   115,   215,  5871,  1547,   189,\n",
      "          443,   427,   840,   238,  6046,   667,  7149,   117,   111,  1824])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4964 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  820, 4327,  216,  207,  231, 1674,  247,  878,  225, 5637,  216,\n",
      "         335,  261,  227,  479, 1876, 1636,  216, 5637,  249, 2329,  211,  479,\n",
      "        1876,  728, 1917,  335,  438,  252,  211, 2214,  324, 1636,  110,  479,\n",
      "        2124,  213, 2025,  235,  608,  117,  111, 1824,  117, 7233,  117,  160,\n",
      "         186,  925])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4841 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   429,   210,  5733,   273,   115,   390,   115,   263,\n",
      "          230,  1160,  1576,   222,   207, 14549,  5142,   210,   207,   148,\n",
      "          176,   182, 10667,   182,   918,   117,   111,  1824,   117,  7233,\n",
      "          117,   160,  1323,   117,   112,   218,  2912,  1020,   115,   207,\n",
      "          148,   176,   182, 10667,   182,   918,   292,  6367,   761,   804])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4795 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  153,  117,  867,  211, 2310, 1327,  152,  184,  866,  126,  128,\n",
      "         222,  145,  867,  211, 2310,  294,  211,  525,  279,  111,  146,  112,\n",
      "         111,  203,  112,  115,  145,  240, 4596,  301,  311, 1599,  207,  397,\n",
      "         889,  832,  213,  207,  759,  221, 1377,  212, 8347,  241,  480, 6295,\n",
      "         213,  145])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3420 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   426,   117,   540, 21215,   820,  4327,   540, 21215,   818,\n",
      "          221,   558,  1131,   212,   525,   210,   744,  2581,   117,   111,\n",
      "         1824,   117,  3755,   117,   160,   186,   896,   115,  1288,   112,\n",
      "          120,   321, 30517,   118,  7959,   166,   117,  1615,  2882,   115,\n",
      "         3643,   150,   117,   200,   174,  4587,   115,  5016,   111,   199])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4407 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1069,   189,   216,   233,  8093,   220,  1958,   211, 30228,\n",
      "          175,   225,   207,   231,  1674,   552,   207,   832,  3854, 15852,\n",
      "          189,   239,   636,   695,   117,   111,  5637,   110,   163,  5918,\n",
      "          117,   213,  1772,   117,   236,   205,   117,   112,   152,   184,\n",
      "         2967,   126,   128,   728,   253,   207,  1286,   832,   246, 15846])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4480 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5637,  2329,   211,   479,  1876,   145,  1547,   115,   207,\n",
      "          231,  1674,   297,  3395,   211,  1876,   216,  1547,   110,   163,\n",
      "          479,   221,   705,   117,   218,  2167,   213,   207,   832,  3854,\n",
      "          115,  5637,   110,   163,   535,   116, 10815,   189,  1787,   270,\n",
      "          639,   327,   315,   117,   111,  1824,   117,  7233,   117,   160])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4616 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   212,   111,   199,   112,   207,  3007,  1128,   215,  1476,\n",
      "          210,   216,   643,   221,  6171,   238,  2403,   475,   215,   645,\n",
      "          221,   145,  2998,   210,   145,  3638,   424,   115,   316, 12174,\n",
      "         3277,   115,   215, 12310,   126,   113,   415,   128,  2753,   117,\n",
      "          105,   354,   265,   166,   117, 27661, 14273,  1610,   117,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4936 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   111,  1154,   112,   117,   126,   113,   774,   128,  1395,\n",
      "        13408,   818,   223,   105,   207,   355,   210,  5508,   195,   643,\n",
      "          115,   390,  4490,  1405,   115,   211,  8693,  1016,   115,   211,\n",
      "         2417,   145,  2010,  2237,   115,   215,   211,  6768,   145,  2381,\n",
      "          117,   105, 23812,   174,  6911,   189,   117,   535,   117,   166])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4451 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   682,   117,  1020,   112,   111,  3284,   146,  6718,   166,\n",
      "          117,   158,  4015, 11429,   189,   117,   115,   521,   117,   115,\n",
      "         9439,   150,   117,   199,   174, 22774,   115, 26547,   111,   206,\n",
      "          475,   682,   117,  1658,   112,   120,   321,   323,  6870, 14459,\n",
      "        19854,  3455, 21142,   547,   166,   117,  9621,  5866,   115,   521])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4360 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   148,   176,   182, 10667,   182,  2048,   207,   393,  8376,\n",
      "          210,   207,  8493,   893,   117,  1762,   213,   226,   256,   297,\n",
      "          960,   750,  2387,  1683,   218,   260,   210,   207,   918,  1423,\n",
      "          218,  5637,   110,   163,   832,  1395, 13408,  6563,   117,  1015,\n",
      "          115,  5637,   110,   163,   867,   211,  2310,   148,   176,   182])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4126 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   820,  3194,   216,  5637,  4414,   145,  3077,   838,   372,\n",
      "          233,  3446,  7822, 11253,   577,   216,   233,   297,  1088,   211,\n",
      "         2170,   700,   479,   212,  1096,   211,   465,   340,   117,   111,\n",
      "         2945,   189,   117,  5918,   117,   213,  8031,   117,   236,   516,\n",
      "          117,   112,   152,   184,  4978,   126,   128,   211,   264,   145])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3419 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  528, 2231,  109,  308, 3489,  115,  483,  865,  119,  820,  115,\n",
      "         624,  383, 5210, 5355,  115,  521,  117,  111,  105,  624,  383,  105,\n",
      "         112,  212, 9232,  905, 3352,  115,  521,  117,  111,  105, 9232,  905,\n",
      "        3352,  105,  112,  111, 1462,  115,  105,  820,  105,  112,  115,  126,\n",
      "         113,  199])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4498 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   212,  5859,  3757,   189,   117,   447,   117,   236, 15413,\n",
      "          117,   298,   226,   273,   115,   207,   522,   881,   561,   126,\n",
      "          113,   202,   128,   237,   308,  6390,   207,  2411,   351,   894,\n",
      "          216,   226,   240,   263,  1146,   213,   713,  1138,   117,   213,\n",
      "          207,   530,  4179,   115,   820,  4327,   216,   894,   247,  1904])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4633 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  222,  692,  433,  115, 1020,  115,  894,  678,  145,  450,  396,\n",
      "        1804,  212, 3757,  189,  117, 1507,  115,  820,  678,  145,  867,  217,\n",
      "         371,  222,  207, 4098,  211, 2310,  411,  210,  894,  110, 3757,  189,\n",
      "         115,  283,  207,  584,  530,  301,  373,  207,  240,  117,  934,  226,\n",
      "         867,  246])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4705 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   522,  5859,   534,   117,   321,   342,   165,   117,   163,\n",
      "          117,   147,   117,   142,   342,   152,   184,  2567,   126,   128,\n",
      "          111,   105,   220,   322,   443,   224,   219,  3580,   213,   275,\n",
      "          316,   215,   359,   218,   744,   210,  1898, 11446,   715,   213,\n",
      "          207,  5859,   534,   245,  4691,  2482,   213,   220,   483,   240])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4469 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   444,   327,   424,  2834,  4310,   117,   105,  2838,   109,\n",
      "         3489,   115,   521,   117,   166,   117,   300,  7699,   171,  1702,\n",
      "         6514,   189,   521,   117,   115,  1099,   165,   117,   163,   117,\n",
      "         6394,   117, 13648,   189, 11376,  1570,   236,   113,   416,   111,\n",
      "          163,   117,   148,   117,   158,   117,   169,   117,   463,   498])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5294 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  5045,   126,   128,   213,  5964,   207,   954,\n",
      "        11939,   658,   210,   237,   543,   115,   207,  1549,  2613,   223,\n",
      "         2427,   211,   216,   329,   222,   145,   867,   211,  2310,   294,\n",
      "          211,  1954,   117,   162,   117,  3195,   117,   160,   117,   279,\n",
      "          111,   146,   112,   111,   203,   112,   115,   229,   226,   240])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5211 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   331,   207,  4090,   263,  4991,   115,   105,   212,   343,\n",
      "          105,  5172,  6187,   270,  1350,   211,  1399, 15746,  1198,   189,\n",
      "          210,  4090,  4375,   117,   105,   207,   818,   213,   207,   530,\n",
      "          256,  2641,   386,   227,  1688,   211,   207,   680,   596,   218,\n",
      "          207,   450,   881,   211,   219,   105,  2974,  1497,   235,   100])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5313 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   356,   264,   145,  1023,  5518,   388,   211,   207,   464,\n",
      "          216,   233,  3503,   216,   126,  3720, 19808, 17460,  9144,   128,\n",
      "          247,  2104,   213, 10442,  1641,   117,   105,   112,   117,   809,\n",
      "          115,   820,   110,   867,   211,  2310,   894,   110,  1023,  5518,\n",
      "         3757,   223,  1463,   117,   426,   117, 11628,  2295,   210,  3588])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4038 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  6678,   126,   128,   225,   267,   211,   207,\n",
      "          904,   210,   331,   145,   450,  1333,   223,  4570,   218,  3520,\n",
      "         8082,   115,   207,   482,   216,   606,  3728,  1325,  3096,   207,\n",
      "          376,   971,   210,  4797,   818,   223,   227,  4463,   120,   696,\n",
      "          223,   233, 11114,   216,   207,   428,   417,  1325,   207,   376])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 88 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113, 10545,   128,  2231,   212,   308,  5635,\n",
      "          146,   117,   162,   117,  9471,   115,   113,  9471,   120,   634,\n",
      "          165,   117,   163,   117,  6394,   117, 13648,   189,  3896,   973,\n",
      "          115,   113,   113,   198,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 200, 210, 204, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2913 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   153,   117,  2500,   226,   223,   145,   942,   615,   213,\n",
      "          237,  8078,   814,   225,   145,   915,  1961,  3176,   117,   207,\n",
      "          256,  4051,   372,   207,  1324,   115,  3080,   147,   117, 11011,\n",
      "          171,   174,  2970,   111, 11011,   171,   174,  2970,   112,  6047,\n",
      "          894,  9773, 10551,   189,  9172,  3745,   115,   521,   115,   111])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 204, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4221 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   222,   647,   508,   115,  1099,   115,   207,   276,  1211,\n",
      "          145,   105,   370,   116,  1016,   232,   105,   294,   211,   207,\n",
      "          158,  2504,   115, 11011,   171,   174,  2970,   212,   207, 15274,\n",
      "          189,   878,   126,   113,   113,   201,   128,   216,   335,   297,\n",
      "          105,   227,  4310,   115,   656,   215,  1419,   115,   213,   220])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 204, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3054 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5220,   216,  9773, 10551,   189,  1246,   241,   210,   239,\n",
      "          277,   242,   207,   158,  2504,   211, 11204,   115,   207,   942,\n",
      "          240,   503,   227,  8357,   213,  1108,   216, 11204,   223,   207,\n",
      "          307,   257,   225,  1566,   211,  1960,   207,   158,  2504,   117,\n",
      "          146,   117,  2660,   210,   207,   158,  2504,   198,   117,  5060])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 204, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3963 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   809,   115,   207,   893,   217,   331,   215,   227,   145,\n",
      "          370,   116,  1016,   232,   223,  1938,   223,  5104,   117, 26342,\n",
      "         3455,   166,   117,  4452,   115,   554,  7221,   117,   342,   111,\n",
      "         4297,  1570,   112,   115,   145,   256,   987,   218,   207,   942,\n",
      "          240,   115,   284,   387,   226,  1393,   525,   115,   221,  1468])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 204, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1569 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2154,   115,   537,   236,  6174,   193,   117,   159,   252,\n",
      "          117,  1871,   117,  1438,   115, 11011,   171,   174,  2970,  1193,\n",
      "          207,   517,   210,   207,  5054,   918,   211,   126,   113,   113,\n",
      "          342,   128,   207,  1712,  8901,   276,   242,   207,  2347,   212,\n",
      "          906,   232,   115,   212,  1130,   207,   276,   503,   227,  4299])\n",
      "Original length: 1742 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113, 10492,   128, 17878,   178,   152,   117,\n",
      "         4219,   115,   881,   865,   119,  8114,  1495,  2963,  2421,   216,\n",
      "         1852,  6519,   189,  1066,  2963,   115,   145,   408,  2218,   115,\n",
      "          249,  2863,   207, 10104,  5859,   325,   212,  5733,  3031,  1406,\n",
      "         5508,   547,   212,  3427,   616,  1748,   117,   207,   483,   240])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3868 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   105,   164,   195,   235,   105,   210, 16536,   212, 18431,\n",
      "         1495,   244,  2581,   210,   606,   522,   212,   264,  5859,   534,\n",
      "          117,   719,   117, 17504,   110,   163,   323,  3503,   216,  1852,\n",
      "         6519,   189,  2329,   211,  2373,   145,  4386,   439,   232,   225,\n",
      "          719,   117, 17504,   110,   163,   115,  2329,   211,  2373,   145])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3670 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  8405,   111,   199,   174,   149,   174,   117,   109,   634,\n",
      "         1772,   117,   112,   117,   310,  1486,   207,   483,   240,   115,\n",
      "          532,  1800,   216,   719,   117, 17504,   110,   163,   388,   210,\n",
      "        28840,  1161, 12982, 10479,  2048,   222,   239,   639,   333,   117,\n",
      "          207,   483,   240,  1468,   216,   152,   184,  2567,   126,   128])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3774 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,  7494,   128,   207,   483,   240,  1629,   207,\n",
      "         3854,   388,   552,   105,   221,   145,   691,   210,   266,   115,\n",
      "          145,   469,   212,   239,   313,   126,   153,   117,   149,   117,\n",
      "          115,  1852,  6519,   189,   212,  7590,   864,   128,   244,  7388,\n",
      "          210, 21322,   225,   289,   789,   211,  2488,   207,  5859,   534])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4202 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   327,   643,   210,  1852,  6519,   189,   213,   207,   164,\n",
      "          195,   235,   327,   217, 16536,   780,  1495,  2641,   246,   227,\n",
      "          632,   117,   321,  8761,  3012,   115,  1586,   150,   117,   200,\n",
      "          174,   236,  4788,   117,   213,  1140,   115,   207,   483,   240,\n",
      "          110,   163,  1675,   213,   227,  3429,   719,   117, 17504,   110])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 300 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3563,   316,  3483,   117,   719,   117,   126,   113,   113,\n",
      "          422,   128, 17504,   110,   163,   249,   227, 10329,  2715,   207,\n",
      "          483,   240,   110,   163,  1601,   292,  1528,  4337,   117,   426,\n",
      "          117,  1113,   217,   207,   788,   853,   115,   532,  2283,   216,\n",
      "          207,   483,   240,   503,   227,  8357,   213,  6214,   207,   595])\n",
      "Original length: 2244 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  8384,   128,  1531,   109,   308,   226,\n",
      "          691,   223,   373,   207,   240,   222,  3883,   217,  1152,   371,\n",
      "          678,   126,   113,   113,   200,   128,   218, 11209,   383,   476,\n",
      "          189,   469,   111,  3192,   117,  8326,   112,   115,   207,   526,\n",
      "          211,   494, 10037,   235,   111,  3192,   117,  9130,   112,   115])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2778 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   145,  2259,  9283,   126,   113,   113,   202,   128,   210,\n",
      "          207,   399,   261,   462,   805,  1158,   117,  2505,  5526,  8307,\n",
      "          235,   212,   159,   190,   172,   244,  8491,   301,  3984,  1472,\n",
      "          213,  5733,   115,   225,  1393,  3425,   389,   116,   957,  2593,\n",
      "          117, 10226,   212,  9715, 22308,  5416,   111,   105,   207, 22308])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2992 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   111,   342,   112,  5320,   890,   174,   171, 12235,   120,\n",
      "          111,   404,   112,   219,   184,  3324,   120,   212,   111,   416,\n",
      "          112,   167,   117,   147,   117,  3612,  1386,   184,   117,  3060,\n",
      "         6745,   476,   189,  7851,   470,   351,  3458,   210,   392,   856,\n",
      "          894,   117,  3060,  6745,   476,   189,   878,   211,  2310, 20649])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3516 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6405,   207,  1214,   614,   211,  3060,  6745,   476,   189,\n",
      "          222,   463,   258,   115,  1167,   117,   202,   207,   744,   217,\n",
      "        11209,   383,   476,   189,   110,  1795,   302,   223,   216,   239,\n",
      "         2438,   323,   494,   207,   307,  5257, 24694,   216,   246,  1029,\n",
      "          213,   207,   839,   236,   207,   259,   117,   126,   113,   113])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4262 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5456,  7532,   223,   207,   707,   210, 22111,   117,   300,\n",
      "          246,   323,   145,   915,   116,   259,   533,   210, 11209,   383,\n",
      "          476,   189,   212,   115,   293, 11209,   383,   476,   189,  3043,\n",
      "          115, 12011,  4473,  1383,   117, 13902,   117,   210,  5456,  7532,\n",
      "          236,   337,   116,   368,   117,   217,   606,   918,   115,  7532])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4918 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6752,  7402,   117,   233,   223,  2680,   216,   207, 22308,\n",
      "         5416,   395,   493,   207,   384,  2112,   189,   213,   226,  1641,\n",
      "          212,   216, 12577,   195,  2382,  1386,   184,   212,   275,   266,\n",
      "         1685,  1475,   207,   384,   630,   117,   230,   289,  3371,   189,\n",
      "          216, 19330,   183,   246,   145,  7413,   116,   595,   117,   258])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4202 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   145,   119,   705,   115,   233,   246,   332,   210,  1357,\n",
      "         2926,   117,   207, 28485,   189,   153,  2752,   246,   216,   207,\n",
      "          159,   190,   172,   189,   292,   213,   116,   116,  9197,   211,\n",
      "          219,   484,   569,   117,   161,   119,   216,  3060,  6745,   476,\n",
      "          189,   246,  9197,   211,   500,   569,   116,   116,   145,   119])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4305 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  207,  240,  223,  936, 5706,  225,  207,  461,  331, 3060, 6745,\n",
      "         476,  189,  249, 1758,  763,  239,  470,  242,  207,  207, 7791,  210,\n",
      "        1762,  233,  249, 1206, 1613,  212,  253,  340,  115,  331,  233,  249,\n",
      "        1564,  885,  460,  211, 2480, 1152,  371,  117,  210, 1024, 2250,  211,\n",
      "         226,  284])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4293 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1009,  207, 1156, 1030,  316,  695,  117, 1782,  115,  335, 2863,\n",
      "         142, 4456,  111,  147,  112,  115,  236,  684,  117, 1700,  115,  595,\n",
      "        3503,  145,  602,  210,  347,  242,  142, 3992,  111,  147,  112,  221,\n",
      "         145,  257, 3580,  218,  744,  210, 1156, 1030,  110,  142, 4456,  111,\n",
      "         147,  112])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4438 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  218, 2618,  175, 5892,  235,  207,  986,  111,  215,  231,  389,\n",
      "         325, 1242,  189,  112,  115, 2872,  372,  972, 3244,  210,  347, 2086,\n",
      "         211, 2433,  207,  376,  711,  117,  342,  126,  113,  113,  912,  128,\n",
      "         233,  223, 5599,  235,  211, 3194,  216,  142, 3992,  111,  147,  112,\n",
      "         210,  207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4855 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 8566,  115,  207, 2412,  311,  219, 4447,  210,  145,  359,  302,\n",
      "         117, 3060, 6745,  476,  189, 3503,  216,  207,  894,  284,  303,  211,\n",
      "        6085,  233,  210,  239,  359,  117,  233,  936, 5550,  175, 3503,  216,\n",
      "         207,  894, 1637,  211, 6085,  207,  389,  210,  145,  359,  302,  116,\n",
      "         116,  728])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4294 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6803,   165,   117,   163,   117,   236,  8995,   116,  1317,\n",
      "          117,   152,   184,  4214,   126,   128,   207,   256,   266,   216,\n",
      "          249,  2403,   184,   459,  2854,   142,   198,  4722,   189,   411,\n",
      "          409,   217,   213, 16875,   174,   191,  5934,  5859, 10815,   189,\n",
      "          340,   216,  2211,  3165,   189,   465,   227, 13280,  1035,   981])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4635 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113, 10751,   128,   231,   986,   247,   407,   207,\n",
      "          376,   853,   211,  2283,   216,   152,   184,  4019,   126,   128,\n",
      "          330,   356,   219,   230,   142,   198,  3854,   317,   289,   469,\n",
      "          212,   789,   469,   216,   233,  2358,  2292,   117,   830,  2644,\n",
      "         1610,   117,   166,   117, 29193,   715,  3834,   117,   109,  3834])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3119 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   541,   128,   200,   117,  5456,  7532,\n",
      "          115,  4986,  8317, 13335,  6022,   109, 12577,   195,  2382,  1386,\n",
      "          184,  2948,   115,  5456,  7532,   115,  4986,  8317, 13335,  6022,\n",
      "          115,   212, 12577,   195,  2382,  1386,   184,   244,  7388,   210,\n",
      "        21322,   225,   207, 22308,  5416,   395,   212,   207,  1215,  1499])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4908 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   802,   207,  1064,   210,   207, 22308,  5416,   395,   110,\n",
      "          399,   117,   207,   460, 10597,   301,   635,   189,   207,   388,\n",
      "          216,   335,  4131,   126,   113,   113,  1124,   128,  1067,   207,\n",
      "        22111,   850,   210,   216,   399, 10444,   117,   606,  1211,   402,\n",
      "          212,   878,   211,   465,   340,   591,   303,   210, 10722,   211])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 516, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4271 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1782,   115,   330,   223,   230,   460,   216,   392,   428,\n",
      "          820,  1125,   667,  1270,  3728,   213,   270,   639,   695,   351,\n",
      "          126,   113,   113,  1323,   128,   207,  5108,  2889,   210,   207,\n",
      "         3060,  6745, 11637, 20867,  1533,   117,   222,   207,  1004,   115,\n",
      "          233,   223,  2680,   216,   207, 22308,  5416,   395,   115,   555])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 498, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1752 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   221,  2395,   218,   207,   105,  4267,   319,   105,   211,\n",
      "          105, 10348,   410,   105,   242,   142, 16407,   115,   441,   126,\n",
      "          113,   113,  1342,   128,   212,   207,   522,  9273,  1181,   478,\n",
      "          126,   113,   113,  1433,   128,   212,   522,  1190,  9273,  1181,\n",
      "         3031,   117,   516,  8842,   126,   113,  7494,   128,   247,  3222])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 442, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4575 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   216,   392,   428,   325,  1242,   189,   244,   207,   428,\n",
      "          325,  1242,   189,   329,   218,   216,   248,   217,   237,  5859,\n",
      "          105,   438,   115,  1614,   115,   126,   100,   128,   215,  3854,\n",
      "          117,   105,   213,   216,   256,   115,   207,   231,   894,  1219,\n",
      "          219, 11933,   211,   207,   464,   216,   233,   356,   219,  3857])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 556, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5034 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  207,  856,  759,  213,  226,  347, 3503,  216,  105,  207,  348,\n",
      "         735,  889, 2859,  237, 2732,  809,  994, 2973,  218,  207,  894,  211,\n",
      "        1915,  145, 5579, 7417,  195,  126, 6457,  128,  213,  207, 5526, 8307,\n",
      "         235, 1104,  281,  207,  444,  327,  115,  241,  213,  683,  210,  522,\n",
      "        5859,  266])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 554, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4797 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   351,   603,   117,   207,   240,   261,   227,   936,  1262,\n",
      "          207,  1258,   210,   207,   142,   199,   470,   126,   113,   113,\n",
      "         1321,   128,   351,   392,   894,   552,   207,   276,   247,   227,\n",
      "         1560,   603,   213,   220,   432,   117,   426,   117,   867,   217,\n",
      "         1680,  1152,   371,   109,  3883,   213, 16587,   175,   207,   894])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 508, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 267 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   111,  3192,   189,   117,  8502,   115,  7853,   115,  8322,\n",
      "          115,   212,  7589,   112,   244,  1463,   115,  1917,   207,  1511,\n",
      "          894,   245,   665,  2304,  3640,   324,  3883,  1831,   207,   142,\n",
      "          199,   470,   115,   253,   220,   115,   244,   353,  1097,  2268,\n",
      "          117,   126,   113,   113,  1147,   128, 22227, 18765,   175,   115])\n",
      "Original length: 2622 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  5092,   128,  3921,   115,   881,   865,\n",
      "          119,  1233,   152,   117, 26786,   212,  1495,   195,   148,   117,\n",
      "          149,  6454,   115,   486,   213,   942,   217,  5072,   195,   115,\n",
      "          521,   117,   115,   111,  1462,   115,  5072,   195,   112,   615,\n",
      "          207,   483,   240,   110,   163,  2957,   242,   522,   525,   210])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4013 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 23804,  4578,   238,   207,  7978, 19068,  4961,   120,   111,\n",
      "          199,   112,   145,  2323,   351,  2625,   235,   207,   914,  7978,\n",
      "        21217,  6833,   120,   111,   200,   112,   207,  3639,   210,  3492,\n",
      "        12141,   188,   111,   153,   175,   112,   115,  2810,  7534,   110,\n",
      "          163,  3492, 28229,   577,  2060,   115,   212,   231,   352,  2060])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4062 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   221,   145,   588,   117,   204,  5072,   195,  4347,   239,\n",
      "         2148,   210,  1395, 13408,   818,   212,   950,   145,   388,   216,\n",
      "         2810,  7534,   110,   163,  4222,   608,   105, 22160,   215,  8746,\n",
      "          207,   708,   210,  2810,  7534,   110,   163,  5262,  3689,   115,\n",
      "          207,  3492, 12141,   188, 28229,   577,   115,   225,  2810,  7534])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4928 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1541,   117,   105,   112,   117,   105,  1296,   115,   207,\n",
      "         2148,   311,   219,   763,   213,   333,   216,   244,  1073,  7640,\n",
      "          696, 13283,   117,   105,  1471, 18298,   117,   535,   117,   115,\n",
      "          368,   150,   117,   200,   174,   236,  3816,   116,   441,   117,\n",
      "          105,  1130,   532,   261,  2214,   216,   207,   820,   356,  2587])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4226 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   111,  1099,   112,   111,   105,   934,   207,  7654,   245,\n",
      "          631,   239,  2775,   189,   225,   207, 10163,   189,   221,   332,\n",
      "          210,   145,  1090,   232,   115,   145, 10163,   245,   219,   560,\n",
      "         2641,   225,   275,   215,   502,   639,   872,   117,   105,   112,\n",
      "          117,  5072,   195,   386,   227,  3194,   126,   113,   113,   422])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4514 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   695,   211,   824,   288,   142,   198, 22055,   547,   225,\n",
      "         2810,  7534,   115,   532,   499,   216,  5072,   195,   249,   152,\n",
      "          184,  2758,   126,   128,   832,   216,  3755,   171,   187,   212,\n",
      "         7664,   571,   365,   212,   231,   771,   213,   477,   217,  2902,\n",
      "          288,   207,  4222,   608,   117,   392,  2148,   244,   885,   217])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4692 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   556,   128,   152,   184,  4290,   126,\n",
      "          128,   213,   207,   525,   210,   744,  1355,   115,   105,   207,\n",
      "         5104,   210,   145,  6124,   223,  5817,   487,   222,   239,  1576,\n",
      "          222,  1016,   221,   145,   945,   281,   207,   444,   327,   117,\n",
      "          105,  6976,   189, 21340,   115, 10673,   150,   117,   199,   174])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3156 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  7349,  1016,  1305,   117,   117,   117,   117,   253,   227,\n",
      "          115,   207,  2613,   223,   236,   237,   711,   120,   207,  1178,\n",
      "          223,  2542,   105,   112,   120,   204,   244,   252,   171,   109,\n",
      "        27593,   184, 12932,   160, 16268,   172,   115,   236,  6305,   111,\n",
      "          105,  7373,   241,   986,  1799,   207,   525,   210,   744,   960])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4557 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   164,   117,   166,   117,   115,   521,   117,   166,   117,\n",
      "          151,   190,   175, 26937,   179,   171,   115,   521,   117,   115,\n",
      "         5802,   165,   117,   163,   117,   910,   115,  1133,   158,   117,\n",
      "          433,   115,  1291,   115,  1124,   156,   117,   149,   174,   117,\n",
      "          199,   174,  8675,   115,  1191,   163,   117,   756,   117,  5629])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4386 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   984,   128,  5072,   195,   296,  2042,\n",
      "          216,  2810,  7534,   110,   163,  1106,   327,   643,   213,   207,\n",
      "         2060,  1472,   223,   885,   211,  2859,  1395, 13408,  1692,   213,\n",
      "          324,  1472,   314,   374,   211,  3755,   171,   187,   110,   163,\n",
      "          215,  7664,   110,   163,   643,   215,   617,   213,   207,   160])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4690 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   324,  5340,   343,   643,   223,   145,  9936,   211,  1108,\n",
      "          237,   232,   211,   219,   237,  3615,  6124,   210,   616,   126,\n",
      "          242,   142,   198,   128,   117,   117,   117,   233,   297,   468,\n",
      "          230,  3466,   211,  1231,   207,   376,   232, 11737,   211,   142,\n",
      "          199,   314,  1683,   210,   643,   117,   105,   112,   117,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4479 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1130,  5072,   195,  3503,   216,  2810,  7534,   110,   163,\n",
      "          608,   225,  3755,   171,   187,   212,  7664,  2050,  1514,  1395,\n",
      "        13408,  1692,   115,   233,   386,   227,   462,   220,  2238,   421,\n",
      "          211,   635,   226,  3949,   117,   516,  1296,   115,  5072,   195,\n",
      "          249,   280,   969,   216,   233,   249,   230,  2249,   210,  1283])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5464 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   728,   253,   233,   481,   219,  1582,   216,   207,   640,\n",
      "         1810,   275,   405,   213,  1598,   211,   115,   212,   213,   207,\n",
      "          291,   210,   115,   207, 22429,   212,   216,   275,  1932,   210,\n",
      "         1714,   212,   563,   804,   263,   227,  1507,  6069,   115,   330,\n",
      "          297,  1253,   207,  5691,   213,   189,  3272,  4482,  4333,  4534])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3998 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5072,   195,   110,   163,   388,   223,  2146,   213, 24458,\n",
      "          238,   207,   388,   242,   731,   213,  3465,  9776,   115,   212,\n",
      "          239,  3338,   210,   145,  3854,  3949,   223,  2661,   211,  8052,\n",
      "          207,  3465,  9776,   525,   117,  2262,   115,   126,   113,   113,\n",
      "         1291,   128,  5072,   195,  2042,   216,  3465,  9776,  5431,   227])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4201 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213, 26218,   175, 23033,   115,   207,  2189,  2306,   246,\n",
      "         3854,   211,  1150,   892,   242,   207,   380,  3090,   325,   218,\n",
      "         3049,   212,  3399,   768,   117,   207,  2963,   210,   207,  3854,\n",
      "          246,   145,  1910,  2615,  3501,   115,   443,   246,   213,   207,\n",
      "          316,   210,  2757,   235,  3399,   301,  1013,   892,   217,   275])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 516, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5014 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  260,  126, 2676,  128,  246, 1930,  216,  241,  292,  213, 1594,\n",
      "        1016,  212,  216,  314, 1305, 8416,  347,  225,  267,  211,  207, 1265,\n",
      "         117,  117,  117,  330,  246,  803,  210,  145, 1436,  826,  210,  207,\n",
      "         316,  212,  667,  261,  210,  207,  117,  117,  117,  675, 1242,  189,\n",
      "         115,  310])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 498, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5279 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 16199,   577, 25897,   115,  3266,   163,   117,   756,   117,\n",
      "          236, 10801,   111,  1108,   216,   595,   944,   227,  5818,   889,\n",
      "          859,   145,  4191,  4474,   256,   210,   551,  1669,   115,   213,\n",
      "          332,   552,   233,   297,   219,   105,   521,  4675,  6474,  2589,\n",
      "          211,   960,   145,   595,   115,   213,   308,   211,  2480,   145])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 442, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4243 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  889,  229,  115,  253, 5108,  115,  297, 4995,  700,  211, 1322,\n",
      "         117,  105,  447,  117,  236, 3472,  158,  117,  205,  117,  310,  115,\n",
      "         213,  216,  256,  115,  532, 5133,  207,  483,  240,  110,  163, 2957,\n",
      "         210,  207,  759,  115, 9757,  235,  216,  105,  207, 1313,  240,  249,\n",
      "         763,  216])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 556, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5275 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   111,   227,   235,  1887,   310,  8998,   235,   211,   549,\n",
      "          233,   552,   207,   459,  3631,  2933,   292,   227,  2702,   221,\n",
      "          894,   112,   120,   213,   665,  3208,  5859, 14150,   117,   115,\n",
      "         2721,   150,   117,   199,   174, 19307,   115, 19184,   111,   202,\n",
      "          475,   682,   117,  1851,   112,   111,   376,   112,   120, 15795])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 554, 210, 554, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2160 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   229,  5262,  3277, 14142,   174,   225,  1547,   189,   117,\n",
      "          212,   330,   261,   219,   230, 12309,  3079,   548,   117,  1296,\n",
      "          115,  1455,  8945,  3606,   210,  1010,   212,  1222,   356, 10723,\n",
      "          220,  2732,   211,  1008,   949,   116,   116,   105,   233,   223,\n",
      "          227, 11223,  1199,   218,   207,  2312,   210,  3511,   547,   117])\n",
      "Original length: 424 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  4297,   128,  6966, 17612,   115,  1122,\n",
      "          881,   865,   119,   226,   223,   145, 12970,   256,   211,  1148,\n",
      "          210, 19504,   184,   115,  1896,  4813,   115,   145,  3418,   469,\n",
      "          166,   117,   145,  3290,  1522,   110,   163,  1432,   115,  1646,\n",
      "          120,   145,   176,   172,   210, 19504,   184,   115,  1646,   120])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4611 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   226,   256,  5185,   238,  6889,  1477,   189,   213,   207,\n",
      "         1148,   210, 19504,   184,   115,  1896,  4813,   565,   993,   915,\n",
      "          825,  1608,   212,   808,   211,   207,  1148,   110,   163,   350,\n",
      "         3658,   126,   113,   113,   199,   128,  2963,   218,  1906,  2104,\n",
      "          213,   216,  1243,   210,  1608,   117,   198,   207,  1148,  3295])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4812 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   210,   522,   585,   117,  1148,   210, 19504,   184,   115,\n",
      "         1896,  4813,   115,  8334,   117,  1610,   117,   115,   166,   117,\n",
      "          145,  3290,  1522,   110,   163,  1432,   115,   521,   117,   120,\n",
      "          145,   176,   172,   210, 19504,   184,   115,   521,   117,   115,\n",
      "         8627,  4452,   212,   207,  1608,  2046,  2639,   116, 10395,   115])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4863 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2917,   216,  1040,  1350,   236,  2020,  2654,   292,   227,\n",
      "         2973,   117,   300, 10163,   210,   275,  5264,   189,   210,   798,\n",
      "         2984,   211,   207,  1148, 16314, 25125,   183,   115,   229,   246,\n",
      "         1686,   211,   207,  8299,   189,   115,   212,   211,   207,  1148,\n",
      "          110,   163,   695,   213, 11197,  5960,   117,   300,   735,   299])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4649 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   145,   117,   207, 10442,  1887,   152,   184,  2567,   126,\n",
      "          128,   230,   577,   188,  3459,   386,   227,   549,   211,  1497,\n",
      "          189,   215,  4179,   189,   216,   244,   145,   105,  3036, 10442,\n",
      "          211,  1294,  1067,   223,  1512,  1569,   353,   268,   237,  2732,\n",
      "          211,  3950,   656,   225,   207,   316,  1389,   189,   210,   145])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4329 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1113,  565,  162,  176,  171,  110,  163, 6984, 3066,  115,  207,\n",
      "         483,  240,  765,  216,  162,  176,  171,  246,  227,  145, 2381,  210,\n",
      "         207,  820,  117,  216, 1408,  223,  227, 1528, 4337,  117,  146,  117,\n",
      "        4611,  383,  115, 3078, 1270,  115, 6616,  115,  212, 6592,  145,  176,\n",
      "         172, 2421])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4341 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   950,   115,   212,  1653,  5679,  4869, 10614,   110,   163,\n",
      "         1115,   117,  4050,   186,   110,   163,   505,   503,   227,  2326,\n",
      "          459,   213,   226,  7444,   117,   145,   176,   172,   321,   189,\n",
      "          226,   221,   145,  3910,   212,   145, 10976,  2732,   211,   468,\n",
      "          145,  3017,   222,   767,   210,  4050,   186,   117,   207,   483])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4577 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   230,   577,   188,   116,  6699,  3155,   246,   487,   740,\n",
      "          211,   350,  4403,   264,  3593,   470,   217,  9624,  2328,   115,\n",
      "         3427,  1016,   115,   212, 11628,  2295,   225,  3588,   636,  2237,\n",
      "          115,   487,   222,   207,   376,  1312,   210,  1497,   235,   207,\n",
      "          389,   216,  4570,   207,   522,  5859,   388,   117,   532,   944])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1041 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   146,   117,  7311,   860,   207,  2148,   216,   145,   176,\n",
      "          172,  6252,   292,  3049,   212, 12479,   244,   207,   376,  2148,\n",
      "          280,   213,   145,   176,   172,   110,   163,  5859,   388,   115,\n",
      "          212,   335,  2827,   117,   152,   184,  4019,   126,   128,   145,\n",
      "         1496,   223, 12479,   307,   253,   233,   223,  3049,   212,  6368])\n",
      "Original length: 406 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,   113,   282,   128,   126,   113,  6016,\n",
      "          128,   465,  5203,   195,   115,   154,   117,   226,   223,   237,\n",
      "          615,   238,   207,   729,   240,   110,   163,  2957,   210,   145,\n",
      "          687,   347,  1333,  1125,   242,   207, 14865,  2078,  2067,   325,\n",
      "          111,   166,   173,   176,   171,   215,   207,   325,   112,   115])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4519 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  529,  229,  595, 1536,  961,  116, 4933,  213,  145, 1038, 2608,\n",
      "         238,  145, 2608,  126,  113,  113,  113,  199,  128, 1547,  111, 1640,\n",
      "         856,  877, 1547,  215, 9178,  112,  117,  487,  222,  207,  354,  265,\n",
      "        1313,  240,  273,  213, 3465, 9776,  535,  117,  166,  117, 3465,  115,\n",
      "        6901,  165])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5102 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  447,  117,  142, 5983,  866,  111,  146,  112,  111, 2937, 1061,\n",
      "         112,  117,  105, 2078,  105,  223,  512,  218,  207,  325,  221,  119,\n",
      "         152,  184, 2837,  126,  128,  220,  322,  443, 4478,  115, 2428,  115,\n",
      "         952,  217,  115,  215,  378,  694,  211,  490,  731,  217,  737,  215,\n",
      "         410,  227])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4852 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6083,   189,   117,   253,   532,  3033,   145, 20556,   195,\n",
      "         1121,   115,   207,  2078,   481,   227,  2745,   207, 13094,   210,\n",
      "         2078,  2067,   117,  2810,  7534,  4965,   216,   789,   273,   115,\n",
      "        19937,   166,   117, 11218,  1111,   109,  5206,   604,   535,   117,\n",
      "          115,   635,   189,   126,   113,   113,   113,   258,   128,   145])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5282 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6996,   165,   117,   163,   117,   236,  2234,   117,   207,\n",
      "          240,   983,  6435,   226,   603,   175,   119,   152,   184,  5484,\n",
      "          126,   128,   105,   372,  5911,  1758,   115,  3465,  9776,   246,\n",
      "          145,   273,  9018,   235,   207,   522,  5859,   534,   115,   227,\n",
      "          145,   273,  5415,   235,   207, 19811,  7030, 10705,   317,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4725 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  226,  240,  249, 1072, 1950,  142, 5307, 1570,  111,  146,  112,\n",
      "         211,  819,  216,  105,  213, 9018,  235,  207,  325,  115,  532, 3212,\n",
      "         211,  207, 7380, 5914,  578,  333,  212,  318,  210,  207,  522,  616,\n",
      "         250,  325,  117,  117,  117,  117,  105, 9763,  115, 4076,  166,  190,\n",
      "         117,  236])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5339 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   543,   115,   532,  1800,   216,   152,   184,  5618,   126,\n",
      "          128,   325,  1237,   503,   227,  5969,   145,   581,   213, 14865,\n",
      "          266,   212,   216,   237,  1977,   518,   246,   575,   211,  1979,\n",
      "          145,   602,   210,   347,   242,   207,   166,   173,   176,   171,\n",
      "          367,   211,   207,   722,   543,   117,   532,   247,  3626,   174])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3927 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   519,   242,   207,   278,  1275,   117,   321,  3322,\n",
      "        20376,   166,   117,  2810,  7534,  1610,   117,   115,  4761,  8254,\n",
      "          117,  1123,   115,  9064,   145,   117,   199,   174, 21383,   115,\n",
      "        20219,   115, 20839,   126,   113,  5051,   128,   111,  8254,   117,\n",
      "          634,   112,   111,   552,   210,  2499,  6259,   860,   454,   115])\n",
      "Original length: 1558 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  528,  126,  113, 4232,  128, 2231,  528,  530,  301, 1282,  373,\n",
      "         207,  240,  223,  145,  857, 4618,  835,  218,  588, 2810, 7534,  469,\n",
      "         212,  207,  595,  265,  210,  350,  871,  115, 3422,  115, 3465,  115,\n",
      "        6401,  115, 5733,  115, 4591,  115, 5060,  115, 1852, 4813,  115,  212,\n",
      "        6861,  111])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  368,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4735 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   232,  2318,  2995, 15480,   189,   145,  3744,   120,   213,\n",
      "          477,   217,   207,  9611,   210,   828,   212,  6054,   210,   803,\n",
      "          115,   207,   276,   260,   851,   459,  6161,   335,  1219,   247,\n",
      "        11965,   263,   335,  2617,   252,   225,   207,  1641,   117,   105,\n",
      "          447,   117,   236, 22264,   111,  3284,   354,   265,   166,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  342,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4548 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   354,   265,   483,   240,   217,   207,   483,   210,  3634,\n",
      "          354,   265,   210,  1760,   115,   595,   115,   166,   117,  2810,\n",
      "         7534,   469,   115,   588,   117,   966,   347,   230,   117,  1164,\n",
      "          116, 23667,   111,   147, 13703,   112,  2231,   528,  1511,   213,\n",
      "          226,   256,   217,   207,   240,   110,   163,   904,   223,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  404,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3473 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2617,  211,  237, 6241,  886,  222,  207,  550,  210, 1249, 2493,\n",
      "         225,  805,  210,  207,  820,  213,  264,  210,  350,  871,  115,  149,\n",
      "         190,  117, 1742,  117,  166,  117, 2810, 7534,  126,  113,  113,  258,\n",
      "         128, 1610,  117,  115,  199,  207,  354,  265,  212, 2810, 7534, 3203,\n",
      "         207,  736])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  416,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5072 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2153,  117,  148,  117,  147,  117, 6402,  115, 1185,  150,  117,\n",
      "         200,  174, 2471, 2758,  115, 3472, 2758,  111,  148,  117,  147,  117,\n",
      "         682,  117, 1138,  112,  111, 3284, 2191, 3176,  112,  117, 1015,  115,\n",
      "         152,  184, 1570,  126,  128,  142,  404,  111,  149,  112,  210,  511,\n",
      "         342, 3282])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  422,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3549 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   889,   210,  5859,  2581,   481,   219,   228,   221,   211,\n",
      "         3190,   207,  1664,   110,   145, 23116,  9746,   210,  1263,   643,\n",
      "          117,   110,   105,  2418,   117,  2477,   210,   266,   236,   212,\n",
      "         2100,   115,   521,   117,   166,   117,   354,   265,   115,  6099,\n",
      "          165,   117,   163,   117,  2153,   117,   148,   117,   147,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  433,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3821 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,  1024,   649,   115,  1263,  1355,   210,   207,   408,\n",
      "          302,   213,   145, 10737,  4771,   325,   814,  3148,   189,   115,\n",
      "         4614,  7080,   301,   115,   225,   237,  1163,   210,   207,  2148,\n",
      "          764,   303,   213,   207,   759,   117,   321,   115,   149,   117,\n",
      "          151,   117,   115, 14876,   383,   115,  9900,   150,   117,  1772])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  339,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3383 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   690,   189,   216,  2765,   261,  1088,   211, 12158,   233,\n",
      "          569,   231,  1029,  1175,   117,   105,   447,   117,   552,   105,\n",
      "         1425,  1029,   529,   249,   870,  7319,   189,   115,   105,   206,\n",
      "         1196,   436,   217,   289,  1029,   529,   261,   227,  2285,   222,\n",
      "          789,  1029,   126,   113,   113,   442,   128,   529,   621,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  441,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4574 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 17430,   115,   145, 28840,   110,   163,   325,   311,   247,\n",
      "          237,   110,  1395, 13408,   363,   117,   110,   216,   223,   115,\n",
      "          233,   311,  2984,   207,  2010,   736,   212,  1724,  2984,  2765,\n",
      "          117,   105,   447,   117,   236,  1306,   111,  2937,   213,   856,\n",
      "          112,   117,   450,   115,   207,   595,   311,   105,  2859,   216])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  478,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4533 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   927,   853,   105,   229,  1948,   207, 13293,   213,   145,\n",
      "          105,  3425,   303,   210,   207,  5691,   345,   358,   210,   808,\n",
      "          235,   207,  3492,   117,   105,   447,   117,  4752,   150,   117,\n",
      "          200,   174,   236,  1456,   111,  2871,  2023,  1396,   112,   117,\n",
      "         1408,   216,   820,   263,  1073, 11503,   252,  2810,  7534,   110])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  516,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4231 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   607, 28229,   577,   117,   447,   117,   111,  3284,\n",
      "         1601,   210,   482,   160,   186,  3864,   116,  1133,   112,   117,\n",
      "          294,   211,   226,   376,   232,   115,  4799, 10102,   227,   211,\n",
      "         7667, 26223,  1242,   391,   207,   105,   607,  2439,   115,   105,\n",
      "          212,   227,   211,   105,   811, 18429,   189,   217,   370,   126])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  498,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4432 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  4080,   183,   117,   105,  2810,  7534,   115,  4752,   150,\n",
      "          117,   200,   174,   236,  1147,   117,   435,   235,   239,  1350,\n",
      "          211,  6369,   207,  1687,   116,  4683,  3910,   210, 21808,   211,\n",
      "          207,  7978,  4683,   115,  2810,  7534,  5617,   105,  8946, 11781,\n",
      "          227,   211,  2907,  3347,   218,  4059,   235, 11781,   110,   163])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  442,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4937 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  529,  210, 1084,  216, 2238, 2596,  311,  219, 2752,  213,  237,\n",
      "        1505,  240,  212, 3323,  555,  729,  116, 1486, 6241,  417,  117,  105,\n",
      "         447,  117, 4752,  150,  117,  200,  174,  236, 1973,  117,  207, 3517,\n",
      "         240, 1653,  207,  483,  240,  110,  163, 1113,  216, 6241,  417,  297,\n",
      "         227,  219])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  556,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4502 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   237,  1035,   826,   211,  1016,   216,  2221,   211,   219,\n",
      "         7712,   115,  7573, 10623,   661,   189,   351,  3861, 11631,  2571,\n",
      "         1322,   117,   321,   200,   244,   252,   171,   109, 27593,   184,\n",
      "        12932,   115,  5859,   266,   160,  8649,   172,   115,   236,   917,\n",
      "          116,   882,   111,   105,   353,  4663,  2125,  1322,   115,  1750])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  554,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4421 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 29829,   176,   180,   142,   426,   117,   145,   117,   207,\n",
      "          454,   435,  1379,  2810,  7534,   211,   462,   436,   326,   211,\n",
      "          145,   105,   852,  9178,   105,   433,   212,   236,   684,   105,\n",
      "         1778,   437,   110,  1588,   211,  2764,   105,   367,   211,   474,\n",
      "          210,   216,  9178,   110,   163,  7978,  1029,   529,   126,   113])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  508,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4603 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  8001,   105,  2578, 30517,   185,   802,  1569,   117,   105,\n",
      "          207,  7480,   210,   226,  4095,   210,   207,  1229,  3335,   238,\n",
      "          237,  1120,   229,  4254,   207,   627,  1091,   221,   145,   432,\n",
      "          210, 10956, 21692,  7621,   217,  2810,  7534,   238,   220,   548,\n",
      "          126,   113,   113,  1306,   128,   217,   207,  1975,   164,   195])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  509,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4773 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  4025,   658,   216,  2810,  7534,  3673,   189,   215,  1472,\n",
      "          221,   472,   332,   210,   145,  2810,  7534,  5262,  3689,   424,\n",
      "          111,   228,   221,   145,   446,  5736,   115,  8067,   115,   215,\n",
      "        15454,  4034,   217,  3492, 12141,   188,   112,   115,   215,   216,\n",
      "          223,   145,  1802,   210,   145,  2810,  7534,  5262,  3689,   424])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  345,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4439 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  426,  117,  145,  261, 1403, 2810, 7534,  238, 5769,  235,  237,\n",
      "        9178,  110,  163, 2053,  211, 3972,  211,  635,  338,  229,  247,  207,\n",
      "         887,  211, 6904, 2810, 7534,  110,  163, 1029,  529, 5508,  195,  117,\n",
      "        1004,  211,  805, 7480,  115,  207,  240,  988,  207,  488, 1064,  210,\n",
      "         226,  454])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  415,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4654 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   248,   426,   117,   146,   223,   786,   211,  3250,   142,\n",
      "          426,   117,   145,   218, 20029,   235,   216,  2810,  7534,   462,\n",
      "         2242,  1214,   333,   211,   852,  9178,   189,   117, 29829,   176,\n",
      "          180,   142,   426,   117,   146,   117,   207,  2242,  4110,   547,\n",
      "          244,   211,   219,   632,   213,   145,   610,   115,   229,   223])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  846,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5102 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3555,  1557,   426,   117,   145,   212,   426,   117,   146,\n",
      "          115,   229,  1769,   145,   353,   336,  1213,   211,   207,   548,\n",
      "         1601,   115,   207,   818,  5645,   252,   213,   142,   426,   117,\n",
      "          147, 21021,   189,   126,   113,   113,  1321,   128,  3947,   211,\n",
      "         2810,  7534,  1748,   229,   292,   765,   211,  2488,   142,   199])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  875,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5437 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6230,   195,   213,   207,  1412,   210,   142,   426,   117,\n",
      "          147,   117,   199,   115,   212,   207,   240,   356, 10899,  2199,\n",
      "          216,   239,   363,   223,   521,   383,  2879,  3519,   225,   207,\n",
      "          548,   213,   226,   256,   117,  4614,   211,   207,  1004,   115,\n",
      "          226,   454,  1763,   211,  4106,   207,   489,   964,   317,  9178])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  912,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5458 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1531,  115,  207,  240,  356,  307, 2283,  216,  207,  354,  265,\n",
      "        6919,  226, 4095,  210, 9178, 7320, 4655,  213,  477,  217,  231, 2843,\n",
      "         229,  115,  213,  207,  389,  110,  163,  631,  115,  261,  353, 2834,\n",
      "         568, 1016,  117,  321, 2810, 7534,  115, 1185,  150,  117,  200,  174,\n",
      "         236, 3963])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  774,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5419 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2069,   147,   117,   201,   210,   142,   426,   115,  5019,\n",
      "         2810,  7534,   238,  2876,   235,   237,  9178,   110,   163,  2053,\n",
      "          211,   105,  1115,   126,   128,  1830,   189,   207,   666,   210,\n",
      "        14480,   231,  1029,  1175,   238,   207,  1241,  3440,   118,  3499,\n",
      "          529,   215,   145,   370,   116,  2810,  7534, 21217,   116,  3361])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  910,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5620 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2810,  7534,  5262,  3689,   338,   221,   145,   540,   117,\n",
      "          447,   117,   142,   426,   117,   152,   117,   198,   117,   111,\n",
      "          146,   112,   117,   207,   240,  1723,   226,   454,   668,   185,\n",
      "          172, 12754,  7621,   452,   212,   489,   301,  9597, 25284,  7368,\n",
      "         9627,   117,   153,   117,  1903,   210,   145,   370,   116,  2810])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  984,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5282 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5545,   115,   212,  4493,   115,   207,   454,   386,   227,\n",
      "         1630,   225,   207,  3517,   240,   110,   163,  1108,   117,  1625,\n",
      "          115,   207,   454,   223,  8470,   452,   213,   216,   233, 10555,\n",
      "          237,  1887,   211,   548,   115,   229,   246,  5334, 12306,   174,\n",
      "          303,   218,   207,  3517,   240,   115,   314,  6913,   188, 27608])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1043,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4540 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2810,  7534,  5262,  3689,   338,   111,   248,   426,   117,\n",
      "          152,   112,   212,   211,  1451,  4320,  5262,  3689,   211,   219,\n",
      "         6136,   174,   213,   239,   669,   111,   248,   426,   117,   147,\n",
      "          112,   117,   105,   354,   265,  9867,   117,   236,  2938,   117,\n",
      "          207, 22325,   210,  1395, 13408,   818,   213,   226,  1158,  2410])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  909,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4822 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113, 5233,  128,  207,  354,  265, 7273,  189,  216,  142,\n",
      "         142,  426,  117,  147,  212,  126,  113,  113, 2206,  128,  426,  117,\n",
      "         152,  261, 5692, 1016,  317, 2810, 7534, 5262, 3689,  212,  370,  116,\n",
      "        2810, 7534, 5262, 3689,  115,  321,  115,  149,  117,  151,  117,  115,\n",
      "        5054,  189])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  673,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3822 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3517,   126,   113,   113,  2628,   128,   240,   435,   657,\n",
      "          216,  2810,  7534,   110,   163,   232,   225,  4799,   115,   105,\n",
      "          229,   223,   606,   237,  9178,   212,   145,  2060,  6653,   115,\n",
      "          105,   554,   447,   117,   236,  1299,   115,   217,  4799,   110,\n",
      "          163,  1382,   355,   210,   153,   175,   221,   207,   607, 28229])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  895,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5604 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   783,  2499,   644,   217,   530,   212,  1149,  5262,  3689,\n",
      "          229,  4310,   189,   225,  2810,  7534,  5262,  3689,   117,   248,\n",
      "          426,   117,   151,   117,   199,   656,  3243,   207,   548,  1408,\n",
      "          351,  2810,  7534,   217,  2902,   288,   952,   225,   153, 17270,\n",
      "          217, 11396, 19068,  4961,  2290,   213,   477,   217,   207,   153])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  925,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4973 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,  1887,   211,   462,   237,  5397, 11021, 10692,   174,\n",
      "        29029,   117,   213,   226,   374,   115,   207,   389,  2635,   216,\n",
      "          145,  1214,   210,  1619,   359,   221,   145, 11081,   217,   378,\n",
      "        17430,   333,   297,   219,   309,   211,   709,   218,   207,   354,\n",
      "          265,   115,   212,   481,   519,   213,   145,   485,   217,  1828])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1027,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5302 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5911,   213,   270,  1427,   115,   142,   142,   426,   117,\n",
      "          150,   212,   426,   117,   151,   227,   307,  3430,   207,  1395,\n",
      "        13408,   818,  1541,   218,   207,  3517,   240,   225,   374,   211,\n",
      "          223,   192,   189,   115,   153, 17270,   115,   212,   153,   178,\n",
      "          192,   189,   115,   310,   392,   318,  2170,   435,   115,   211])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1007,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5487 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   394,   210,   530,   212,  1149,  5262,  3689,  3202,   117,\n",
      "          228,  5055,   223,   489,   301,  1613,   116,  3212,   235,   212,\n",
      "        17285,   225,   207,   336,   266,   210,  1126,   213,  5859,   266,\n",
      "          117,   321,   354, 11565,   115,  5574,   165,   117,   163,   117,\n",
      "          236,  2364,   117, 15560,   210,   226,  1229,   213,   207,   389])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  899,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5195 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6791,   213,   207,   389,   110,   163,  1598,   211,   408,\n",
      "         2101,   115,   207,  5096,   210,  6184,   294,   211,   142,   426,\n",
      "          117,   148,   386,   227,  4010,   211,   219,  3615,   215,  6651,\n",
      "          117,  1019,  1886,   392,  4994,   115,   934,  5010,  4013,   211,\n",
      "          231,  1104,  2593,   213,  7691,  2810,  7534,   110,   163, 15982])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1100,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5354 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  211,  105, 1097,  500, 2237,  210, 4025,  658,  105,  215,  221,\n",
      "         207, 2053,  211,  247,  105, 7409,  808,  105,  211,  410,  117,  447,\n",
      "         117,  236,  774,  116,  910,  117, 4921,  435, 4819,  115,  207,  389,\n",
      "         115,  213,  239, 1598,  189,  211,  207,  408, 2101, 3704,  216,  142,\n",
      "         426,  117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1195,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5204 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  265, 5918,  117,  236, 1123,  117,  467,  207, 1613,  116, 3212,\n",
      "         235,  817,  210,  142,  426,  117,  149,  115,  212,  207,  488, 1213,\n",
      "         210, 7858,  189,  211,  207,  548, 1146,  213,  207,  256,  115,  207,\n",
      "         240, 7019,  126,  113,  113, 3111,  128,  216,  207,  644,  189,  213,\n",
      "         142,  426])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1048,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4971 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1214,   210,   228,   286,   213,   207,  1345,   210,   145,\n",
      "          480,  2662,   297,  2768,   237,  7001, 27576,  3582,  1249,   306,\n",
      "         2810,  7534,   117,   149,   117,  1959,   473,   808,   497,   211,\n",
      "          392,  1019,   318,   244,   207,  1840,   210,  2070,   212,  1499,\n",
      "          216,   324,  1843,   228,  6184,   244,   227,  3877,   808,   211])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1036,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5169 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6184,   280,   294,   211,   207, 29829,   176,   180,   117,\n",
      "         1004,   211,   805,  2101,   115,  1569,   213,   226,   454,  2612,\n",
      "          189,  2810,  7534,   211,  6513,   351,   411,  2070,   215,   918,\n",
      "          487,   222,   270,   645, 20695,   195,   215,   227,   116,   217,\n",
      "          116,  1714,  1304,   117,  2388,   115,  2069,   154,   117,   199])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  541,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4860 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   203,   117,   739,   212,  1177,   207,   627,   850,   189,\n",
      "          210,   207, 29829,   176,   180,  2653,   207,   920,   217,  1177,\n",
      "          210,   207,  1664,   117,   221,  1468,   126,   113,   113,  3841,\n",
      "          128,   348,   115,   152,   184,  5531,   126,   128,  2607,  4216,\n",
      "          189,   207,   240,   211,   105,   490,  1686,  1626,   105,   211])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1039,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4974 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1980,  225,  374,  211,  286, 1013,  238, 2810, 7534,  117,  447,\n",
      "         117,  142,  628,  117,  146,  117,  205,  117,  151,  117,  213,  482,\n",
      "         115,  207,  954,  627,  126,  113,  113, 3668,  128,  371, 3282,  189,\n",
      "         216,  207,  526,  212,  239, 4280,  212, 1005,  824,  288, 1980,  608,\n",
      "         225, 2810])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1133,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4956 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  389,  126,  113,  113, 3099,  128,  245,  115,  415,  213, 1141,\n",
      "        4950,  210,  207,  927,  526,  115,  126,  113, 5111,  128,  309,  211,\n",
      "        2542, 3166,  115, 3807,  212, 5311, 2810, 7534, 1959,  473,  115, 1921,\n",
      "         115, 1062,  115, 3617,  115, 8564,  171,  115,  212,  207, 1486,  213,\n",
      "        2887,  225])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1124,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5222 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   210,   207,  1664,  6368,   211,  3260,   207,  2061,  7424,\n",
      "          115, 13727,  7347,  3481,   216,   330,  2431,   145,   105,  1698,\n",
      "          210,   207, 15043,  2589,  5683,   105,   525,   115,   228,   216,\n",
      "          220,   286,   213,   207,  5850,   210,   207,   526,   356, 10899,\n",
      "          219,  6327,   218,   820,   213,  2211,  1177,   417,   373,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1291,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5087 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   240,   128,   222,   216,   115,   105, 18895,   235,   216,\n",
      "          253,   207,   240,   105,   249,   220,  2044,   236,   241,   126,\n",
      "          233,   128,   261,  5102,   126,   207,   276,   128,  1996,   212,\n",
      "          126,   335,   128,   261,  4034,   233,   117,   105,   447,   117,\n",
      "          221,   207,   954,   627,   371,   223,   530,   301,  1780,   252])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1144,  210, 1144,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 632 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1927,   218,   207,   276,   210,   237,   543,   211,   142,\n",
      "         1420,   229,  5638,   207,   276,   110,  2249,   216,   207,   240,\n",
      "         2406,   126,   113,  4761,   128,   585,   211,   500,   347,  4220,\n",
      "          171, 15621,   213,  2887,   225,   207,  1177,   210,   207,  1664,\n",
      "          117,   306,   806,   210,   228,   237,   543,   115,   253,   126])\n",
      "Original length: 1685 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  528,  126,  113, 1348,  128,  126,  321,  873,  210, 2804,  213,\n",
      "         856,  128, 2231,  528,  105,  330,  223,  145, 1249,  217,  241, 2762,\n",
      "         310, 1466,  117,  117,  117,  117,  105,  198,  126,  113, 1288,  128,\n",
      "        2500,  530,  301, 1282,  373,  207,  240,  244,  428,  466, 3696, 1249,\n",
      "        3280, 2041])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  282,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3002 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1125,   470,   294,   211,   606,   522,   212,   264,   266,\n",
      "          117,   392,   428,   649,   292,  1609,   115,   212,   298,   145,\n",
      "         7396,   729,   213,   207,  1609,   649,   115,   865,  3324,  6580,\n",
      "         3540,  5640,  1266,   216,  2810,  7534,   263,  2863,   142,   142,\n",
      "          198,   212,   199,   210,   207, 10104,   325,   115,   342,   165])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  279,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4575 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   210,   392,  2229,   115,   820,  1661,  1909,  1743,   238,\n",
      "         1704,   117,  7028, 10368,   115,  8011,   210,  2608,  6458,   236,\n",
      "         9684,  1386,   184,  3406,   115,  1784,   207,   240,  1519,   221,\n",
      "          237,  1909,   213,   207,  1416,   210,  2608,  6458,   212,  2060,\n",
      "         3996,   117,   820,   110,   307,   231,  1909,   246,  1704,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  337,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3052 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,  3517,   240,  4051,   239,   528,   218,  4245,   820,\n",
      "          110,   206,   126,   113,   113,   279,   128,   142,   199, 10104,\n",
      "          325,   470,   212,   115,  1280,   115,   331,   207,   483,   865,\n",
      "          263,  1541,   207,  1549,   327,   217,   504,   210,  3356,  2810,\n",
      "         7534,   110,   163,  5508,   195,   643,   117,   207,  3517,   240])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  368,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4256 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   289,  1029,   529,   261,   227,  2285,   222,   789,   126,\n",
      "          113,   113,   368,   128,  1029,   529,   621,   207,  6653,  2670,\n",
      "          189,   207,   105,   259, 12887,   235,   212, 10629,   105,   736,\n",
      "          210,  5314,   235,   212,  3241,   235,   115,  1330,   213,   207,\n",
      "         1104,   221,   105,  2307,   235,   115,   105,   207,   352,   211])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  342,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4309 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   210,   207, 28840,   110,   163,   818,   115,   105,   372,\n",
      "         3356,   207,   964,   317,   207,  1395, 13408,  2984,   212,   207,\n",
      "         9597, 25284,  7368,  9627,   363,   115,   207,   729,   240,   305,\n",
      "         4000,   222,   207,   105,   363,   210,   126,   207, 17430,   128,\n",
      "          818,   115,   227,   207,  1870,  5651,   233,   117,   105,   447])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  404,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4710 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   830,   235,   239,  1626,   211,  2810,  7534,   110,   163,\n",
      "          105,   608,   225,  1338,   153, 17270,   115,   105,   433,   229,\n",
      "          207,   483,   240,   105,  9116,   252,   105,   221, 17430,   115,\n",
      "          207,  3517,   240,  1541,   947,  2810,  7534,   872,  1280,  2022,\n",
      "          126,   113,   113,   516,   128,   306,   218,   207,   483,   240])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  416,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4272 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  297,  128,  110, 3641,  239,  126,  113, 1001,  128,  795,\n",
      "         211,  355,  153,  175,  117,  110,  105,  447,  117,  111, 3284, 1601,\n",
      "         210,  482,  115, 1244,  150,  117, 1772,  117,  199,  174,  236, 1057,\n",
      "         115,  160, 5995,  112,  111, 5199,  189, 1396,  112,  117,  126,  113,\n",
      "         113,  554])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  422,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4408 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2810,  7534,   297,  3395,   211,  1904, 11781,  3202, 22160,\n",
      "          225,  7978,   117,   105,   447,   117, 11781,  2952, 18323, 19821,\n",
      "          293,  2810,  7534,  2145,   211,   635,   237, 11781,  2381,   115,\n",
      "        11440,   115,   253, 11781,   110,   163,  1350,   225, 21808,  1795,\n",
      "          117,   447,   117,   207,  3517,   126,   113,   113,   415,   128])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  433,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5343 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   417,   297,   227,   219,  4013,   115,   227,   235,   216,\n",
      "          105,   145, 17173,   802,  1149,  1534,   223,   227,   115,   221,\n",
      "          145, 17173,   115,   220,   597,   145,  2238,   550,   117,   105,\n",
      "         2810,  7534,   115,  4752,   150,   117,   200,   174,   236,  2377,\n",
      "          117,  1296,   115,  1468,   207,  3517,   240,   115,   105,  1780])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  339,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5155 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2810,  7534,   110,   163,  1726,  4892,   860,   189,   216,\n",
      "          207,   483,   240,   110,   163, 25542,  1408,   246,  1528,  4337,\n",
      "          115,   207,  3517,   240,  1280,  1463,  2810,  7534,   110,   163,\n",
      "         1923,   216,   233,  6986,  1408,   210,   482,   115,  1244,   150,\n",
      "          117,  1772,   117,   199,   174,   236,  1036,   115,   160,  3999])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  441,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5013 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1395, 13408,   818,   481,   247,   255,  1710,   221,   207,\n",
      "          421,   217,   207,   483,   240,   110,   163,   105,   971,   210,\n",
      "          818,   105,   548,  1408,   115,   207,  3517,   240,   297,   247,\n",
      "          633,   228,  1601,   213,  4624,   207,   421,   217,   207,   483,\n",
      "          240,   110,   163,   105,   971,   210,   818,   105,   548,   126])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  478,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4801 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   305,   219,   105,   221,   599,   221,   798,   115,   227,\n",
      "          307,   213,   207,  3650,   210,   239,  1322,   115,   310,   213,\n",
      "          239, 11978,  1524,   115,   340,   216,   276,   245,  1996,   126,\n",
      "          128,   270,   770,   212,  5397, 11021, 10692,   174,  8418,   189,\n",
      "          245,   227,  2008,   117,   105,   213,   190,   110,   156,  3181])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  516,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4034 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,  2377,   128,  1322,   117,   321,   200,   244,\n",
      "          252,   171,   109, 27593,   184, 12932,   115,  5859,   266,   160,\n",
      "         8649,   172,   115,   236,   917,   116,   882,   111,   105,   353,\n",
      "         4663,  2125,  1322,   115,  1750,  1126,   228,   221,  8034,  1733,\n",
      "          211,  4224,   207,  5508,   195, 10972, 11559,  5550,   115,  2807])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  498,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4698 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3517,   240,   115,   228,  3197,  3606,  1222,   237,   105,\n",
      "         6241,   886,   222,  1126,   116,   211,  4608,   212, 19260,   303,\n",
      "          207,   537,   286,   117,   105,   447,   117,   207,   126,   113,\n",
      "          113,  1185,   128,   276,   212,   207,   240,  8867,   207,  7770,\n",
      "          736,   210,  4471,   228,   237,  6241,   886,   212,  8854,   252])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  442,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5210 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2810, 7534, 1908, 3648,  221,  521,  383, 2879, 3519,  207, 2819,\n",
      "         210, 5262, 3689,  238,  207,  327,  212,  820,  110,  126,  113,  113,\n",
      "         743,  128, 3880,  210,  548,  216, 2810, 7534,  110,  163, 9404,  210,\n",
      "         207, 5262, 3689, 3910,  481,  291,  211, 1975, 5508,  195, 1476,  213,\n",
      "         683,  210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  556,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5184 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5262,  3689,   115,   215,   236,   684,   221,   145,   105,\n",
      "         4204, 10998,   105,  3910,   229,   223,   578,   211,   207,  5262,\n",
      "         3689,  6616,  1560,   391,   207,   548,  2252,   117,   126,   113,\n",
      "         2220,   128,   213,   207,   298,   248,   115,   207,   240,  1811,\n",
      "          189,  2238,   301,   207,  1338,  5262,  3689,   212,  5262,  3689])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  554,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5135 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2104,  213,  207, 2525,  210, 3181,  212,  497,  338,  115, 1056,\n",
      "        2038,  105,  222,  428, 2790,  217, 5163,  860,  210, 3181,  338,  117,\n",
      "         105,  447,  117,  236, 7278,  117,  207,  588, 1904,  174,  207, 2790,\n",
      "        5767,  555, 2428,  117,  447,  117,  207,  333,  210,  207,  877, 2428,\n",
      "         329,  207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  508,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5380 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2093,  818,  244,  213,  230,  811,  211, 5455,  216,  335, 1518,\n",
      "         207, 1874,  210, 2419,  216,  270, 1149, 3667,  189, 2326,  281,  207,\n",
      "         266,  117,  105,  447,  117,  225,  392,  428, 5839,  115,  207, 1313,\n",
      "         240, 1863,  216,  152,  184, 2967,  126,  128,  145, 1249,  213,  237,\n",
      "        5859,  256])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  509,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5106 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1300,   210,   548,  3232,   218,   207,   756,   117,   210,\n",
      "         1409,   236,   673,   117,   207,   240,   386,   227,   753,   216,\n",
      "          226,  1207,   210,   266,   115,  1917,   233,   245,   219,   105,\n",
      "         5600,   115,   105,   356, 13508,   207,  4636,  1874,   126,   113,\n",
      "          113,  1368,   128,   300, 19297,   174,   306,   233,   218,   820])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  345,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5258 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2593,  244, 1024, 1749,  211,  247,  207,  444, 2238, 1002,  117,\n",
      "         321, 2810, 7534,  115, 1074,  150,  117, 1772,  117,  199,  174,  912,\n",
      "         120, 1601,  210,  482,  115, 1244,  150,  117, 1772,  117,  199,  174,\n",
      "         206,  117,  809,  115,  211,  207,  464,  216, 2810, 7534,  110,  163,\n",
      "        2381,  189])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  415,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4709 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2189, 1029,  529,  117,  105, 1601,  210,  482,  115, 1244,  150,\n",
      "         117, 1772,  117,  199,  174,  236,  345,  115,  160, 1368,  117,  207,\n",
      "         548, 1601,  210,  226,  256, 3093, 2161, 2810, 7534,  110,  163, 1350,\n",
      "         211, 4310,  225,  428,  599,  126,  113, 2623,  128, 5262, 3689,  338,\n",
      "         119,  111])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  846,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4823 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5767,   488,   218,   207,  1121,   216,   335,  2495,   222,\n",
      "         2810,  7534,   110,   163,  5508,   195,   424,   116,  7978,   115,\n",
      "          934, 20019,   235,   145,  1832,   210,  4025,   658,   555,  1583,\n",
      "         7319,   189,   117,   447,   117,   105,   370,   116,  2810,  7534,\n",
      "         5262,  3689,   105,   223,  6327,   213,  1106,   850,   189,   210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  875,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5247 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   105,  5262,  3689,   105,  1479,   115,  2248,   210,   239,\n",
      "        13402,   658,   215,  3117,   210,  4486,   117,   420,   115,   207,\n",
      "         2129,  3165,   317,   207,   428,   126,   113,   113,   962,   128,\n",
      "         1914,   223,  4703,   252,   213,   207,   548,   904,   115,   221,\n",
      "          705,   221,  2946,  2511, 11717,   211,   207,  2858,   210,  1650])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  912,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4489 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   473,   216,   126,   113,  3067,   128,  1539,   105,  2810,\n",
      "         7534,  5262,  3689,   115,   105,   215,   805,   578,   850,   210,\n",
      "         7978,   115,   696,   465,   820,  1204,   216,   228,   961, 22237,\n",
      "          223,  4495,   117,  1625,   115,  2810,  7534,   530,   189, 10259,\n",
      "          460,   216,   115,   314,   145,  1243,   218,   229,   211,  2600])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  774,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4286 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2608,  112,  218, 1417,  410, 2278, 7319,  189,  215, 1798, 5500,\n",
      "         189,  211,  228,  231, 2060,  115,  212,  481,  115,  253, 2307,  252,\n",
      "         211,  215,  280, 1044, 4442,  452,  225, 2969, 1029, 1175,  115, 1708,\n",
      "        2060,  338,  436,  217,  216, 5262, 3689,  211,  219, 2495,  222, 2969,\n",
      "        1029,  529])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  910,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4790 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5262,  3689,   424,   105, 28553,   189,   237, 23558,   916,\n",
      "          210,  4025,  3606,   115,   260,   210,   229,  2490,  1539,   145,\n",
      "         1182,   105,  2810,  7534,  5262,  3689,   424,   117,   105, 11011,\n",
      "          142,   478,   117,   168,   117,   153,   117,   207,  1673,   850,\n",
      "          210,   207,  1229,  1379,   216,   115,   213,   308,   211,   219])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  984,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3807 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   217,  1363,   115,   105,  2644,  2506,   115,   660,   212,\n",
      "         4014,  4574,  2060,   105,   223,  1541,   221,   145,  1331,   210,\n",
      "          105,  2810,  7534,  5262,  3689,   424,   117,   105,  1043,   126,\n",
      "          113,   113,  2556,   128, 11011,   142,   142,   201,   115,   478,\n",
      "          117,   168,   117,   153,   117,   261,  3045,   175,   115,   145])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1043,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4750 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   321,   323,  2904, 27950,   160,   186,  1237,   116,  1431,\n",
      "          120,  7582,  4149,   160,  4387,   117,   217,   226,   744,   115,\n",
      "         8149,   189,   967,  2608,  1175,   245,   219,  2118,   213,   333,\n",
      "          210,  1459,   105, 12003,   301,   105,   215,   105,  1097,   105,\n",
      "          335,  1044,  4442,  1990,   117, 12874, 13049,   160,   186,  1144])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  909,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4594 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1029,  1175,   117,   117,   117,  1196,   436,   211,  6418,\n",
      "          207,  7319,   189,   210,   145,  1410,  1029,   529,   111,  2388,\n",
      "          210,   207,  7319,   189,   210,   145,   382,   160,   173,  1029,\n",
      "          529,   112,   356,   219,  2922,   967,  2487, 13045,   216,   465,\n",
      "          227,  2149,   126,   113,   113,  2925,   128,  3177,   207,   376])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  673,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4882 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   105,  4683,   105,   223,   271,   218,   207,   850,   210,\n",
      "          207,  2060,   229,  2495,   189,   222,   145,  7858,   215,   207,\n",
      "          850,   210,   207,  2060,   229,   223, 12321,   252,   211,   207,\n",
      "         2487,   284,   116,   211,   186,  2665,   117,   321,   126,   113,\n",
      "          113,  3116,   128,  1741,   117,   236, 21104,   116,   337,   111])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  895,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3995 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   465,   340,   117, 21966,   160,   186,  9691,  4214,   117,\n",
      "         6012,  4254,   226,  1641,   221,   145,   358,   218,   229,   211,\n",
      "         2417,   808,   211,   207,  1959,   473,   217,   153,   175,   212,\n",
      "         1724,  1915,   145,  1802,   210,   153,   175,   217,   207,  6012,\n",
      "         1029,   529,   117,  1741,   117,   236,  4572,  2311,   116,  1244])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  925,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4658 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2810,  7534,   110,   163,   117,   761,  2647,   117,   241,\n",
      "        22130,   160,  1456,   117,  5756, 14331,   117,   761,   223,   145,\n",
      "          284,   210,  3584,   216,  2060,  6653,   189,   356,   355,   211,\n",
      "         6338,  2060,  1190,   189,   216,   261,  2495,   281,  2810,  7534,\n",
      "          110,   163,   117,   761,  1274,   117,   447,   117,   160,  1317])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1027,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4626 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   350,   105,  5262,  3689,   105,   215,   105,  4683,  3910,\n",
      "          105,  3202,  1541,   218,   820,   115,   307,   289,   210,   207,\n",
      "         1245,   116,  7858,   118,  1410,  7366,   116,   249,   255,  1582,\n",
      "          211,   530,   301,   247,   145,   480,  1814,   210,   105, 29145,\n",
      "         2759,   207, 12498,   105,   222,   616,  1146,   218,  2810,  7534])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1007,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4311 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,   805,  2119,   115, 11682,   175,  3332,  2060,   356,\n",
      "          219,  1486,  1383,   174,   211,   207,  5262,  3689,  4683,  6616,\n",
      "         3124,  1435,   213,   226,   256,   115,   228,   221, 26223,  1242,\n",
      "          115,   552,   207,  1479,  3704,   145,  7129,   105,   350,   105,\n",
      "         7270,   115, 11682,   175,  3332,  4025,   658,   212,  4375,  6373])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  899,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4762 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  4486,   210,   207,  1107, 24943,  2551,   115,   820,   110,\n",
      "         3880,   216,   226,  1963,  1745,   145,   105,  4683,  3910,   115,\n",
      "          105,   221,   216,  3795,   249,   255,   407,  2321,   226,   814,\n",
      "          115,   223, 10101,   252,   117,   207,  3880,   210,   207,   105,\n",
      "         4683,  3910,   105,  7806,   218,  5262,  3689, 24704,   189,   216])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1100,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4032 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   716,   301,   115,   820,  4293,   216,   207,  3178, 13402,\n",
      "          658,   213,  1107, 24943,  2516,  6527,   207,  1498,   211,  6904,\n",
      "         2810,  7534,   110,   163,  5508,   195,   213,   207,   327,   217,\n",
      "        11781,   116,  2267,   160,   173,  1029,  1175,   218,  4527,   145,\n",
      "         5090,   327,   212,  1706,   188,  2078,  1222,   217,  1687,   116])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1195,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4821 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1252,   213,   207, 13402,   658,   210,   207,   160,   173,\n",
      "          212,   207, 10190,  1252,   213,   207,  2355,  1222,   217,  2810,\n",
      "         7534,   110,   163,  7978,  1029,   529,   386,   227,  2575,   105,\n",
      "         5012,   195,  1505,   211,  1016,   105,   207,   327,   217,   160,\n",
      "          173,  1029,  1175,   117,   213,   190,   110,   156,  3181,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1048,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4203 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  8225,   115,   237,   440,  1249,   311,   219,  2423,  1613,\n",
      "          116,  3212,   235,   211,  2170,  1886,   207,   599,  5262,  3689,\n",
      "         6616,  1560,   391,   207,   548,  2252,   115,   228,   216,  2810,\n",
      "         7534,   356, 10899,  2641,  6299,   207,   376,   818,   225,   267,\n",
      "          211,   231,   810,   212,   578,  2177,   210,  5262,  3689,  4025])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1036,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5276 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 26223,  1242,   212, 21808,   950,   207, 19061,   235,   210,\n",
      "          239,   639,  1802,   189,   210,   392,  3202,   211,   207,  2819,\n",
      "          210, 26223,  1242,   212,  3347,   110,   163, 21808,   117,   321,\n",
      "         2810,  7534,   115,  4752,   150,   117,   200,   174,   236,  1123,\n",
      "          116,  1368,   117,   994,   238,   145,  3036,   927,   658,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  541,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5420 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   148,   117,   832,   105,  3078,   105,  1275,   218,  2810,\n",
      "         7534,  2321,   226,  2252,   210,   207,   814,   115,   820,   247,\n",
      "        10019,   174,   217,   207,   240,   110,   163,   568,  2481,  2940,\n",
      "         1601,   210,   482,   835,   218,   865,  5640,   391,   207,   548,\n",
      "         2252,   948,   872,   484,   218,  2810,  7534,   117,   392,  1601])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1039,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4344 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 10190,   363,   210,   241,   210,   226,   818,   306,  1044,\n",
      "         4442,  1051,   213,   207,  1158,   210,  7858,   118,  1410,  7366,\n",
      "          115,   207,   240,   261,   785,   392,  1275,   769, 14844,   117,\n",
      "          293,  4624,   207,   460,   213,   226,   126,   113,   113,  4076,\n",
      "          128,   374,   115,   207,   240,  3119,   216,   207,   832,   105])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1133,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3797 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  965, 1123,  210,  207, 3845,  126,  113, 3724,  128,  216, 1708,\n",
      "         189,  370,  116, 2810, 7534, 7858,  189,  211, 1086,  938,  212, 4465,\n",
      "        2766,  235, 2570,  225, 2810, 7534, 7858,  212, 2487, 1029, 1175,  117,\n",
      "        7582, 4149,  160, 3871,  120, 1739,  160, 1348,  117,  553,  117, 7582,\n",
      "        4149,  832])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1124,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4379 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2159,  211,  462, 1196,  216,  297, 1451, 4473,  184, 7923, 1247,\n",
      "        4660,  211,  808,  286, 2783,  174,  222, 2810, 7534,  110,  163, 8345,\n",
      "         182, 7858,  189,  215,  211, 1451, 4473,  184, 7923, 7858,  189,  211,\n",
      "        1044, 4442, 1990,  213,  145,  411,  812,  213,  145, 8345,  182, 7858,\n",
      "        1410,  117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1291,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5148 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1479, 2971,  115,  310,  216,  228, 2761,  189,  245,  519,  238,\n",
      "        2810, 7534,  110,  163, 1725,  210,  239,  639, 1381,  213, 1598,  211,\n",
      "         207, 3161,  210,  207,  327,  117, 1741,  117,  236, 6961, 4214,  111,\n",
      "        7361,  189,  112,  117,  199,  117, 2521,  105, 3078,  105, 1275,  491,\n",
      "         211, 1044])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1144,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5229 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 3517,  240,  117,  447,  117,  213,  382,  115,  207,  872,  802,\n",
      "         229,  820, 5455,  244,  227, 3555, 2810, 7534,  110,  163,  965,  210,\n",
      "         239,  639, 4080,  183, 2971,  225, 3347,  110,  163, 4080,  183,  117,\n",
      "        5490,  447,  117,  115,  225, 2810, 7534,  115, 4752,  150,  117,  200,\n",
      "         174,  236])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1185,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4255 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  5618,   126,   128,   347,   229,   223,  4971,\n",
      "          211,  2381,   189,   386,   227,   115,   218,  1092,   115,  2488,\n",
      "          207,  5859,   534,   117,  2810,  7534,   115,  4752,   150,   117,\n",
      "          200,   174,   236,  1306,   117,  1130,   105,   800,   249,   643,\n",
      "          211, 17972,   207, 20342,   189,   210,   145,  9116,   252,  1286])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1323,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5159 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  150,  117,  200,  174,  236, 2220,  117,  207, 3517,  240,  435,\n",
      "        2175,  216,  105, 3803,  228, 8098,  383,  115,  207, 5859,  588,  110,\n",
      "         163, 2093, 6563,  305,  219, 7111,  218,  110,  237, 2411,  351,  207,\n",
      "        2849,  210,  216,  818,  117,  110,  105,  447,  117,  111, 3284,  200,\n",
      "         244,  252])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1306,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4958 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   199,   117,   636,  1743,   213,  4624,   865,  5640,   110,\n",
      "          163,   650,   211,  1231,   237,  6241,   886,   367,   211,  2858,\n",
      "          210,   207,   253,   180,   115,   207,  3517,   240, 24656,   207,\n",
      "         2312,   210,  1436,  2238,  2596,   115,  7597,   235,   213,   382,\n",
      "          207,  6122, 17173,   189, 10262,   218,   820,   110,  1909,  6832])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1123,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5039 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   291,   115,   215,   207,   105,  6101,   115,   105,   211,\n",
      "          229,  2810,  7534,   110,   163,   818, 18328,   252,   215,   217,\n",
      "        15286,   207,   810,  1196,  8312,   211,  1003,   211,  2236,   351,\n",
      "          207,  3910,  7806,  2258,   218, 26223,  1242,   212, 21808,   117,\n",
      "         1741,   117,   236,  5196,  4214,   116,   743,   111, 20858,  3688])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  743,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4223 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   249,  4892,   252,   220,  4040,   216,   275,  1355,   126,\n",
      "          113,   113,  4232,   128,  8908,   189,   115,  8948,   189,   115,\n",
      "          215,   450,   116, 15887,   395,   207,  1601,   210,   207,  3517,\n",
      "          240,   115,   207,   240, 12259,   117,   321,   115,   149,   117,\n",
      "          151,   117,   115,  1741,   117,   236,  4542,  2758,   111,  9153])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1247,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5251 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   211, 25004,   175,  7978,   261,  1114,   207,   645,   212,\n",
      "          708,  3345,   217,   370,   116,  2810,  7534,  5262,  3689,   115,\n",
      "        15005,   213,  8784, 14092,   235,   207,   327,   238,  2810,  7534,\n",
      "          110,   163,  1395, 13408,   818,   117,   820,   115,   390,   115,\n",
      "          247,  1096,   211,  1204,   216,   228,  4222,   261,  1512,   568])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1342,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5351 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   111,  3508,   235,  1229,   210,   105,   370,   116,  2810,\n",
      "         7534,  5262,  3689,   105,   112,   117,  4085,   115,   207,   240,\n",
      "          261,  5645,  2810,  7534,   238,  2696,   207,  4475,   210,   220,\n",
      "          228, 18429,   117,   390,   115,   207,   240,   261,   227,  1403,\n",
      "         2810,  7534,   238,  3762,   370,   116,  3529,  1779,   222,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1433,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4577 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2810, 7534,  110,  163, 2467,  252,  630,  117, 5490, 2810, 7534,\n",
      "         115, 4752,  150,  117,  200,  174,  236, 1449,  111,  105,  532, 1231,\n",
      "         216,  225,  207, 1887,  210,  207,  289, 2102, 5076, 2527, 9131, 1623,\n",
      "        5500,  189,  115,  241,  210,  207, 9178, 1214, 1265,  236,  550, 2277,\n",
      "        3214,  210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1449,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4069 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   210,  2810,  7534,   110,   163,   424,  1650,   115,   253,\n",
      "          220,   115,   297,   725, 25542,  1886,   207,   723,   210, 28229,\n",
      "          577,   473,   212,  1029,   529,   473,   117,   321,  1813,   145,\n",
      "          115,   332,   168,   117,   146,   117,   820,  1968,   211, 14147,\n",
      "          207,   631,   210, 25542,   211,  8117,   220,  3591,   213,   229])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1237,  210, 1237,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 2743 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,  3976,   128,   207,   240,   323,  2752,   212,\n",
      "          479,   189,  4663,  1743,   216, 23175,   189,   207,   812,   213,\n",
      "          229,   207,  4823,  1916,   210,  2060,   473,   238,   207,  7978,\n",
      "         1029,   529,   261, 11321,   207,  1104,   115,   126,   113,   113,\n",
      "         4692,   128,  2984,   235,   606,   223,   192,   189,   212,  2765])\n",
      "Original length: 4725 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,  4679,   128,   126,   113,  3976,   128,\n",
      "         1569,   213,   207,  7531,  2189,   207, 25542,   548,  1408,  1379,\n",
      "         1916,   210,  2060,   473,   211,  1249,   207,   683,   117,   211,\n",
      "          207,  1004,   115,   207,   460,  1564,   211,   207,   240,  3120,\n",
      "          216,   207,  2053,   211,  3068,   711,   116,  1830,   808,   211])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  202,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3808 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 27856,   189,   115,   105,  1237,   126,   113,   113,  5016,\n",
      "          128,   207,  3517,   240,  2027,   189,   252,   207,   126,   113,\n",
      "         3099,   128,   105,   584,   599,   872,  2810,  7534,  1789,   211,\n",
      "        11256,   153,   175,   211,  7978,   105,   212,  4048,  1146,   548,\n",
      "          217,   307,   428,   228,   872,   117,   126,   113,   113,  3663])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  203,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4952 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   808,   105,   229,  1676,  1977, 22325,   210,   145,   382,\n",
      "         4025,   658,   221,  3672,   301, 10539,   252,   211,  1362,   207,\n",
      "          620,   210,   226,   256,   117,   321,  2810,  7534,   115,  4752,\n",
      "          150,   117,   200,   174,   236,  2518,   117,   353,   489,   301,\n",
      "        10539,   252,   223,   145,  1249,   229,   783,  9178,   189,   212])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  204,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4651 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   363,  1351,   218,   207, 13293,   117,   321,  2810,  7534,\n",
      "          115,  4752,   150,   117,   200,   174,   236,  1237,   111,   105,\n",
      "          552,   207, 13293, 10921, 16883,   189,   110,  5424,   617,   212,\n",
      "         1839,   189,  2810,  7534,   110,   163,  5508,   195,   115,   233,\n",
      "          211,   185,   223,  1395, 13408,   117,   105,   112,   115,  2810])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  205,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4444 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 14222,   189,   210,  1084,   212,   408,   663,   960,   216,\n",
      "          207, 27610,   224,  1769,   207,   803,   210,   207,  5238,   229,\n",
      "          126,   113,   113,  5073,   128,   275,   639,  4885,   249,  1920,\n",
      "          117,   105,   112,   117,  1265,   222,  2810,  7534,   110,   163,\n",
      "         1756,   211,  9630,   175,   215,  6904,  6127,   311,   219,  5334])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  206,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3939 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1299,  126,  113,  113, 4572,  128,  447,  117,  207,  276,  110,\n",
      "         841, 3280,  217, 2242, 2361,  217, 7978, 1029,  529,  338,  244, 8470,\n",
      "        9038,  578,  115,  225,  260, 2424, 2242,  658,  213,  207,  333,  212,\n",
      "         364,  210,  207, 2361,  217,  207, 2253, 9178,  189,  225,  207, 3479,\n",
      "        1198,  210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  258,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4368 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   256,   211,  8784, 14092,   207,   327,   238,  2810,  7534,\n",
      "          110,   163,  1975,  5508,   195,  1476,   117,   447,   117,   390,\n",
      "          115,   207,   807,   222,  2810,  7534,   110,   163,  2053,   211,\n",
      "         1168,   237,  9178,   110,   163,  7978,  1214,   311,   227,   219,\n",
      "          340,  2499,   221,   211, 16156,  2810,  7534,   110,   163,  2053])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  282,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5548 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2581,   210,   207,  5859,   534,   115,   390,  5093,   211,\n",
      "         2581,   765,   218,   207,   240,   115,   110,   105,  2810,  7534,\n",
      "          115,  1185,   150,   117,   200,   174,   236,  3963,  2311,   111,\n",
      "         3284, 16773,   178,  2582,   115,  5886,   165,   117,   163,   117,\n",
      "          236,  3725,   116,   875,   112,   115,   696,   223,   207,   240])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  279,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5245 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  212, 2039,  213, 1213,  211,  207, 2143,  590,  210,  207,  223,\n",
      "         192,  211,  635, 2810, 7534,  110,  163, 2060,  215,  211, 2258, 2585,\n",
      "        2060,  217, 2810, 7534,  117,  200,  117, 1382,  608,  213,  428,  870,\n",
      "         850,  189,  126,  113,  113, 5366,  128,  210,  239,  528,  115,  207,\n",
      "        3517,  240])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  337,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4967 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1619,   359,   212,  1724, 20486,  2819,   210,   207,   232,\n",
      "          238,   207, 27380,  5564,   210,   207,  3430,  3582,   850,   210,\n",
      "          207,  1664,   117,   321,   447,   117,  1625,   268,  5038,   228,\n",
      "          237,  2761,   115,   207,   240,   261,  1232,  3338,   210,   237,\n",
      "         7012,   454,   213,   228,   608,   307,   213,   488,   620,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  368,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3823 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  411, 2810, 7534, 7319,  189,  115,  927,  286,  115,  212, 1798,\n",
      "        3845,  217,  207,  504,  210, 5038,  235, 1044, 4442,  860,  117,  198,\n",
      "         117, 7319, 6184,  111, 1044, 4442,  860,  317, 7978,  212, 2810, 7534,\n",
      "        5262, 3689,  112,  207, 1249, 1146,  218,  207,  240,  225,  374,  211,\n",
      "         207, 1019])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  342,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4630 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113,  113, 5552,  128,  213,  241, 3986,  115,  207, 1121,\n",
      "         216, 2810, 7534, 2097,  212, 1214,  207, 1798, 3845, 6327,  218,  239,\n",
      "         160,  173, 1029, 1175,  211, 3493,  225, 2810, 7534,  110,  163, 7858,\n",
      "         189,  223,  207, 1024, 1613,  116, 3212,  235,  454,  213,  207,  240,\n",
      "         110,  163])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  404,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5234 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  818,  229, 2629, 1688,  211,  548,  213,  226,  256,  212,  820,\n",
      "         110, 2148,  210, 2810, 7534,  110,  163,  350,  105, 3078,  105, 1275,\n",
      "         444,  211, 1798, 3845,  212,  231, 1104, 1327,  117,  820,  110, 2664,\n",
      "         306, 2521,  105, 3078,  105, 1275,  223, 2948, 5599,  235,  117,  820,\n",
      "        5343,  211])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  416,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5272 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   569,   116,  2499,  1019,   115,   228,   221,   216,   954,\n",
      "          218,   820,   115,   311,   323,   219,  6423,   552,   233,   261,\n",
      "         1749,  1708,  4551,  1081,   235,   215, 21729,   235,   210,  7978,\n",
      "          314,   126,   113,  4188,   128,  7469,   235,  2810,  7534,   110,\n",
      "          163,  1619,   359,   277,   117,   321,   447,   117,   218, 21729])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  422,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4917 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  888, 5500,  189,  117, 2810, 7534,  110,  163,  160,  173, 1029,\n",
      "        1175,  115, 1486, 1024, 2060,  115,  247, 3865,  105,  888,  105, 5500,\n",
      "         189,  115,  507, 5500,  189,  216,  244,  227, 5731,  212, 1878,  217,\n",
      "         355,  218, 1196,  117,  321, 1813,  145,  115,  332,  953,  117,  330,\n",
      "        2086,  145])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  433,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4811 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   316,   115,   934,  9601, 10325,   644,  7019,  2810,  7534,\n",
      "          110,   163,  2467,   189,   212,   239,  1230,  3095,   211,  1839,\n",
      "         1092,   238,  2467,   683,   117,   447,   117,   644,   351,   392,\n",
      "         6616,   223,   489,   213,   207,  1158,   210,   145,  1249,   212,\n",
      "          311,   219,  2025,   252,   288,   220,  1664,   117,   390,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  339,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4489 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   820, 21873,   207,   629,  1510,   213,   145,  2120,   229,\n",
      "         5315,   145,   394,   210,  1106,   126,   113,  2760,   128,  2570,\n",
      "          117,   820,   110,  1290,   783,  7424,   216,   207,   629,  1510,\n",
      "          223,  4546,   211,   105,  3583,  2810,  7534,   110,   163,   739,\n",
      "          225,   126,   207,  5865,  1664,   128,   283,   991,   241,  1275])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  441,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5098 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6042,   210,   207,  1823,   210,   228,  1840,   117,   820,\n",
      "          247,   115,   213,   363,   115,  1670,   207,   629,  1510,   225,\n",
      "          207,   770,   210, 10398,   115,  1580,   115,   212,   865,   117,\n",
      "          207,   240,   249,  3612,  1270,   216,   228,   145,  1286,   297,\n",
      "         2587,   211,   219,   630,   452,   213,  1178,   117,   213,   775])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  478,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5017 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   454,   229,  2182,  1916,   307,   218,   207,  1509,   448,\n",
      "          714,   210,  2810,  7534,   225,   207, 13982,   210,   207,   526,\n",
      "          216,  1759,   207,   714,   117,   447,   117,   142,   416,   117,\n",
      "          148,   117,   207,   739,   714,   989,   211,   207,  1509,   448,\n",
      "          714,   212,   211,   207,   526,   229,  1759,   700,   215,   502])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  516,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4958 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   218,   207,   259,   145,   240,   356,  2351,   548,   115,\n",
      "         3573,   115,   338,   115,   212,   207, 11990,   244,  1749,   211,\n",
      "          247,  2818, 15072,  1797,   117,   226,   115,   213,  1736,   115,\n",
      "         6904,   189, 20150,  2946,  2911,   217,   986,  2793,   207,   489,\n",
      "         1008,   210,  1322,   213,  2125,  1177,   872,   115,   606,   213])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  498,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3994 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 6563,  212, 1009,  189,  145, 3880,  210, 1016, 2632,  252,  391,\n",
      "         207,  548, 2252,  210,  226,  256,  117, 2144,  115,  865, 5640, 1266,\n",
      "         115,  212,  820,  465,  227, 1059,  115,  216,  207, 1415,  301,  810,\n",
      "        1196, 8312,  211, 1003, 5769,  252,  207, 2053,  210, 4799,  212, 4473,\n",
      "         184, 7923])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  442,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4900 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   820,   247,   227,   632,   216,   145,  3491,   222,   105,\n",
      "         7472,  2295,   105,   261,  5038,  1016,   213,   207, 24369,   174,\n",
      "          327,   215,   967,  2060,   225,   207,   887,   211,  6904,  2810,\n",
      "         7534,   110,   163,  5508,   195,   117,   321,  1813,   145,   115,\n",
      "          332,   168,   117,   147,   117,   210,  1706,   189,   190,  2161])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  556,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4198 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  820, 2823,  216,  207, 1249, 1146,  218,  207,  240,  305, 6679,\n",
      "        2810, 7534,  110,  163,  818,  225,  374,  211, 2593,  126,  113, 4862,\n",
      "         128,  213,  226, 4179,  126,  113,  113, 4822,  128,  212,  207, 1333,\n",
      "        4375,  678,  218,  207,  354,  265,  351, 2810, 7534,  117, 1130,  211,\n",
      "        7035,  174])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  554,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5378 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   820,   374,   207,  3347,   116,  8213,   190, 21808,  4683,\n",
      "          221,   145,  1106,  2010,  3910,   211,  2810,  7534,   110,   163,\n",
      "         1029,   529, 15982,  1303,   212,   631,   207,  2649,   708,   210,\n",
      "          207,  3347, 14921, 13786,   190, 21808,  4683,   221,   145,   358,\n",
      "          218,   229,   211, 19061,   207,  4683,   110,   163,  2053,   211])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  508,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5090 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 6535,  211,  207,  231, 1106, 4095,  210,  226,  454,  115,  207,\n",
      "        1121,  216, 2810, 7534, 1069,  635,  217, 1104, 1327,  343,  233,  249,\n",
      "         280, 2524,  126,  113,  113, 6099,  128, 3113,  211,  207, 1327,  115,\n",
      "         207,  240, 2635,  216,  226, 4095,  210,  207, 1290,  223, 5093,  211,\n",
      "         220, 1408])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  509,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5022 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1681,  261,  126,  113, 4705,  128, 2799,  266, 1177,  451,  213,\n",
      "        1718, 2810, 7534,  110,  163,  721,  704,  217, 2581,  210,  207, 5859,\n",
      "         534,  117,  820,  115,  390,  115,  247,  227, 1564,  460,  216,  220,\n",
      "         266, 1177,  451,  297,  219, 2634,  213, 1843,  228,  286,  117,  321,\n",
      "        1813,  145])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  345,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5048 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3036, 29924,   210,   207,  3543,   818,   765,   211,   219,\n",
      "         1395, 13408,   212,   223,  1613,   116,  3212,   235,   213,   207,\n",
      "         4994,   210,   207,  1322,   271,   117,  1296,   115,   207,  1249,\n",
      "         1146,   218,   207,   240,   223,  6657,   252,   211,   126,   113,\n",
      "          113,  6347,   128,  5038,  1016,   213,   207, 24369,   174,   327])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  415,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3421 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   391,   226,  1641,   115,  3117,   189,   247,   255,   280,\n",
      "          222,   767,   210,  2810,  7534,   216,   207,   243,   261,   581,\n",
      "          126,   113,   113,  5091,   128,   239, 16822,   195,  1748,   229,\n",
      "          247,   255,   332,   210,   239,  2010,  2773,   213,   308,   211,\n",
      "          902,   225,   207,  5865,  1664,   117,   207,   240,   261,  1231])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  846,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5122 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1214,   239,  7978,  1029,   529,   227,   307,   211,  9178,\n",
      "          189,   115,   310,   323,   211,   393,   276,   115,   228,   221,\n",
      "         2644,   918,   115,   126,   113,   113,  5900,   128,  2060,  6653,\n",
      "          189,   115,   212,  1285,  2634,   213,  3321, 25004,   252,  1802,\n",
      "          189,   210,   207,  7978,  1029,   529,   211, 10509,   211,  9178])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  875,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4613 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3359,   115,   207,   460,   223,  2661,   211,   635,   145,\n",
      "         1113,   216,   393,   116,   257,  4222,   115,   221,  1564,   213,\n",
      "          126,   113,  5125,   128,   820,   110,   954,  1249,   115,   261,\n",
      "          568,  1016,   117,   924,   126,   113,   113,  6348,   128,   199,\n",
      "          117, 18429,  1650,   212,  2290,   207,  3517,   240,   545,   216])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  912,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4144 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   447,   117,   820,   465,   227,  1115,   220,   460,   216,\n",
      "          207,  2053,   210,  9178,   189,   211,  7667, 18429,   189,   215,\n",
      "         1739, 16555,   189,   229,   105,  3387,   207,  4025,   658,   210,\n",
      "          207,  1830,  5500,   115,   105, 29829,   176,   180,   142,   426,\n",
      "          117,   147,   117,   199,   115,   261,   247,   220,  2540,   363])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  774,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5469 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2810,  7534, 26826,   189,   236,   728,   207,  2974, 14480,\n",
      "          210,  2060,   229,  2495,   189,   281,   207,  7978,  1830,  5500,\n",
      "          212,  3704,  1743,   115,   221,   233,   503,   391,   207,   548,\n",
      "         2252,   115,   216,   228,  2974, 14480,   223,  4971,   211,  2765,\n",
      "          117,  4219,   160,  1039,   117,   555,   207,  1743,   210,   553])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  910,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4370 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   548,  2252,   211,  2636,   226,  1393,   807,   117,   207,\n",
      "         3517,   240,  1653,   226,   376,   105,  2078,  4819,  1151,   105,\n",
      "          391,   207,   548,  2252,   115,   487,   306,   207,  5286,   216,\n",
      "          105,   211,   207,   464,   207,  9178,   189,   110,  3113,   126,\n",
      "          391,   207, 21217,  6833,   128,   602,  2078,  4819,   115,   210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  984,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4610 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1489,   115,   207,  3338,   210,   105,  1977, 22325,   105,\n",
      "          213,   820,   110,  1229,   210,   105,   711,   116,  1830,   808,\n",
      "          105,   297,   519,   213,   207,   251,   210,   818,   115,   218,\n",
      "          142,   199,   117,   147,   117,   628,   210,   820,   110,   954,\n",
      "         1249,   115,   217,   229,   207,  3517,   240,  6069,   211,   221])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1043,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4425 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   221,   145,   830,   519,   210,   207,  2499,  1229,   210,\n",
      "          105,  5262,  3689,   105,   679,   213,   820,   110,  1249,  1290,\n",
      "          115,   456,  1970,   210,   207,  1424,  2570,   210,  7978,   115,\n",
      "          994,  2577,   238,  1280,  1541,  4025,   658,   228,   221,  5004,\n",
      "          116, 28229,   235,   115,   960,   723,   221,   145,   105,   607])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  909,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5472 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2768,   548,   222,   226,   564,   115,  2810,  7534,   115,\n",
      "         4752,   150,   117,   200,   174,   236,  1237,   115,   216,   207,\n",
      "        13293,   210,   207,  1830,   110,   163,   607, 11909,   218,   145,\n",
      "         2810,  7534,   424, 10921,   207,  5424,   617,   210,   207,   370,\n",
      "          116,  2810,  7534,  2060,   212,  1724, 10921,  3986,   216,   228])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  673,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5403 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2048,   211,  2397,   145,   480,   927,  1121,   115,   105,\n",
      "        29829,   176,   180,   142,   426,   117,   152,   117,   199,   117,\n",
      "        10368,   160,  3111,   120,  9265,   160,  3472,   117,   606,  2229,\n",
      "         4530,   216,  2810,  7534,   245,  7038,   226,  1412,   211,  2768,\n",
      "          145,  1121,   216,   207,  1963,  2060,   635,   145,  6046,   210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  895,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4349 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   216,   207,   368,   116,   414,   259,   287,   223,   885,\n",
      "          212,   216,   728, 22585,  1303,   711,  1830,   189,   261,  3652,\n",
      "          216,   335,   244,   227,  2528,   211,  1451,   207,  1190,   211,\n",
      "          105,  4256,   116,   459,   105,   207, 19068,  4961,   218,  3782,\n",
      "        18429,   189,   288,   145, 20512,   115,   238,   229,   207, 18429])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  925,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4702 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   345,   115,   310,  2294,   216, 11798,   503,   227,   462,\n",
      "         2810,  7534,   225,   207,  3440,   233,   787,   217,   569,   145,\n",
      "          923,   212,   728,   207,   184,   115,   145,  3425,   437,   293,\n",
      "         2810,  7534,   263,   787,   876,   210,   207,  1211,   232,   115,\n",
      "         1741,   117,   236, 14416,   116,   279,   111, 27705,   112,   120])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1027,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4495 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2632,   189,   117, 27705,   160,   186,  2720,   116,   279,\n",
      "          117,   226, 14889,   210,   157,   174,   186,   189,   223,   487,\n",
      "         3313,   306,   207,   631,   216,  2810,  7534,   356,   355,   207,\n",
      "          333,   212,  1177,   210,   207,   157,   174,   186,   189,   211,\n",
      "         9630,   175,   351,  9178,   189,   212,  8617,   231,  1764,   210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1007,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4345 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   145,   117,   972,  2060,  7149,   111,   105,   223,   192,\n",
      "          189,   105,   112,   212,   972,  2710,  3689,  7149,   111,   105,\n",
      "          153,   178,   192,   189,   105,   112,   207,  2238,   212,   548,\n",
      "         1601,   213,   226,   256, 21193,   145,  1178,   218,  2810,  7534,\n",
      "          210,  2145,   212,  1035,  6127,   351,  2060,   212,  2710,  3689])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  899,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3514 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1213,   211,   207,  2672,   210,   338,   229,  4310,   225,\n",
      "         2810,  7534,   110,   163,  5508,   195,   424,   115,   213,  1529,\n",
      "         5530,   221,   820,   110,  1249,   142,   205,  1585,   211,   223,\n",
      "          192,   189,   212,   153,   178,   192,   189,   115,   613,   142,\n",
      "          205,  7168,   211, 17826,   142,   426,   117,   150,   117,   198])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1100,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4390 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   363,   210,   207,  1756,   271,   211,  9178,   189,   213,\n",
      "        16778, 18440,  2759,  7978,   117,  1015,   115,   606,   954,  1126,\n",
      "          698,   318,   229,  2061,  6127,   218,  2810,  7534,   351,  9178,\n",
      "          189,   217,  2993,   207,   231,  1604,   271,   213,   207,   627,\n",
      "          371,   115,   221,   705,   221,   115,   213,   336,   333,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1195,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4443 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   553,   117, 10060,   189,  7187,   110,   163,  7480,   213,\n",
      "          226,   374,  2048,   211,   366,   217,   207,   482,   216,   207,\n",
      "         3517,   240, 10052,   301,  6069,   211,  2768,   548,   306,  2810,\n",
      "         7534,   217,   105,  1417,   145,  1390,   237,  8643,  3017,   115,\n",
      "          105,  2810,  7534,   115,  4752,   150,   117,   200,   174,   236])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1048,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3751 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   482,   115,  4132,   238,  1038,  8431,   212,   462,   237,\n",
      "          105,  1623,  4683,   105,   211,   216,   210,  1038,  8431,   117,\n",
      "         2367, 21966,   160,   279,   111,  1314,   212,   940,   112,   117,\n",
      "         1015,   115,   207,   240,  1723,   553,   117, 21966,   110,   163,\n",
      "         4347,  7480,  5599,   235,   117,   213,  1152,   115,   207,   240])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1036,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3871 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   226,   454,   213,   820,   110,   954,  1249,   223,   216,\n",
      "          233,   297,  3430,  2810,  7534,   238,  5086,   235,   145,  7978,\n",
      "         1214,   115,   728,   343,   207,  1719,   223, 10052,   301,   213,\n",
      "          397,   681,   115,   340,   915,   221,   207,  1719,  2764,   189,\n",
      "          207,   681,   281,   207,   612,   287,   117,   447,   117,   120])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  541,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4898 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5262,  3689,   424,  2010,   225,   220,  7978,  1029,   529,\n",
      "          424,   215,  2810,  7534,  5262,  3689,   424,   117,   105, 11011,\n",
      "          142,   282,   117,  2628,   207,  1436,  3186,   211,   226,   454,\n",
      "          223,   142,   426,   117,   150,   117,   199,   213,  2810,  7534,\n",
      "          110,   163,   954,  1249,   117,  1486,   142,   282,   213,   820])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1039,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4880 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   488,   213,   239,  2053,   211,   824,   288,   105,   630,\n",
      "          116,   217,   116,  4592,   105,   608,   115,   229,   297,   219,\n",
      "          702,   294,   211,   142,   426,   117,   150,   117,   199,   210,\n",
      "         2810,  7534,   110,   163,   954,  1249,   117,   553,   117,  9265,\n",
      "          115,   222,   767,   210,   820,   115,  1661,   207,   376, 12042])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1133,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5067 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1946,   210,  5949,   212,  1265,   222,   207,   355,   210,\n",
      "         2905,  5360, 18953,  8586,  4333,   473,   117,   105,  2720,  9265,\n",
      "          160,  4901,   117,  2948,   115,  7656,   182,  8857, 23671,   115,\n",
      "          207,  1509,   448,   714,   210, 26188,  3202,   115,   145,   243,\n",
      "          126,   113,  4974,   128,   229,  2658, 11682,   175,  3332,  2060])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1124,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5222 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 10060,   189,  7187,   110,   163,  7480,   210,   142,   426,\n",
      "          117,   150,   117,   199,   115,   207,   240,  2635,   216,   553,\n",
      "          117, 10060,   189,  7187,  1096,   211,  3412,   207,   568,   211,\n",
      "          219,  2756,   218,  2696,  2810,  7534,   110,   163,  2053,   211,\n",
      "          824,   288,  2332,   857,  2168,   608,   117,   221,   207,   240])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1291,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4004 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6919,   216,   142,   203,   503,   227, 17146,   225,   275,\n",
      "         1909,   631,   210,   207,  1075,   229,   305,  4990,   207,  6657,\n",
      "          235,   210,   145,  1249,   126,   113,   113,  6901,   128,   213,\n",
      "          226,   256,   117,   447,   117,  4741,   211,   185,   115,   142,\n",
      "          203,   117,   149,   210,   820,   110,  1249,  1290,   386,   227])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1144,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5419 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1576,  210,  220,  850,  189,  210,  142,  201,  210,  820,  110,\n",
      "         954, 1249,  126,  113, 5273,  128,  229, 2480,  207,  240,  110,  163,\n",
      "        1366,  904,  221,  211,  207, 1064,  210,  207, 1249,  117,  221,  207,\n",
      "         240,  126,  113,  113, 6540,  128, 2175, 1435,  115,  207, 1229,  213,\n",
      "         820,  110])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1185,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4742 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  7285,   395,   870,   238,   324,   210,   789,   424,   117,\n",
      "          447,   117,   160,   186,   951,   116,  1001,   117,  2242,   658,\n",
      "          213,  3970,   967,   207,   338,   210,  1338,  3573,   212,  1141,\n",
      "        11936,  1051,  7643,   392,  2749,   210,  1016,   117,   447,   117,\n",
      "          253,  2534,   115,   207,  2327,   210,   142,   201,   297,   219])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1323,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5224 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2381,   189,   218, 20140,   235,   603,   210,   207,   828,\n",
      "          301,  1874,   210,  3738,  3996,   212,   350,  3760,   115,   994,\n",
      "          353,   268,   249,   255,  1582,   211,  1009,  1016,   117,   820,\n",
      "          110,   636,  1909,   115,  1704,   117, 20858,  3688,   115,  2294,\n",
      "          216,  2810,  7534,   249,   145,  2332,   302,   213,  4648,   351])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1306,  210, 1306,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 392 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1831, 1583,  115,  233,  223, 2835,  211,  581,  207,  408, 5500,\n",
      "         407,  211,  808, 1029,  529,  410,  117,  226,  223,  552, 1034,  188,\n",
      "         189,  443,  247,  126,  113,  113, 7528,  128,  436,  473,  216, 4906,\n",
      "         222,  216,  408, 5500, 4908,  233,  227,  211,  581,  117, 1106, 1378,\n",
      "         211,  207])\n",
      "Original length: 945 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,  7528,   128,   126,   113,  5033,   128,\n",
      "        11667,   160,   186,   882,   116,   951,   120,   321,   323,  4219,\n",
      "          160,  2628,   117,   207,  2608,  6458,  2604,   216,  2522,   213,\n",
      "          226,   256,  4725,   301,   753,   216,   207,  2499,  1019,   210,\n",
      "         4902,   668,  8059,   115,   215,   105,   888,   115,   105,  5500])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 199, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4866 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2810,  7534,   126,   113,   113,  7099,   128,   223,   227,\n",
      "         6271,   213,   216,   233,  1069,   189,   888,  5500,   189,   115,\n",
      "         1741,   117,   236, 21341,   116,  1185,   111,  2904, 27950,   112,\n",
      "          115,   696,   223,   207,   394,   210,   228, 10274,  5500,   189,\n",
      "         6271,   117, 12874, 13049,   160,  4153,   117,   248,   201,   210])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 200, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5070 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 20858,  3688,   110,   163,  5101,   211,  8264,   207,  3543,\n",
      "          812,   213,   229,   207,   968,   297,  2075,   117,   213,   236,\n",
      "          684,   289,   374,   115,   207,  1019,   210,  7319,   189,   115,\n",
      "          217,  1363,   115,  1704,   117, 20858,  3688,   110,   163,  2016,\n",
      "          216,  6184,   280,   294,   211,   207,   105,  6561,   968,   105])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5364 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2642,  2810,  7534,   110,   163,  3338,   210,   142,   142,\n",
      "          426,   117,   148,   212,   426,   117,   149,   213,   239,  1249,\n",
      "         1290,   115,  2810,  7534,  1564,   207,  1743,   210,  1704,   117,\n",
      "         2356, 15062,  4313,   216,   207,  3338,   210,   207,   318,  2001,\n",
      "         5638,   207,   105,   851,   116,   212,   116,   500,   105,   210])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4165 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  788,  115,  207,  240, 1723,  207, 7480,  210,  207, 1345,  210,\n",
      "         145, 1229,  210,  207,  425,  105, 1044, 4442, 1990,  105,  213, 2810,\n",
      "        7534,  110,  163, 1249, 1290,  211,  219,  314, 4991,  117, 2623,  126,\n",
      "         113,  113, 5950,  128,  126,  113, 4844,  128, 1420,  117, 4454, 2361,\n",
      "         606, 1249])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4520 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3733,   126,   113,   113,  7108,   128,   340,   221,   211,\n",
      "         1403,  2810,  7534,   238,   361,   105,   207,  2053,   211, 15774,\n",
      "          126,   207,   128,  1549,   821,   126,   210,   231,  5865,   318,\n",
      "          128,   555,   411,   333,   212,   364,   126,   113,  5722,   128,\n",
      "          210,  1619,   359,  2361,   117,   105,   447,   117,   213,   775])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5219 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1704,   117,   241, 22130,  2494,  2810,  7534,   110,   163,\n",
      "          631,   216,   105,   423,  4728,   311,  8296,   145,  7194,   964,\n",
      "          317,   472,   126,   113,   113,  5494,   128,  1505,   217,   709,\n",
      "          212,   472,  3569,   211,  1839,   599,   286,   117,   105,   241,\n",
      "        22130,   160,   339,   117,   213,   226,   374,   115,  1704,   117])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4718 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2611,   117,   425,   210,   207,  1664,   207,   276,  1115,\n",
      "          488,   460,   222,   207,   309,   210,   207,  1549,   425,   217,\n",
      "          207,  5865,  1664,   213,   226,   256,   117,   820,   110,   636,\n",
      "         1909,   115,  1704,   117, 20858,  3688,   115,  2522,   216,   207,\n",
      "          105,   997,  3534,   212,  1749, 22081,  4655,   210,  2810,  7534])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3911 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  820,  110,  954, 1249, 1676,  145,  394,  210,  318,  217,  229,\n",
      "         330,  223,  230, 1436, 3186,  213, 2810, 7534,  110,  163,  954, 1249,\n",
      "         117,  213,  226,  850,  210,  207,  528,  115,  207,  240, 1811,  189,\n",
      "         207, 2238, 1743, 1661,  217,  212,  351,  207, 2858,  210,  260,  228,\n",
      "         454,  117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4219 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 4320,  117,  105, 1601,  210,  482,  115, 1244,  150,  117, 1772,\n",
      "         117,  199,  174,  236,  516,  115,  160,  541,  117,  865, 5640, 2175,\n",
      "         353, 1309,  216,  105,  207, 1505,  116, 1959, 1955,  210, 1196,  645,\n",
      "         245, 1114,  207, 1267,  210, 1196,  216, 2495,  222,  370,  116, 2810,\n",
      "        7534,  160])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4692 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   212,  1498,   126,   113,   113,  7352,   128,  1830,   189,\n",
      "          210,  4799,  8431,   216,   145,  1393, 13402,   212,  1014,   352,\n",
      "          115,  2810,  7534,   648,   115,   261,  1088,   211,   219,   537,\n",
      "          222,  4799,  8431,   105,   212,   115,  1724,   115,   105,  2907,\n",
      "         1403,  2810,  7534,   238, 11952,   235,   145,   830, 16883,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4835 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   878,   126,   113,   113,  7634,   128,   728,   213,   275,\n",
      "          830,  1743,   216,   300,   481, 20394,   210,   126,   113,  5307,\n",
      "          128,   105,   230,   636,   744,  2715,  2467,   189,   212,  2038,\n",
      "          305,   935,   220,   629,   723,   213,  3032,   225,   231,   993,\n",
      "          517,   117,   105, 20858,  3688,   160,   924,   117,   789,   210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5465 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   820,   110,  2608,  6458,  1909,   115,  1704,   117, 10368,\n",
      "          115,  3372,   820,   110,   631,   115,  5415,   235,   126,   113,\n",
      "          113,  8014,   128,   237,  1029,   529,  1309,   221,   105,  2060,\n",
      "          216,  4212,   189,   212,  2292,   145,  2608,   110,   163,  2710,\n",
      "         3689,   212,   783,   145,  4683,   222,   229,   352,  1190,   189])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5261 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  248,  198,  210,  820,  110, 1249, 1290,  223, 9365,  213,  216,\n",
      "         233,  386,  227,  462,  220,  525,  115,  215,  728, 2832,  211, 2810,\n",
      "        7534,  115,  217, 1455,  229,  473,  281, 7978, 1539,  207, 1338,  105,\n",
      "        2810, 7534, 5262, 3689,  338,  115,  105,  606, 2050,  212,  221,  145,\n",
      "         540,  117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5407 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   382,   126,   113,   113,  4911,   128,  2562,   245,   602,\n",
      "          207,   650,   210,   231,  2298,   216,  2022,   306,   207,  2577,\n",
      "         2562,   115,   228,   216,   207,  1511,  2298,   465,   227,  2285,\n",
      "          105,   314,  1987,  6934,  7621,   115,   105, 11011,   142,   198,\n",
      "          117, 11667,   160,   186,   984,   116,  1321,   120, 12874, 13049])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4893 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5994,   366,   217,   207,   810, 20935,   547,   340,   221,\n",
      "          211,  3190,  1338,   105,  2810,  7534,  5262,  3689,   338,   105,\n",
      "        12666,   175,   314,  5413,   207,  1511,   850,   210,   207,  1029,\n",
      "          529,   117,  1704,   117, 10368,  1541,  1245,   231,  6557,   115,\n",
      "         4200,   238,  3528,   306,  7978,   168,   186, 10927,   252,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3991 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   257,  5262,  3689,   117,   321,   115,   149,   117,   151,\n",
      "          117,   115, 10368,   160,   186,  3561,   115,  3604,   120, 21378,\n",
      "          188,  5416,   160,  3976,   120,  1741,   117,   236,  7160,   111,\n",
      "        10060,   189,  7187,   112,   120,  1741,   117,   236, 24960,   111,\n",
      "         2904, 27950,   112,   120,   321,   323,  1741,   117,   236,  7079])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5502 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   222,   145,   680,  2427,   225,   207,   530,   680,   210,\n",
      "         2274,   115,   207,  1338,  7422,   189,   210,   239,  1029,   529,\n",
      "          424,  1920,   218,   142,   198,   210,   820,   110,  1249,   117,\n",
      "        11667,   160,   541,   120,  5208,  4219,   160,  1293,   117,  2948,\n",
      "          115,  1704,   117, 11667,  2522,   216,   142,   198,   210,   820])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4483 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1874,  1146,   306,   603,   211,  3248,   207,  1540,   216,\n",
      "          297,  1749,   519,   238,   820,   110,   668,   172,   890,   174,\n",
      "          235,  1290,   213,   142,   198,   210,   270,   954,  1249,   117,\n",
      "         3340,   126,   113,   113,  7454,   128,   820,  2827,   211,  1115,\n",
      "          460,   229, 25484,   189,   207,  2911,  4848,   213,   142,   198])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5093 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  207,  944,  217,  142,  202,  213,  820,  110, 1249, 1290,  211,\n",
      "         207, 4940,  186, 2906,  115,  735,  213, 2231,  528,  115,  332,  426,\n",
      "         117,  148,  112,  117,  207,  240,  249, 1078, 1266,  216, 2148,  210,\n",
      "         350,  105, 3078,  105, 1275,  115,  283,  324,  948, 1044, 4442, 1051,\n",
      "         115,  244])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5392 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1704,   117, 20858,  3688,  2522,   216,   142,   200,   261,\n",
      "          568,  2765,   552,   233,   783,  1706,   188,  2078,  2266,   117,\n",
      "        20858,  3688,   160,  3978,   117,  4741,   115,  1704,   117, 20858,\n",
      "         3688,   503,   227,  3412,   207,   568,   211,  1016,   213,  3178,\n",
      "         2078,  2266,   317,   870,  1802,   189,   210,  2810,  7534,   110])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4095 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1975,   164,   195,   235,   213,   683,   210,   142,   198,\n",
      "          210,   207, 10104,   325,   117,  2810,  7534,   115,  4752,   150,\n",
      "          117,   200,   174,   236,  1244,   117,   207,  3517,   240,   246,\n",
      "         2159,   211,  4129,   865,  5640,   110,   163,  1408,   210,   548,\n",
      "          217,   164,   195,   235,   117,   447,   117,   552,   207,   388])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 516, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3861 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   248,   337,   210,   820,   110,   954,  1249,   115,   575,\n",
      "          105, 21808,   708,   115,   105,  1379,   216,  2810,  7534,  1904,\n",
      "          105,   145,  2010,   301,  2903,  7978,   116,  2267,  1802,   210,\n",
      "          207, 21808,  2495,  3719,  1834,   117,   117,   117,  8213,   190,\n",
      "          225,   207,  3008,  3347,  2810, 17028,  1479,  3526,  9216,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 498, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3984 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 13251,   174,   210,   392,  1840,   212, 13733,   210, 10052,\n",
      "         2810,  7534, 11095,   115,   553,   117,  2637,   110,   163,  1743,\n",
      "          223,  5473,   221,  3612,   353,   268,   237,  2732,   211,  1009,\n",
      "         3347,   116,  8213,   190, 21808,  3202,   555,   226,   814,   117,\n",
      "          553,   117,  2637, 25742,   252,   275,   631,   210,   207,  2237])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 442, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5304 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113,  113, 6604,  128,  152,  117,  820,  110,  954, 1249,\n",
      "         142,  404,  248,  404,  210,  820,  110,  954, 1249, 3282,  189,  216,\n",
      "         105,  253, 2810, 7534,  408,  301,  470,  216,  220,  210,  239,  338,\n",
      "         244, 8213,  190,  126,  113, 5276,  128,  225,  220,  927,  847,  117,\n",
      "         117,  117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 556, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4899 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,   226,   256,   117,   321,  2231,   528,   115,   332,\n",
      "          426,   117,   148,   117,   213,   226,   374,   115,   207,   240,\n",
      "          249,  1653,   820,   110,  7522,   216,   207,  2858,   210,   548,\n",
      "          217,  2810,  7534,   110,   163,  2618, 17735,  1485,   948,   239,\n",
      "        21808,  6653,  3584,   115,  2810,  7534,   115,  4752,   150,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 554, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3382 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2810,  7534,   338,   212,  2810,  7534,   338,   117, 11011,\n",
      "          142,   433,   117,   150,   117,   820,   465,   227,  1115,   220,\n",
      "         6762,   460,   216,  2810,  7534,   936,   215,   213,   207,  1936,\n",
      "          249,  3372,   105, 21438,   105,   470,   210,  1619,   359,  1354,\n",
      "          217,   207,   592,   210,  5769,   235,  1044,  4442,   860,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 508, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3607 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  588, 2810, 7534,  469,  111,  105, 2810, 7534,  105,  112,  361,\n",
      "        4012,  212,  678,  239, 1804,  120,  207,  483,  240,  361,  835, 1601,\n",
      "         210,  482,  222,  713,  202,  115,  751,  212, 2314,  210,  266,  222,\n",
      "         692,  200,  115,  722,  120,  207,  354,  265,  240,  210, 1409,  217,\n",
      "         207,  483])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 509, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4419 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  610,  115,  211,  219,  632,  218, 2810, 7534,  212, 1583,  222,\n",
      "         145, 5004, 1934, 4846,  211,  207,  820,  212,  241,  852, 9178,  189,\n",
      "         115,  216,  783,  217, 2242, 4110,  547,  217, 7978, 1029,  529,  338,\n",
      "         115,  419,  216,  119,  198,  117,  207,  610,  245, 2688,  870, 4110,\n",
      "         547,  217])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 345, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4712 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   149,   117,  3790,   584,   720,   293,   207,  1003,   210,\n",
      "          226,   627,   371,   211,   207,   240,   115,  2810,  7534,   224,\n",
      "          468,   537,   217,   355,   218,   393,   276,   115,   217,   207,\n",
      "          996,   592,   210,  1044,  4442,  5934,   215, 10381,   235,   225,\n",
      "          145,  7978,  1029,   529,   424,   115,   126,   113,   113,  8848])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 415, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4798 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 23804,  4578,   612,   213,   207,  7978,  1885,   221,   472,\n",
      "          488,   211,   338,   216,   462,   382,  2085,   210,  4025,   658,\n",
      "          115,   271,   216,   207,  1265,   244,   370,   116,  3529,   225,\n",
      "          267,   211,   370,   116,  2810,  7534,   212,  2810,  7534,   338,\n",
      "          120,   212,   111,   146,   112,  4454,   215, 12937,   235,  2974])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 846, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4125 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  201,  117,  207,  333,  210,  220, 1214,  614,  242,  226,  248,\n",
      "         244,  213,  241, 2119, 1545,  225,  207, 1618,  333,  210,  226,  627,\n",
      "         371,  117, 1886,  207, 1618,  333,  210,  220, 1214,  614,  218, 2810,\n",
      "        7534,  294,  211,  226,  248,  115,  226,  627,  371,  386,  227,  115,\n",
      "         656,  215])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 875, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4349 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  200,  117,  207,  820,  224,  227, 2097,  126,  113,  113, 6876,\n",
      "         128,  220,  286,  215,  402, 1013,  238, 2810, 7534,  242,  226,  627,\n",
      "         371,  419,  217,  207,  592,  210, 3380,  739,  225,  226,  627,  371,\n",
      "         115,  213,  145,  384,  814,  211,  229,  289,  215,  353,  210,  207,\n",
      "         820,  223])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 912, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3977 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  146,  117,  213,  220, 1177,  814,  213,  229,  207,  240,  249,\n",
      "         765,  216, 2810, 7534,  249, 2104,  213,  145, 4435,  210, 3007,  212,\n",
      "        5493, 2581,  115,  207,  820,  245,  549,  211,  207,  240,  217,  145,\n",
      "         289,  116,  259, 1722,  210,  226,  627,  371,  210,  459,  211,  428,\n",
      "         637,  115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 774, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4201 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   145,   117,  3492, 28229,  1662,   115,  3566,  2487,  2060,\n",
      "          115,  1410,   252,  8285,   118,  4183,  2487,  2060,   115,  3107,\n",
      "        18975,   235,  2060,   215,   146,   117,  4025,   658,   271,   218,\n",
      "         2810,  7534,  2060,   216,   116,   116,   275,   115,   215,   213,\n",
      "          207,   398,  1452,   207,   126,   113,   113,  8061,   128,   993])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 910, 210, 910, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2558 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  207,  505,  126,  113, 6177,  128,  221,  145, 3245,  213,  207,\n",
      "         354,  265,  213,  145, 1222,  671,  215, 4179,  117,  220,  424, 1904,\n",
      "         174,  242, 7650, 1982,  215, 5609,  333,  215,  145,  505, 6751,  210,\n",
      "         207, 2810, 7534,  111,  162,  112,  215, 7978,  111,  162,  112, 3736,\n",
      "         836,  225])\n",
      "Original length: 1199 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  528,  126,  113, 4043,  128, 2231,  528, 1511,  213,  226,  256,\n",
      "         217,  207,  240,  110,  163,  904,  223,  207, 1374,  210,  145, 1090,\n",
      "         550,  119,  331, 1003,  210,  207,  627,  371,  954,  218,  207,  276,\n",
      "         223,  213,  207,  408,  302,  117,  207,  240, 1907,  226,  904,  294,\n",
      "         211,  207])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  204,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4597 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   222,   245,   422,   115,  1020,   115,   207,   354,   265,\n",
      "          678,   145,   966,   759,  3187,   216,  2810,  7534,   263,  2104,\n",
      "          213,  1395, 13408,   818,   213,   683,   210,   142,   142,   198,\n",
      "          212,   199,   210,   207, 10104,   325,   115,   342,   165,   117,\n",
      "          163,   117,   147,   117,   142,   142,   198,   115,   199,   117])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  205,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3616 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126, 2810, 7534,  128,  117,  117,  117,  225,  220,  714,  215,\n",
      "         533,  210,  207,  354,  265,  565,  215,  444,  211,  105,  207,  954,\n",
      "         582, 1664,  117, 1507,  115, 2810, 7534,  126,  113,  113,  206,  128,\n",
      "        2395,  226, 1248,  222,  624,  339,  115,  634,  117,  207,  354,  265,\n",
      "         571,  846])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  206,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5102 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  342,  165,  117,  163,  117,  147,  117,  142,  404,  111,  149,\n",
      "         112,  117,  207,  148,  117,  147,  117,  881, 6632,  174,  392, 2511,\n",
      "         353, 2641,  221,  237, 2613,  288,  207,  105,  592,  115,  507,  115,\n",
      "         212, 8253,  195,  210,  207, 1664,  117,  105, 2810, 7534,  115, 1185,\n",
      "         150,  117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  258,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3107 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  467,  207,  548, 1601,  115,  332,  210,  207,  408,  302, 1355,\n",
      "         261,  960,  731,  210,  207,  464,  211,  229,  207,  954,  582, 1664,\n",
      "         105, 1362,  189,  207,  506,  217,  237, 5859, 1249,  117,  126,  113,\n",
      "         113,  416,  128,  105,  236,  109,  164,  115, 6293,  150,  117, 1772,\n",
      "         117,  236])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  282,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3429 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,  3517,   240,  4051,   239,   528,   218,   126,   113,\n",
      "         3978,   128,  4245,   820,   110,   201,   142,   199, 10104,   325,\n",
      "          470,   212,  1280,   115,   331,   207,   483,   865,   263,  1541,\n",
      "          207,  1549,   327,   217,   504,   210,  3356,  2810,  7534,   110,\n",
      "          163,  5508,   195,   643,   117,   207,  3517,   240,  1266,   216])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  279,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4293 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1029,   126,   113,   113,   478,   128,   529,   621,   207,\n",
      "         6653,  2670,   189,   207,   105,   259, 12887,   235,   212, 10629,\n",
      "          105,   736,   210,  5314,   235,   212,  3241,   235,   115,  1330,\n",
      "          213,   207,  1104,   221,   105,  2307,   235,   115,   105,   207,\n",
      "          352,   211,   207,  1623,  1029,   529,   117,   447,   117,   236])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  337,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4187 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1260,  226, 1274,  115,  207, 3517,  126,  113,  113,  554,  128,\n",
      "         240, 1560, 2810, 7534,  110,  163, 2533,  211,  260,  210,  207, 1601,\n",
      "         218,  207,  483,  240,  117,  207, 3517,  240, 1671,  207,  483,  240,\n",
      "         110,  163, 1245, 1241, 1340,  210, 1601,  225,  374,  211,  142,  199,\n",
      "         548,  213])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  368,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4729 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   111,   198,   112,  1417,   153,   175,   677,   210,  1136,\n",
      "          211,   153, 17270,   126,   120,   128,   117,   117,   117,   111,\n",
      "          199,   112,  1417,   153, 17270,   145,  8317, 22342,   217,   260,\n",
      "         1390,   207,   153,   171,   186,  4004,   459,   217,   446,  1260,\n",
      "          207,   153,   175, 28229,   577,   126,   120,   128,   117,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  342,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4383 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,  3517,   240,   540,   252,   207,  1595,  1504,   210,\n",
      "         2810,  7534,   818,   242,   207,   940,   105, 21808,   105,   213,\n",
      "          538,   211,   105,   145,   284,   210,  3202,  2268,   218,  3347,\n",
      "         2810, 17028,   105,   111,   105,  3347,   105,   112,   117,   447,\n",
      "          117,   207, 21808,  3202,   244,   735,   221,   105,   789,   816])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  404,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4393 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1602,   211,   207,   348,   116,   735,  2858,   210,   548,\n",
      "          294,   211,   142,   199,   210,   207, 10104,   325,   115,   207,\n",
      "         3517,   240,  1146,   548,   306,  2810,  7534,   217,  2581,   210,\n",
      "          207,   444,   264,   266,  1768,   210,   105,   207, 10104,   325,\n",
      "          117,  4752,   150,   117,   200,   174,   912,   115,  1100,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  416,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4878 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5309,   207,   931,   189,   210,   145,  5865,  1664,   213,\n",
      "          237,  5859,   256,   117,   447,   117,   213,   226,   374,   115,\n",
      "          207,  3517,   126,   113,   113,  1007,   128,   240, 10019,   174,\n",
      "          216,   105,   145,  1126,  1664,   213,   237,  5859,   256,   311,\n",
      "         1968,   211,   110,  8784, 14092,   145,   327,   238,  1395, 13408])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  422,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4201 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 15982,   190,   811,   213,   207,   159,   189,   327,   117,\n",
      "          117,   117,   207,  5859,   126,   113,  4217,   128,   588,   110,\n",
      "          163,   126,   113,   113,  1048,   128,  2093,  6563,   305,   219,\n",
      "         7111,   218,   110,   237,  2411,   351,   207,  2849,   210,   216,\n",
      "          818,   117,   110,   105,   447,   117,   111,  3284,   200,   244])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  433,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4938 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3028,   225,   207,   627,   371,   117,   105,   447,   117,\n",
      "          450,   115,   212,   353,  1106,   301,   115,   207,   454,   386,\n",
      "          227,  3430,  2810,  7534,   238,   105,  1283,   731,   211,   220,\n",
      "         9178,   225,   267,   211,   220,  2810,  7534,   424,   215,   446,\n",
      "          343,   216,   731,   126,   113,   113,  1133,   128,   223, 11633])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  339,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4349 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2810, 7534,  249, 9163, 8746,  428,  338,  229,  244, 1182,  242,\n",
      "         207, 5859,  534,  117, 2388,  115,  207, 1229, 2001, 7019,  216,  115,\n",
      "         221,  145, 2946,  691,  115, 2810, 7534, 7851,  207,  126,  113,  113,\n",
      "        1185,  128,  643,  211, 1025,  229, 2060,  473,  233,  261,  698,  213,\n",
      "         207,  338])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  441,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4958 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,  1123,   128,   207,  1673,   850,   210,\n",
      "          207,  1229,  1708,   189,  3338,   213,   207,  1664,   210,  1149,\n",
      "         3202,   115,   271,   207,   350,  3202,  1362,   411,   506,   117,\n",
      "          211,  2260,   281,   207,   961, 23419,   850,   210,   207,  1229,\n",
      "          115,   207,  2060,   473,   311,   296,   219,  1904,   174,   221])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  478,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4356 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  222, 6616, 1625, 7080,  301,  115,  227,  235,  216,  343,  207,\n",
      "        6127, 1092,  223, 2003,  115,  220, 3910,  210, 6127,  223, 8191,  212,\n",
      "         115,  420,  115,  314,  643,  117,  354,  265, 9867,  117,  236, 1033,\n",
      "         117,  934,  145, 6104, 3359,  115,  207,  240,  988,  145, 3491,  222,\n",
      "        6616,  210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  516,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4910 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,  1456,   128,   207,  2592,  1500,  7480,\n",
      "          210,   142,   426,   117,   146,  2042,   216,   233,   305, 14680,\n",
      "         2810,  7534,   110,   163,  1178,   210,  1260,   327,   645,  3057,\n",
      "          111,   105, 10592,   189,   105,   112,   212,   231,  2632,   189,\n",
      "          117, 16672, 14722,  1917,   226,  7480,   245,   219,   115,   233])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  498,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5310 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   447,   117,   142,   953,   117,   157,   117,  1881,   238,\n",
      "          207,  3282,   174,  7320,  4655,   213,   142,   426,   117,   147,\n",
      "          117,   198,   223,  2810,  7534,   110,   163,  2053,   211,  1042,\n",
      "          207, 18429,   189,   115,  1739, 16555,   189,   115,   215, 23804,\n",
      "         4578,  1869,   213,   145,   382,   839,   623,   211,   207,  4025])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  442,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5549 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2974, 14480,   210,   370,   116,  2810,  7534,  5262,  3689,\n",
      "          338,   307,   343,   105,   145,  2810,  7534,  5262,  3689,   424,\n",
      "          216,   783,   578,  4025,   658,   297,   378,   219,  9131,  2527,\n",
      "          236,   216,   259,   105,   212,  5826,  7588,  1779,   306,   228,\n",
      "         2974, 14480,   117,   447,   117,   343,   228,   370,   116,  2810])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  556,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5148 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   148,   117,  3492,   808,  2126,   111,   105,   153,   171,\n",
      "          186,   105,   112,  3704,   213,   207,   914, 21217,  6833,   213,\n",
      "          336,   115,  2069,   147,   117,   202,   223,  3820,   236,  1249,\n",
      "          235,   207,  1265,   222,  9178,  1828,   210,   207,   914, 21217,\n",
      "         6833,   116,   105,   207,   736,   216,  2299,   207,   296,   259])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  554,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5543 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3517,   240,   765,   216,  2810,  7534,   110,   163,  1265,\n",
      "          222, 18429,  4289,   212,  2290,   115, 20512,   189,   212, 23804,\n",
      "         4578,   115,   212,   231,  3916,   189,   210,   207,  4289,   210,\n",
      "          207, 19068,  4961,   263,   207,   363,   210,   105,  3762,  1106,\n",
      "          418,   306,   207,  9178,   189,   117,   105,  2810,  7534,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  508,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5629 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 18371,  4418,   189,   210,   142,   426,   117,   152,   117,\n",
      "          199,  5455,   216,  2810,  7534,   481,  4710,   239,  2060,   211,\n",
      "         1859,  2068,   281,   207,   506,   126,   113,   113,  1076,   128,\n",
      "          210,   145,   105,  1182,   211,   186,   116,   680,  6197,   105,\n",
      "         4475,   235,   591,   105,   241,   210,   207,  1830,  5500,  1877])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  509,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4938 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,  9372,   152,   117,   199,   111,   146,   112,  1137,\n",
      "          189,   207, 29253,   189,   210,   207,  3517,   240,   110,   163,\n",
      "         1108,   212,  1951,  1069,   189,   207, 28885,   195,   210,   207,\n",
      "         2803,   850,   210,   207,   454,   117,   213,  3130,   340,   115,\n",
      "          142,   426,   117,   152, 17146,   189,   225,   207,   408,   302])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  345,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4675 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2810,  7534,   115,  4752,   150,   117,   200,   174,   236,\n",
      "         1237,   116,  1431,   111, 26913,   189,  1315,   213,   856,   112,\n",
      "          117,  5932,  2810,  7534,   110,   163,  2238,  4040,   216,   233,\n",
      "          386,   227, 14588, 28229,   235,   212,   370,   116, 28229,   235,\n",
      "          473,   115,   207,  3517,   240,  6435,   216,   105, 25542,   249])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  415,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3823 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2768,   145,  2404,  4495,   212,  4948,   126,   113,   113,\n",
      "         2377,   128, 14618,   222,  2060,  1650,   212,   481,   247,  1160,\n",
      "         4839,   189,   217,  2765,   117,   105,   354,   265,  9867,   117,\n",
      "          236,  2925,   116,   339,   117,   207,   389,  2107,   395,   222,\n",
      "          211,  7273,   216,   119,  1378,   211,   207,  1029,   529,   216])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  846,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4920 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   683,   210,   142,   199,   117,  4752,   150,   117,   200,\n",
      "          174,   912,   115,  1400,   116,  1525,   117,   716,   301,   115,\n",
      "         1831,  2096,  3189,   211,   207,   105,   296, 10091,   608,   115,\n",
      "          105,   207,  3517,   240,  1266,   216,   105,   211,   207,   464,\n",
      "         2810,  7534,   110,   163,   296, 10091,   608,   225,   223,   192])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  875,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5430 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  147,  117,  857, 5975,  115,  857,  645,  608,  115,  212,  857,\n",
      "         410,  608,  126,  113,  113, 2425,  128,  207, 1673,  850,  210,  142,\n",
      "         426,  117,  151, 1907,  969,  216, 1073, 2069,  151,  117,  198,  696,\n",
      "         151,  117,  199, 5019, 2810, 7534,  238, 2902,  126,  113, 4396,  128,\n",
      "         288,  608])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  912,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5169 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  111, 2937,  213,  856,  112,  117,  263,  207,  389,  255, 2159,\n",
      "         211, 1115,  145, 3579, 3483,  212, 7273, 3079,  363,  210,  207, 1887,\n",
      "         213,  142,  426,  117,  151,  217,  608,  213,  229, 2810, 7534, 2361,\n",
      "        1619,  359,  115,  207,  240, 1219,  705,  247, 1266,  216,  207, 1887,\n",
      "         386,  227])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  774,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5354 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  201,  117, 7319, 1019,  115, 1044, 4442, 1051,  115,  212,  497,\n",
      "         318, 1557,  426,  117,  148,  212,  426,  117,  149, 1309, 2161,  207,\n",
      "        1019,  210, 7319,  189,  115, 1798, 3845,  115,  212,  231,  927,  286,\n",
      "         217,  504,  210, 2651, 2973, 1044, 4442,  860,  225,  126,  113,  113,\n",
      "        2925,  128])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  910,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5562 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   529,   424,  3214,   211,  1842,   306,   216,  7978,  1029,\n",
      "          529,   424,   213,   308,   211,  1150,   220,   410,   238,   216,\n",
      "         7978,  1029,   529,   424,   117,   105, 29829,   176,   180,   142,\n",
      "          426,   117,   148,   117,   805,  1499,   995,  2101,  5455,   216,\n",
      "          207,  1229,   210,  7319,   223,   211,   185,  4533,   117,   213])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  984,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5109 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   302,   213,  2157,  7319,   189,   216, 22018,  2810,  7534,\n",
      "          110,   163,  1415,  1958,   211,  2097,   603,   116,  2076,   115,\n",
      "          343,  2810,  7534,   110,   163,   639,  5262,  3689,   223,  4320,\n",
      "          225, 16883,  5262,  3689,   216,  6904,   189,   207,  1196,  8312,\n",
      "          211,  1003,   117,   105,   447,   117,   213,   226,   374,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1043,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5443 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   986,  4624,   954,   126,   113,   113,  3725,   128,   582,\n",
      "         1664,   189,   294,   211,   207, 10737,  4771,   325,   244,  3236,\n",
      "          211,   490,  1686,  1626,   211,   220,  1498,  6230,   195,   115,\n",
      "         1750,   626,   233,   223,   207,   483,   240,   229,   311,  1960,\n",
      "          207,  1664,   117,  2810,  7534,   115,  1185,   150,   117,   200])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  909,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4735 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3947,   497,   211,   207,  6184,  3282,   174,   218,   142,\n",
      "          426,   117,   148,   212,   149,   223,   142,   426,   117,   153,\n",
      "          115,   229,  2044,   207,   333,   294,   211,   229,  2810,  7534,\n",
      "          311,  1214,   239,  1619,   359,   277,   213,  2887,   225,   233,\n",
      "          357,   242,   207, 29829,   176,   180,   117,  2069,   153,   783])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  673,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5298 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1044, 4442,  860,  117, 2810, 7534,  115, 1191,  150,  117, 1772,\n",
      "         117,  199,  174,  236, 1456,  117,  213,  226,  374,  115,  207,  253,\n",
      "         180,  329, 2810, 7534,  211,  468,  207,  444,  850,  189,  210,  207,\n",
      "        1959,  473, 4846,  236,  145,  105, 2136, 1068,  105,  343,  489,  652,\n",
      "         481,  105])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  895,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4815 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   954,  1664,   211,  6513,   215, 25187,  1104,  2593,\n",
      "          236,   239, 20863,   183,   117,   447,   117, 19654,   301,   115,\n",
      "         9372,   111,   148,   112,   386,   227,  4995,  2810,  7534,   211,\n",
      "          207,  1719,   110,   163,  1619,   359,   115,   310,  2388,   783,\n",
      "          217,   972,   393,   116,   257,  2274,   212,  2478,   126,   113])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  925,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4676 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1248,   189,   126,   113,   113,  4043,   128,   210,   207,\n",
      "          318,   117,  1831,   226,  2791,   223,   632,   115,   207,   240,\n",
      "          261,  1811,   207,  1024,   346,   212, 25212,   190,  1840,   802,\n",
      "          207,   739,   212,  1177,   318,   212,  2351,   331,   392,  1840,\n",
      "         3190,   226,  4095,   210,   207,  1664,   211,   219,   213,  7020])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1027,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5168 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  221,  207,  927,  526,  223,  207, 1177, 3659,  210,  207,  389,\n",
      "         115,  207, 1436, 3186,  211,  207,  526,  223,  207, 2810, 7534,  888,\n",
      "         739,  714,  117,  213, 3860,  126,  113,  113, 4153,  128,  211,  207,\n",
      "        6666,  210,  207,  927,  526,  115,  390,  115,  207,  739,  714,  223,\n",
      "         994,  353])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1007,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5494 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   240,   117,   447,   117,   142,   628,   117,   145,   117,\n",
      "          201,   117,   220,   228,  2764,   261,   227,   725,   145,  1372,\n",
      "          211,  1177,   225,   267,   211,   105,  7472,   115,  3007,   215,\n",
      "         5493,  2581,   117,   105,   447,   117,   361, 10187,  2950,   207,\n",
      "          318,   599,   211,   739,   212,  1177,   954,   218,   207,   276])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  899,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5428 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   146,   117,  3169,   210,   585,   289,   210,   207,   353,\n",
      "        25212,   190,  2044,   126,   113,   113,  3926,   128,  1810,   213,\n",
      "          207,  2101,   223,   207,   482,   216,  1073,  2810,  7534,   115,\n",
      "          696,   207,   389,   115,   244,  3236,   242,   207,   954,  1664,\n",
      "          211,   594,   211,   207,   240,   948,  2810,  7534,   110,   163])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1100,  210, 1100,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4199 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   212,   207,   354,   265,   111,   212,   207,  8289,   235,\n",
      "          265,   112,   297,   753,   211,  1836,   207,   954,   627,   371,\n",
      "          211,  1388,   217,   207,   240,   115,   213,   775,   211,   207,\n",
      "         1482,   530,   301,   612,   213,   207,   954,   627,   371,   115,\n",
      "          207,   643,   211,  4220,   171, 15621,   550,  1730,   215,  6096])\n",
      "Original length: 3996 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,   113,   113, 17711,   128,   126,   113,\n",
      "        20923,   128,  3304, 13867,   189,   171,   115,   881,   865,   117,\n",
      "          207,   595,   116,  1421,   115,   153,  8401,  2107,  8401,   111,\n",
      "          105,  2107,  8401,   105,   112,   115,  2913,   709,   210,   207,\n",
      "          627,   371,   210,   207,   354,   265,   483,   240,   217,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4017 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2107,  8401,   105,   758,  2583,   212, 15220,   241,  2198,\n",
      "          212,   241,   804,   210,   126,  2107,  8401,   110,   163,   128,\n",
      "         4629,  8793,   117,   105,   207,   671,   323, 12350,   216,   650,\n",
      "          211,  1168,  2198,   212,  1809,   210,   207,  5497,   189,   105,\n",
      "          261,   519,   213,   384,   347,   217,  1023,   212,  3245,  1354])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3527 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1023,   212,   207,   110,  9797,  1023,   117,  5795,  7696,\n",
      "          110,   163,  3757,   189,   950,  3245,   212,  3427,  1016,   470,\n",
      "         3442,   211,   207,   643,   116, 11701,  1233,   115,   212,  1354,\n",
      "          210,   207,   110,  7551,  1650,  1023,   117,  1130,  5795,  7696,\n",
      "          503,   227,  3757,   217,  1354,   210,   207,   110,  9797,  1023])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5270 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   240,   545,   152,   184,  1570,   126,   128,   216,   142,\n",
      "        21304,   111,   145,   112,   111,   198,   112,  2612,   189,  1143,\n",
      "          211,   542,  3517,   585,   569,   307,   126,   113,   113,   113,\n",
      "        10990,   128,   324,  1409,   213,   229,   207,   759,   755,   207,\n",
      "          483,   240,   211,   542,   585,   294,   211,   508,   165,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4957 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  4019,   126,   128,   105,   221,   145,   336,\n",
      "         5696,   115,   372,  4624,   145,   483,   240,   110,   163,   371,\n",
      "         1918,   522,  5859,   266,   115,   532,   244, 11324,   218,   207,\n",
      "          266,   210,   207,  1166,   881,   213,   229,   216,   483,   240,\n",
      "         6379,   189,   117,   117,   117,   117,   105,   213,   665,   213])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4340 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   293,   116,   327,  9740,  2305,  5497,   327,   217, 20634,\n",
      "        13725,  9740,   189,   105,   314,   435,  2483,   460,   117,  5795,\n",
      "         7696,  1564, 21881,  3730,   126,   113,   113,   113, 13241,   128,\n",
      "          126,   113,   113,   441,   128,   460,   216,   239,  2305,  5497,\n",
      "          189,  4310,   351,  1812,   405,   174,  5497,   189,   216,   244])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4429 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   319,   211,   117,   117,   117,  1960,   239,  1023,   115,\n",
      "          212,   216,  1676,  7219,   832, 13964,   189,   225,  1333,   117,\n",
      "          105,  4577,  6238,   115,   521,   117,   166,   117,  6279,  9272,\n",
      "          115,   521,   117,   115, 10208,   150,   117,   199,   174,  4153,\n",
      "         2450,   115, 24512,   115,  5273, 15517, 10313,   115,  8614,   111])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4902 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  211, 3726, 1333,  351, 3226,  171,  117,  202,  934,  226, 2734,\n",
      "         245,  215,  245,  227,  462,  145,  421,  238,  229,  145,  480, 1336,\n",
      "         481, 8247, 3078, 1270, 6216, 3226,  171,  111,  229,  223,  227,  207,\n",
      "         550, 1438,  112,  115,  233, 1528,  386,  227,  462,  145,  885,  421,\n",
      "         238,  229])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3136 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   111,  1954,   117,   682,   117,  1138,   112,   111,  3284,\n",
      "         1672,  1610,   117,   166,   117,  2307,  2202,   115,   521,   117,\n",
      "          115,  9421,   150,   117,   199,   174,  8676,   115,  9471,   115,\n",
      "          516, 15517,   973,   174,  3794,  2837,   115, 27167,   111,  1954,\n",
      "          117,   682,   117,  1167,   112,   111,  1108,   216,  2664,   222])\n",
      "Original length: 74 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   308,   665,   119,   595,   110,   163,   867,   217,\n",
      "         1152, 10088,   153,   117,  2500,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 199, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3487 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   894,  6954, 23151,  7246,   189,   115,   521,   117,   212,\n",
      "         5772,  2983,  9712,   115,   239,   706,   866,   115,  2778,  2120,\n",
      "          194, 14778,   189,   115,  2625,   603,   225,   370,   116,  2120,\n",
      "          194,  1483,   115,  2406,   207,  2120,   194,  3245,   115,   212,\n",
      "          126,   113,   199,   128, 10333,   212,   665,   116,  1240,   207])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 200, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3439 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   707,   120,   200,   112,  7046,  1650,   120,   201,   112,\n",
      "          254,  9503,   120,   202,   112,  5943,   190,   116,  1510,   120,\n",
      "          203,   112, 17363,  9328,   120,   204,   112, 23434,   116,  1510,\n",
      "          120,   205,   112, 24175,   188,   120,   206,   112,  2120,   194,\n",
      "        18973,   171,   120,   258,   112, 18973,   171,   115,   555,   959])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3139 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6954, 23151, 10333,   189,   213,  8513,   189,   212,   236,\n",
      "          616,  2777,   212,  1280,  2675,   189,   207,  2606,   211,   219,\n",
      "          280,   218,  1809,  2120,   194, 14778,   189,   225,  1061,  7246,\n",
      "          189,   117,   111, 14803,   100,   100,   896,   116,  1244,   112,\n",
      "          117,   217,  1363,   115,   145,  6954, 23151,  1273,  2787,  1904])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3262 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   145,   117,   207,  7246,  1946,   189,   244,  8543,   174,\n",
      "          225, 14708,   212,   340,   244,   353,   309,   211,  2068, 30518,\n",
      "          115,   221,  2878,   211,  3037,  2120,   194,  7246,  1946,   189,\n",
      "          216,   244, 20611,   252,   222,   207, 14923,   117,   111, 14803,\n",
      "          100,   743,   112,   120,   146,   117,   207,   370,   116,  3037])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2650 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2711,   174,   213,   145,  3037,  2120,   194,  7506,  2665,\n",
      "          117,   111, 14803,   100,  1039,   112,   117,   390,   115,   207,\n",
      "          201,   118,   404,   118,   662,  6326,   246,   307,  2513,   218,\n",
      "          145,  6954, 23151,   804,   806,   212,  3682,   117,   111, 14803,\n",
      "          100,  1133,   112,   117,  1700,   115,   207,   298,   405,  1477])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3933 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2120,  194, 3620,  189,  217, 1152,  371,  222,  239, 3245, 9634,\n",
      "         235,  115, 1354,  212, 5426,  470,  117,  207, 1540,  213, 1059,  213,\n",
      "         226,  256,  247,  255, 1287,  213, 5691, 2797, 1641,  213,  226,  212,\n",
      "         231,  881,  189,  117,  207,  240,  261,  844,  226,  867,  222,  207,\n",
      "         376, 1030])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4682 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 11840,   189,  2147,   216,   275,  1963,  1483,   244,   227,\n",
      "         3037,  2120,   194,  1483,   120,   216,   300,   223,   227,  1070,\n",
      "          174,   225,  2120,   194,   120,   212,   216,   207,   775,   210,\n",
      "          370,   116,  2120,   194,  1483,   261,  2416,   207,  2120,   194,\n",
      "         1890,   117,   390,   115,   300,  3915,   216,   207,  1483,   300])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5021 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   934,   352,   210,   207,   163, 21076,  9272,   893,  1528,\n",
      "         4722,   189,   207,  3986,   210,  4819,   213,   226,   256,   115,\n",
      "         5374,   211,   207,   163, 21076,  9272,  1355,   223,  2710,   301,\n",
      "          369,   117,   207,  2023,   212,   737,   244,   227,   578,   115,\n",
      "          335,   244,  2797,   100,   894,   244,  1260,  2120,   194,   110])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4279 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6640,  1615,   225,   237, 14315,   215, 11737,   424,   215,\n",
      "          446,   117,   105,   447,   117,   115,   236, 15549,   158,   117,\n",
      "          204,   117,  3986,   210,  4819,   223,   227,   329,   217,   145,\n",
      "         1408,   210,  3245,  5426,   117,   447,   117,   236, 19196,   120,\n",
      "          321,   323,  7669,   441,  1071,  1471,  1610,   117,   166,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4532 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   249,   255,  1653,   117,   105,  9292,  6954,  1315,   189,\n",
      "        11608, 23809,   166,   117,   166,   117,   149,   117,   146,   117,\n",
      "         9292,  6954,  1315,   189,   115,  8553,   184,   171,   115,  5146,\n",
      "          150,   117,  1772,   117, 19137,   115, 17883,   111,   163,   117,\n",
      "          148,   117,   158,   117,   169,   117,  3671,   112,   120,   321])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 449 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1113,   213,  1140,   115,  6954, 23151,   110,   163, 22325,\n",
      "          210,   207,   296,   496,  1372,   223,   213, 16975,  8950,   190,\n",
      "          175,   115,   212,   233,   249,   227,  1624,   239,  1874,   210,\n",
      "         1683,   948,   239,  1372,   210, 24412,   184,  5850,   117,   809,\n",
      "          115,   221,   207,  5220,   889,   635,   470,   217,  3245,  1354])\n",
      "Original length: 2202 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   528,   109,   308, 11203,   117, 15002,  4066,   577,\n",
      "          115,  3461,   117,   115,   483,   865,   119,   588, 22943,   171,\n",
      "         1610,   117,   111,   105, 22943,   171,   105,   112,  3620,   189,\n",
      "          217,  1152,   371,   294,   211,  1954,   117,   162,   117,  3195,\n",
      "          117,   160,   117,   111,   105,  2166,   173,   186,   105,   112])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3506 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   867,   211,  2310,   217,   650,   211,   264,   145,   388,\n",
      "          306,   229,  1322,   481,   219,   614,   117,   213, 14069,   252,\n",
      "        17531,   115,   884,   117,   166,   117, 22943,   171,  1610,   117,\n",
      "          115,   634,   165,   117,   163,   117,  6394,   117, 13648,   189,\n",
      "         4448,  2967,   115,   527,  3195,   117,  9852,  2567,   115,  7739])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5255 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   608,   292,   745,   236,   870,  1729,   111,   153,   117,\n",
      "          149,   117,   115,   979,  1094,   212,   463,  1094,   112,   115,\n",
      "          212,   420, 22943,   171,   503,   227,  1136,   207,   428,   640,\n",
      "          189,   870,   907,   213,  5602,   693,   115,   221,   329,   218,\n",
      "          207,   162,   116,   160,   325,   117,   152,   184,  1570,   126])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5186 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 6624,  289,  238, 7372,  235,  145,  850,  210,  207,  593,  211,\n",
      "         207, 1486,  338,  236,  116,  550,  111,  153,  117,  149,  117,  115,\n",
      "         207,  160,  189, 8551,  174,  424,  112,  117,  207, 1549, 1725,  210,\n",
      "         344,  280,  217,  207,  160,  189, 8551,  174,  424, 1763,  218, 1092,\n",
      "         211, 2807])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5225 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213, 14119,   126,   113,   342,   128,   210,   213, 14069,\n",
      "          252,   110,   163,   388,   242,   207,   162,   116,   160,   325,\n",
      "          115,  1152,   371,   305,   219,   614,   351,   213, 14069,   252,\n",
      "          217,  3399,  6339,   117,   221,  1582,   348,   115, 22943,   171,\n",
      "          110,   163,   388,   948,   207,   213, 14119,   210,   213, 14069])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4966 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 15434,   195,  1741,   117,   236,   160,   117,  3674,   116,\n",
      "         3016,   117,   809,   115,   213, 14069,   252,   356, 10899,   388,\n",
      "          115,   221,   145,  2946,   691,   115,   216,   233,   481,   227,\n",
      "         2764,   207,   397,   681,   281,   207,   632,   743,   437,   117,\n",
      "          213,   374,   211,   207,   393,   550,   115,   213, 14069,   252])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2878 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2015,  115,  105,  145,  438,  245,  219, 5823,  252,  218,  145,\n",
      "        1815,  257, 5446,  301,  253,  275,  582,  211,  219, 1492,  218,  207,\n",
      "         232,  246, 4750,  174,  591,  218,  145,  397, 4611,  383,  115, 1917,\n",
      "        8856,  301,  280,  115,  215,  145, 5716,  117,  105,  202, 3000,  117,\n",
      "        3195,  117])\n",
      "Original length: 1954 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  528,  126,  113, 7011,  128,  308,  530,  301,  373,  207,  240,\n",
      "         223,  588,  719,  117, 2341,  110,  163,  780,  529,  115,  521,  117,\n",
      "         115,  212,  719,  117, 2341,  110,  163, 8860,  212, 8026,  189,  115,\n",
      "         521,  117,  111,  105,  719,  117, 2341,  105,  112,  110,  163,  867,\n",
      "         217, 1152])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4399 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  371,  221,  145,  691,  210,  266,  117,  207,  240,  323, 2769,\n",
      "         216, 6010,  195, 1322,  223,  227,  145, 1549, 1249,  217, 7076,  110,\n",
      "         163, 8026,  213,  226,  256,  117, 2366,  153,  117,  847,  152,  184,\n",
      "         866,  126,  128,  525, 1185,  111,  147,  112,  115,  522,  409,  210,\n",
      "         966,  492])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4645 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,  1077,   115,   719,   117,  2341,   110,   163, 16274,\n",
      "          252,   222,   145,  7082,   211, 10908,   239,   780,  1495,  1410,\n",
      "          281,   207,  5211,  3540,   299,   117,   211,   226,   711,   115,\n",
      "          719,   117,  2341,   110,   163,  1637,   211,   438,   212,   118,\n",
      "          215,  1070,   225,  3522,   212,  2594,   189,   213,   237,  3386])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4682 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  221,  332,  210,  207, 3562,  210,  207, 8026,  115,  719,  117,\n",
      "        2341,  110,  163,  212,  595, 8860,  835,  288,  145, 3238,  232,  117,\n",
      "         242,  207,  350,  232,  115,  595, 8860,  297,  230, 1579,  219, 1492,\n",
      "         218,  207,  856, 1200,  227,  211, 4310,  765,  213,  207,  551,  952,\n",
      "         117,  595])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4527 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 3019, 7349,  616,  120,  199,  112,  216,  207,  316, 1999,  223,\n",
      "         145,  558, 1131, 2151,  327, 1725,  120,  212,  200,  112,  216,  207,\n",
      "         316, 1999, 2488,  189, 5448, 5859, 3031,  117, 7076,  110,  163, 8026,\n",
      "         245, 3337,  222,  241,  210,  239,  207, 7791,  115,  805,  126,  113,\n",
      "         113,  404])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4052 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   435,   460,   216,   207, 10104,   325,   246,   227,  1733,\n",
      "          211, 13609,   226,   816,   210,  1178,   223,   207,   482,   216,\n",
      "        11710,   189,   356,   212,   247,  1956,   207,   719,   117,  2341,\n",
      "          110,   163,  1410,   211,   438,   225,   289,   210,   719,   117,\n",
      "         2341,   110,   163,  2381,   189,   117,   207,   952,   317,   719])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3272 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   221,   211,  1683,   210,   327,   643,   115,  7076,   110,\n",
      "          163,  8026,  3503,   216,   115,   487,   222, 12603, 22652,   110,\n",
      "          163,   594,   115,   719,   117,  2341,   110,   163,   249,  1593,\n",
      "          215,   438,   252,   225,  1185,   108,   210,   207,  4927,   116,\n",
      "        10446,   184,  8860,   213,   207,  5211,  3540,   327,   117,  7076])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4896 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113,  113,  508,  128,  146,  117,  207,  316, 1999,  221,\n",
      "         145, 4809,  327, 1725, 2481,  290,  210, 7076,  110,  163, 8026,  110,\n",
      "         163,  759, 3503,  216,  207,  316, 1999,  115,  835,  288,  317,  719,\n",
      "         117, 2341,  110,  163,  212,  595, 8860,  367,  211,  207, 3562,  210,\n",
      "         207, 7076])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4509 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   487,   222,   392,   889,   115,   207,   316,  1999,   126,\n",
      "          113,   113,   846,   128,   246,  3132,  1181,   211,   207,  3238,\n",
      "          232,   117,   207,   350,   316,  1999,   246,   369,   552,   115,\n",
      "          217,   289,   115,   233,  1622,   207,   595,  8860,   211,  1178,\n",
      "         5024,   117,   233,   297,   219, 16034,   301,   217,   207,   595])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4484 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1152,  371,  236,  404,  117,  111, 2937, 2195,  126,  113,  113,\n",
      "         910,  128,  213,  856,  112,  117,  809,  115, 6252, 7076,  110,  163,\n",
      "        8026,  115,  719,  117, 2341,  110,  163,  307, 2913,  211, 3430, 1149,\n",
      "        1016,  117,  282, 1067, 7076,  110,  163, 8026, 8908,  189,  223,  216,\n",
      "         207,  997])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5226 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1021,  235, 1410, 8398,  217,  220,  350, 8860, 6695,  218, 7076,\n",
      "         110,  163, 8026,  117, 7076,  110,  163, 8026,  470,  216,  392,  872,\n",
      "         247, 5769,  252,  207, 5758,  820,  115,  390, 7076,  110,  163, 8026,\n",
      "        1907,  230,  485,  217,  949,  696,  386, 7076,  110,  163, 8026, 1968,\n",
      "         220, 4162])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1332 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1182,  872,  210,  719,  117, 2341,  110,  163,  117,  230,  485,\n",
      "         211, 8347,  207,  333,  210,  207, 8398,  232,  249,  255,  280,  115,\n",
      "        1917,  552, 7076,  110,  163, 8026,  470,  681,  210,  207, 3137,  340,\n",
      "        1824, 1242, 8136, 2589,  105,  667, 1270,  212, 1156, 2775,  105,  827,\n",
      "         115,  233])\n",
      "Original length: 1961 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   615,   230,   117,   527,   116,  4434,  2837,  4301,   634,\n",
      "         5830,  2153,  5479,   113,   120,  5366,  7573,   117,   199,   174,\n",
      "         3643,   113,   113,   120,  8221,   158,   117,   167,   117,   199,\n",
      "          174,  9012,   113,   113,   113,   120,   634,  7573,   173,   117,\n",
      "         2153,   117, 13648,   189, 15225,   113,   113,   113,   113,   120])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 199, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1924 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   224,   219,  4191,  4474,   460,   210,  1870,   215,   363,\n",
      "          211,  4750,   207,   427,   210,   231,  5212,   115,   215,   211,\n",
      "         3427,   301, 10868,   616,   238,   145,  2381,   115,   215,   211,\n",
      "          378, 16324,   145,  2381,   117,  5859,   109,   616,   266,   123,\n",
      "          408,  1177,   123,   264,   966,   872,   152,   184,   973,   126])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 200, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2163 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 5859,  109,  616,  266,  123,  408, 1177,  123,  264,  966,  872,\n",
      "         152,  184, 2450,  126,  128,  408, 1177,  115,  264,  966,  872,  321,\n",
      "        7573,  117, 3496,  117,  142,  613,  117,  345,  111,  199,  112,  111,\n",
      "         147,  177,  112,  117, 5859,  109,  616,  266,  123,  408, 1177,  123,\n",
      "         264,  966])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1691 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  966,  492,  123, 1409,  123, 1327,  210,  709,  123, 1528, 4337,\n",
      "         709,  152,  184, 4290,  126,  128, 1327,  210,  709,  115, 1528, 4337,\n",
      "         709,  237, 3517,  240,  261, 4129, 4757, 1041,  253,  207,  881,  240,\n",
      "         740,  207, 1367,  266,  211,  207,  444,  889,  210,  879,  213,  145,\n",
      "        3579,  812])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2959 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  966,  492,  123,  117,  117,  117,  123, 1152,  371,  123, 3095,\n",
      "         221,  691,  210,  266,  123,  336, 9283,  966,  492,  123,  117,  117,\n",
      "         117,  123, 1152,  371,  123, 2483,  840,  123,  336, 9283,  966,  492,\n",
      "         123, 1409,  123, 1327,  210,  709,  123,  406, 4976,  709,  152,  184,\n",
      "        5531,  126])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1996 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  966,  492,  123,  117,  117,  117,  123, 1152,  371,  123, 2483,\n",
      "         840,  123,  336, 9283,  966,  492,  123,  117,  117,  117,  123, 1152,\n",
      "         371,  123, 3095,  221,  691,  210,  266,  123,  336, 9283,  152,  184,\n",
      "        3875,  126,  128, 1152,  371,  115, 2483,  840, 6861, 3496,  117,  142,\n",
      "        8647,  117])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2040 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  152,  184, 5349,  126,  128,  408, 1177,  115,  264,  966,  872,\n",
      "         207,  333,  105, 1870,  105,  212,  105,  363,  105,  221,  407,  213,\n",
      "        7573,  117, 3496,  117,  142,  613,  117,  345,  111,  200,  112,  244,\n",
      "         207,  376,  117,  390,  115,  260,  425, 8625,  249,  145,  870,  507,\n",
      "         117,  207])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2148 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1486,   241,  3031,   115,   207,  6861,  3427,   804,   325,\n",
      "         3897,   189,   145,  2592,  3468,   210,  1515,   658,   115,   212,\n",
      "         1425,  3468,   311,   219, 28610,   174,   211,  4696,   207,   325,\n",
      "          117,  1515,   266,   123,   117,   117,   117,   123,  1621,  2843,\n",
      "          123,  1263,   109,  2191, 12498,   123,   569, 18372,   475,   109])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2505 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  7071,   126,   128,  1263,   109,  2191, 12498,\n",
      "          115,   569, 18372,   475,   109,  7640,  1268,   210,  1053,   728,\n",
      "          343,   797,  3031,   244,   560,   115,   145,  1119,   223,   227,\n",
      "         2416,   217,  7640,  1268,   253,   115,   218,   207,  1424,   736,\n",
      "          210,  1701,  1134,   115,   145,  4624,   240,   356,   851,   145])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3842 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   222,   767,   210,  3288,   864,   327,  1662,  1102,   210,\n",
      "         6861,   118,  6861,  1102,   210,  3616,  2783,   189,   115,   521,\n",
      "          117,   115,   207,   602,   246,  1744,   301,  1785,   218,   212,\n",
      "          642,   222,   207,  2259,   210,  3324,   156,   117, 16798,  7312,\n",
      "         6023,   210, 11193,   219,   188, 16684,  5697, 11385,  3358,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4549 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   512,   213,   226,   248,   224,   219,  4191,  4474,   460,\n",
      "          210,  1870,   215,   363,   211,  4750,   207,   427,   210,   231,\n",
      "         5212,   115,   215,   211,  3427,   301, 10868,   616,   238,   145,\n",
      "         2381,   115,   215,   211,   378, 16324,   145,  2381,   117,   105,\n",
      "          447,   117,   199,   126,   113,   160,  2837,   128,   126,   113])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3921 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  833,  115, 3115,  115,  212,  231, 1162,  227, 1078,  950,  117,\n",
      "         202,  126,  113,  113,  113,  113,  204,  128,  207, 5282,  190, 3297,\n",
      "         207,  184, 2704,  189,  145, 1233, 3272,  210,  206,  117,  422,  108,\n",
      "         211, 1294,  207,  828,  210, 3130,  316,  117,  248,  613,  117,  345,\n",
      "         111,  199])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3922 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2701,  889,  221,  324,  213,  207,  856,  759,  115,  212,  126,\n",
      "         113,  113,  113,  113,  258,  128,  207,  650,  211,  505, 9802, 6603,\n",
      "         115,  521,  117,  213,  216,  759,  263,  227, 6869, 2821, 1522,  110,\n",
      "         163, 2053,  211, 2236,  351,  207,  470,  117,  126,  113,  160, 2967,\n",
      "         128,  207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4255 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2791,   126,   113,   160,  5045,   128,  1565,  3754,   217,\n",
      "         1152,   371,   115,  3018,  3614,   216,   330,   292,  8883,   126,\n",
      "          113,   113,   113,  8928,   128,  4027,   222,   229,  2821,  1522,\n",
      "          110,   163,  1165,  2390,  1343,  2305,   688,  1701,   828,   314,\n",
      "          995,   145,   326,   225, 13841,   173,   186,   115,   212,   330])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3828 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 9341,  300, 7390,  117, 2821, 1522,  110,  163, 4530,  216, 1073,\n",
      "         246, 2358,  782,  212,  207, 1398,  246,  227, 5375,  174,  212,  246,\n",
      "        2190, 9788,  117,  279,  126,  113,  160, 5042,  128,  126,  113,  113,\n",
      "        4448,  128,  126,  113,  113,  113,  113,  422,  128,  213,  775,  115,\n",
      "        2821, 1522])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3442 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   233,   246,  1156,   211,   606,   276,   117,   225,   267,\n",
      "          211,  2821,  1522,   110,   163,   231,  1328,   189,   211,  1565,\n",
      "          110,   163,  6378,   115,   207,   240,  1287,   216,   207,  2231,\n",
      "         1565,  1250,   238, 13841,   173,   186,   246,  1073,  2358,   782,\n",
      "          696,  9788,   115,   212,   246,  2335,   217,   207,   592,   210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4087 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  260,  414,  210,  683,  294,  211,  142,  613,  117,  345,  111,\n",
      "         202,  183,  112,  117,  422,  532,  614, 2821, 1522,  110,  163, 1497,\n",
      "         217,  126,  113,  113, 3805,  128, 1516,  211,  615,  207,  729,  240,\n",
      "         110,  163,  308, 1848, 1152,  371,  117,  433,  126,  113,  113,  113,\n",
      "         113,  556])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3896 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2335,  213,  460,  115,  105,  245, 3217, 6241, 5839,  216,  244,\n",
      "        1752,  211,  207,  881,  240,  110,  163, 1045,  117,  217, 1363,  115,\n",
      "         207,  461,  331,  145, 1480, 8578,  221,  237, 1909,  223, 1309, 1956,\n",
      "         211,  207,  881,  240,  110,  163, 1045,  117,  264,  166,  117, 7465,\n",
      "         115, 3963])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4132 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  446, 2658,  207, 5282,  190,  405,  115,  221,  512,  213,  207,\n",
      "        1119,  115,  217,  207,  149,  195, 9004, 3959,  117,  532, 2283,  216,\n",
      "         207, 4099, 2195,  218, 1565,  110,  163, 6378,  116,  216,  300,  263,\n",
      "         255, 1946, 2390, 2305,  907,  626, 1138,  115,  263, 2950,  207, 1119,\n",
      "         212,  697])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3233 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   613,   117,   345,   111,   199,   112,   111,  1824,   112,\n",
      "          198,   183,   117,   146,   117,   498,  2821,  1522,   110,   163,\n",
      "         2042,   216,   207,  3959,   105, 11298,   190,   211,   207,  8281,\n",
      "          105,   223,  1758,  1950,   221,   207,  3959, 11298,   190,   211,\n",
      "          239,  1215,  6413,   115,   229,   223,   213, 14842,   189,  3004])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4325 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113,  160, 4438,  128,  532, 2214,  314, 4357,  216, 2821,\n",
      "        1522,  110,  163, 8561,  223, 1367,  116,  216, 7573,  117, 3496,  117,\n",
      "         142,  613,  117,  345,  111,  200,  112, 1379,  216,  207,  430,  212,\n",
      "         207,  832, 3580,  257,  219, 2381,  189,  225,  267,  211,  207,  382,\n",
      "        2955,  215])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3651 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2381,  117,  105,  554,  372,  216, 2069,  246, 3071,  218, 1572,\n",
      "        7573,  117,  325, 3792,  211, 3102, 3504,  212, 3264,  225,  145,  966,\n",
      "        3879,  115,  207, 1412, 5415,  235,  145,  683,  246, 2577,  212, 1192,\n",
      "         218,  105,  217,  220,  683,  210, 1063,  117,  200,  115,  105,  229,\n",
      "         246,  207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 516, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3159 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   225,   275,  2333,  3629,   117,   321,  2561,   404,   117,\n",
      "         8371,   189,   210,  2381,   110,   163,   907,   212,  3965,   215,\n",
      "         2101,   487,  2247,   216, 24865, 20659,   983,   280,   222,   324,\n",
      "         1083,   211,  3412,   115,   217,   504,   210,   226,  1641,   115,\n",
      "         1459,   212,  2715,   300,   263,   284,   324,   907,   244,   227])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 498, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4606 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1471,   189,   115,   521,   117,   115,  6961,   165,   117,\n",
      "          163,   117,  7729,   115,  8279,   115,  1299,   156,   117,   149,\n",
      "          174,   117,   199,   174,  5143,   115,  2377,   163,   117,   756,\n",
      "          117, 18871,   111,  1776,   112,   111,   105,   152,   184,  8343,\n",
      "          126,   128,   207,  2030,   210,  7640,  1268,   216,   207,  2652])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 442, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4215 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   264,   166,   117,  7774,   115,  5366,  7573,   117,  6553,\n",
      "          115,  1048,   158,   117,   167,   117,   199,   174,  6550,   111,\n",
      "         7136,   112,   120,   264,   166,   117,   149,   171,   191, 25416,\n",
      "         1129,   535,   117,   115,   774,  7573,   117,   199,   174,  9266,\n",
      "          115,  3923,   158,   117,   167,   117,   199,   174,  9109,   111])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 556, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4756 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   113,   113,   743,   128,  1113,   126,\n",
      "          113,   160, 11211,   128,   532,  2283,   207,   881,   240,  1758,\n",
      "         2031,   239,  1045,   213,  3429,   207,   450,   396,   759,   211,\n",
      "         2434,  1605,   211,   207,   995,   210,   207,   856,   759,   117,\n",
      "          532,   323,  2283,   119,   111,   198,   112,   330,   223,   230])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 554, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4978 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2821, 1522,  110,  163, 5421,  182, 2305, 2960,  235,  117, 2821,\n",
      "        1522,  110,  163, 2238, 8262, 6368,  211, 1596,  216,  230,  480, 5421,\n",
      "         182, 2305,  430,  297, 1495,  802,  207, 5421,  182, 2305,  907, 1670,\n",
      "         218, 2821, 1522,  110,  163,  117,  126,  113,  160, 9063,  128,  207,\n",
      "        3629,  210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 508, 210, 508, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1370 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   160, 15508,   128,   153,   499,   216,   207,\n",
      "         7770,  1375,   273,   307,  7168,   211,  7115,   207, 10723,   174,\n",
      "         2238,   212,   384,  1540,  4543,   225,   373,   207,   881,   240,\n",
      "          117,   207,  4533,   550,   153,   785,   246,   289,   210, 19681,\n",
      "          189,   117,   934,  2821,  1522,   110,   163,   280,   207,  1151])\n",
      "Original length: 2140 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,   113,  5155,   128,   126,   113,  4587,\n",
      "          128,  2356, 14399,  4871,   175,   115,   865,   117,   226,   256,\n",
      "          223,   237,   615,   115,   256,   230,   117,   145,  3767,   171,\n",
      "        21475,   115,   212,   145,  1687,   116,   615,   115,   256,   230,\n",
      "          117,   145,  3767,   171,  4768,  4214,   115,   238,   237,  1434])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4683 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1677,   207,   276,   211,  3148,  1434,   210,   207,  2596,\n",
      "          242,   207,   232,   117, 10304,   171,  1637,   211,   247,   207,\n",
      "          908,  6672,   115,  3187,   216,   207,  5379, 26454,   252,   126,\n",
      "          113,  4945,   128,   270,   434,   242,   126,   113,   113,   113,\n",
      "          200,   128,   207,   232,   212,   207,   266,   115,   310,   207])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4102 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   111,   147,   112,  1438,   207,  5379,   110,   908,  7189,\n",
      "        10304,   171,   211,  3068,   207,  3529,  1142,   210,   107,   198,\n",
      "          238,   207,  1844,  6766,   211,   260,  1186,  1390,   213,   207,\n",
      "         9725,   189,   110,  3354,   395,   211,  1859,  3939,   222,   207,\n",
      "          232,   117,   152,   184,  1570,   126,   128,   237,  1434,   908])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5126 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   390,   115,   213,   207,  1623,   211,   207,  1249,   115,\n",
      "         1249,   230,   117,   202,  3704,   115,   126,   113,   113,   113,\n",
      "          258,   128,   310,   386,   227,   308,   115,   237,  1623,  1249,\n",
      "          216, 10304,   171,   481,  3972,   211,   549,   211,   241,  1415,\n",
      "         1186,  1636,   213,   207,  9725,   189,   110,  3354,   395,   115])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1965 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1415, 1186, 1830,  189,  213,  207, 9725,  189,  110, 3354,  395,\n",
      "         211,  219,  493,  211,  207, 9725,  189,  217,  239,  410,  236,  220,\n",
      "         259,  115,  419,  221,  378,  488,  218,  207,  232,  117,  296,  115,\n",
      "        1249,  230,  117,  202,  210,  207, 9725,  189, 2726,  145, 1249,  213,\n",
      "         229,  207])\n",
      "Original length: 2882 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1734, 23320,  2121,   111,   198,   171,   112,   126,   128,\n",
      "          111,   198,   171,   112,  2121,   111,   198,   172,   112,   126,\n",
      "          128,   111,   198,   172,   112,  2121,   111,   198,   173,   112,\n",
      "          126,   128,   111,   198,   173,   112,  3427,  1016,   142,   204,\n",
      "          100,   355,   210,   616,  3899,   100,  2015,  2242,   616,  3899])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 342, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2112 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  351,  207, 2714,  210,  207, 3569,  218, 4462,  358,  212,  207,\n",
      "        1230,  355,  215, 1019,  210,  207, 6640, 1405, 3569,  117,  330,  244,\n",
      "        1338, 2332,  358,  115,  228,  221, 3738, 3996,  115,  218,  229,  145,\n",
      "         616, 3569,  356,  219, 1405,  212,  407,  117,  661,  119, 5554, 6671,\n",
      "         109, 2109])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 342, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4126 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   153,   117,   513,   210,   889,   207,   444,   889,   115,\n",
      "          221,   763,   213,   207,  4201,   881,   110,   163,  2122,   308,\n",
      "          211,   226,   240,   115,   244,   221,   587,   119,  8910, 10222,\n",
      "         1650,  1175,   115,   521,   117,   115,   212, 19335,   184,   190,\n",
      "          104,   469,  4310,   213,   207,  1416,   210,   126,   113,   113])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 342, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4284 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   967,   207,  1503,  6036,   246,   145,  6583,   216,  3993,\n",
      "         4473,  1383,   116,   218,   116,  4473,  1383,  1081,   235,   210,\n",
      "         8910, 10222,  1959,   473,   213,  1344,   218,   145,  1398,  8910,\n",
      "        10222,   533,   212, 19335,   184,   190,   104,  5924,   117,   213,\n",
      "          463,  1138,   115,  8910, 10222,  6047, 19335,   184,   190,   104])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 342, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5209 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 24590,  1485,   223,   207,  4797,   991,   215,   355,   210,\n",
      "         1787,   359,   115,   260,   350,   355,  1745,   145,   350,   388,\n",
      "          210, 24590,  1485,   117,   221,   261,   219,  3124,   115,  1073,\n",
      "         8910, 10222,   110,   163,   696, 19335,   184,   190,   104,   110,\n",
      "          163,   811,   223,  3313,  1367,   117,   213,   308,   211,  1804])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 342, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5439 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5979,   480,   326,   210, 24590,  1485,  2431,   115,   584,\n",
      "          637,   223,   885,   259,   211, 21135,   289,   110,   163,   384,\n",
      "          277,   117,   110,   111,   368,  1631,   110,   163,   165,   117,\n",
      "          534,   237,   184,   117,   115,  2559,   115,   165,   117,   616,\n",
      "         3899,   325,   115,  1871,   117,   211,   142,   203,   115,   160])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 342, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4422 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 24590,  1485,  3316,   373,   270,  4075,   213,   207,   469,\n",
      "          117,   335,  1637,  1152,   371,   213,   332,   222,   207,  1030,\n",
      "          216,   335,   481,   227,   219,   545,  1311,   552,   207,   914,\n",
      "        24590,  1485,   263,  1402,   373,   335,  2209,   270,  3183,   117,\n",
      "          207,   240,   210,   126,   113,  4579,   128,   615,  1653,   216])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 342, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1262 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1126,   115,   283,  4162,  1322,   351,   435, 24590,  1485,\n",
      "          111,   142,  6525,  2837,   117,   199,   112,   115,   949,   217,\n",
      "         1035,   826,   111,   142,  6525,  2837,   117,   200,   112,   115,\n",
      "          212,  1322,   238,  8437,  6750,  1346,   111,  5541,   117,   112,\n",
      "          117,   203,   126,   113,   113,   113,   113,   442,   128,  1590])\n",
      "Original length: 1985 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  152,  184, 4332,  126,  128, 2078,  644,  115, 3049, 2796,  211,\n",
      "        2443,  217,  145,  388,  210,  105, 3049, 2796,  105,  242,  207, 6908,\n",
      "        3670,  325,  115,  145,  595,  311, 1596,  216,  111,  198,  112,  207,\n",
      "         588,  280,  145, 3049,  215, 3312,  513,  210,  482,  565,  275,  639,\n",
      "         424,  215])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3210 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   372,   145,   588,   249,   310, 16041,   252,   145,   388,\n",
      "          210,  3638,   658,   218, 20135,   235,   233,   211,   207,  1132,\n",
      "          210,  2137,  2274,   115,   145,   595,   311,  2587,   307,   216,\n",
      "          207,  2359,  2022,   306,   292,   227,  2423,  4575,   211,  1232,\n",
      "          289,   211,  2283,   225,   480,  3680,   216,   335,   632,   207])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2455 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  6897,   126,   128, 10820,   383,   210,  1969,\n",
      "          115,  3520,  8082,   388, 10820,   383,   213,  1023,  1354,   649,\n",
      "          223,   512,   221,   145,  1179,   212,   627,   371,  2705,   213,\n",
      "          237,   347,  7412,   235,   207,   595,   110,   163,   470,   294,\n",
      "          211,   207,   409,   210,   880,   212,  2061,   117,   207,   388])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2788 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1023,  266,  123, 1354,  872,  123, 6971,  235, 1275,  123,  336,\n",
      "        9283,  152,  184, 7913,  126,  128, 1354,  872,  115, 6971,  235, 1275,\n",
      "         511,  774,  165,  117,  163,  117,  147,  117,  163,  117,  142, 5552,\n",
      "         111,  153,  112,  783,  216,  237,  105, 1115,  217,  496,  105,  215,\n",
      "         105, 1115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2401 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   952,   266,   123,   438,  3766,   123,  3704,   123,   336,\n",
      "         9283,  1023,   266,   123,   117,   117,   117,   123,  1372,   189,\n",
      "          123,   213, 14878,   818,   123,   336,  9283,  1023,   266,   123,\n",
      "         1354,   872,   123,   336,  9283,  1023,   266,   123,  1354,   872,\n",
      "          123,  6971,   235,  1275,   123,   336,  9283,   152,   184,  9017])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2279 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1023,   266,   123,  1354,   872,   123,  6971,   235,  1275,\n",
      "          123,   336,  9283,   152,   184,  9942,   126,   128,  1354,   872,\n",
      "          115,  6971,   235,  1275,  8911,   235,   207, 10804,   317,  1283,\n",
      "          644,   217,  9790,  3946,   212,  1859,   235,  4139,  3729, 20717,\n",
      "        23199,   363,   210,   354,   265,  1023,   266,   115,   207,   354])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2820 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   105,  3427,  1016,   105,   236,   346,   266,   213,  6401,\n",
      "         1379,  1683,   216,   207,   588,  2104,   213,  1275,   215,  1748,\n",
      "          217,   207,   504,   210,  9601,  5934,   207,   616,   210,   145,\n",
      "         2381,   115,   228,   221, 24590,  1485,   215,  4611,   383,   115,\n",
      "          212,   228,  1275,  3580,   207,   595,   218,   744,   210,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2357 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113, 7548,  128, 2231,  126,  113,  113,  199,  128,  528,\n",
      "         212,  308,  226,  347,  223,  373,  207,  240,  222,  588,  110,  163,\n",
      "        3883,  117,  296,  115,  207,  240,  261, 1262,  207,  588,  110,  163,\n",
      "        4583,  867,  217, 1680, 1152,  371,  948,  470, 5179,  373,  245,  339,\n",
      "         115,  751])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3933 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   959,   246,  2054,   236,   207,   165,   117,   163,\n",
      "          117,  1023,   212,  3245,   648,   111,   105, 14296,  1386,   105,\n",
      "          112,   222,   609,   199,   115,  1755,   117,   238,   463,   509,\n",
      "          115,  1697,   115,   598,   207,  1655,   210,   207,   110,  5648,\n",
      "         1023,   222,   732,   282,   115,   722,   115,   330,   292,   230])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4132 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   212,   820,   465,   227,  1059,   216,   253,   207,  5059,\n",
      "          252,   529,  1063, 12418,   223,  1671,   115,   207,  6450,   223,\n",
      "          216,   606,   276,   247,  4477,   674,   327,   617,   117,   213,\n",
      "          220,   377,   115, 29289,   249, 21861, 10353,   174,   216,  1416,\n",
      "        14554,   176,  2785, 10786,   110,   163,  2110,  2381,   115,   991])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4126 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 10786,   110,   163,   105,  1261,   212,  2274,   105,  1005,\n",
      "         1475,   221,   190,   183,   150,   116,  4600,  2274,   222,  1298,\n",
      "         1416, 14554,   176,  2439,   189,   117,   207,  1132,   210,   392,\n",
      "         2359,  3690,   216,   324,  4593,   292,  1106,   301,  2710,   577,\n",
      "          268,  2427,   370,   116,  1416, 14554,   176,  4593,   210,  2427])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3859 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   416,   128, 10786,   728,  1736,   252,\n",
      "          211,  1416, 14554,   176,   110,   163,  1110,  1350,   217, 20473,\n",
      "        19432,  1485,   213,   239,   639,  1110,  7082,   115,   225,  1276,\n",
      "          699,   211,  1636,   802,  1416, 14554,   176,   110,   163,  1596,\n",
      "         8901,  4593,   236,   207,  3406,   210, 10408,   212, 12734,  8887])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4653 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   198,   117,   207,   815,  2657,   218,   207,   245,   339,\n",
      "          115,   751,   906,   232,  7296,   211,   241,   470,  1833,   218,\n",
      "          216,   254,   212,   223,   227,   488,   211,   324,   470,   212,\n",
      "         1540,  1810,   213,   207, 18408,  3155,   347,   213,   207, 18408,\n",
      "         3155,   347,   115,   820,   110,  4208,   213,   302,   115,  8834])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4001 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   276,  1280,   763,   216,   207,   470,   213,   207,\n",
      "        18408,  3155,   347,   292,   950,   213,   216,  1207,   210,   105,\n",
      "          220,   212,   241,   470,   115,  3161,   115,   212,  3244,   210,\n",
      "          347,   115,   105,  1528, 12143,  2759,   145,  5090,  1207,   210,\n",
      "          105,   470,   115,  3161,   115,   212,  3244,   210,   347,   105])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4116 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   508,   128,   820,  4293,   216,   588,\n",
      "         2863,   142,   199,   210,   207, 10104,   325,   218,  5508,  7370,\n",
      "          215,  7137,   211, 24369,   207,   327,   217,   105,  4717,  8294,\n",
      "          176,  6206,  4593,   117,   105,   342,   165,   117,   163,   117,\n",
      "          147,   117,   142,   199,   117,  3093,   115,   820,  3194,   216])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4502 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3501, 11565,   535,   117,   166,   117,   354,   265,   115,\n",
      "         5749,   165,   117,   163,   117,  5631,   115,   205,   156,   117,\n",
      "          149,   174,   117,   199,   174,  5351,   115,  1371,   163,   117,\n",
      "          756,   117, 14298,   111,  4456,   112,   111,   483,   240,   115,\n",
      "         8911,   235,  1104,  1178,   212,   408, 10756,   115,   489,   301])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4773 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1395, 13408,   363,   591,   210,   207,   683,   215,   210,\n",
      "         1395, 13408,  1275,   280,   798,   218,   207,   683,   117, 10926,\n",
      "          189,  6943,  1610,   117,   166,   117, 27373, 17644,   116,   159,\n",
      "          116,  8329,   115,   521,   117,   115,  6310,   165,   117,   163,\n",
      "          117,  5457,   115,  7729,   115,   541,   156,   117,   149,   174])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 516, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4682 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  4934,   126,   128,   220,   322,   443,   115,\n",
      "          222,   215,   213,   445,   225,   220,   737,   215,   410,   100,\n",
      "         3214,   213,  2525,   220,   100,  3049,   215,  3312,  1248,   210,\n",
      "          482,   115,   215,  3049,   215,  3312,  1555,   210,   482,   115,\n",
      "          229,   100, 12271,   189,   207,   817,   115,  2063,   115,   126])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 498, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3545 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  225,  374,  211,  768,  802,  207, 5095,  159,  189, 7142,  185,\n",
      "        2439,  212,  207, 4255, 5581,  233,  115, 1073,  207, 4892,  860,  189,\n",
      "         210,  820,  696,  207,  643,  210,  226,  240,  356, 2808,  185, 3176,\n",
      "         117,  207, 1534,  216,  588, 3673,  189,  211, 1636,  503, 2008,  212,\n",
      "         244,  145])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 442, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4199 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   105,   145,  3315,   210,   384,   511,   218,   207, 11831,\n",
      "          356,   219,   280,   307,   210,   207,  1458,  1023,   115,   237,\n",
      "         8875,   252,   332,   215,   617,   210,   207,  1458,  1023,   115,\n",
      "          215,   241,   277,   242,   207,  1023,   213,   145,   612,  2812,\n",
      "         1805,   210,   207,   354,   265,   117,   105,   422, 19405,   116])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 556, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4780 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  7183,   115,  7239,   116,   896,   117,   340,   233,   223,\n",
      "          213,   207,   691,   236,  1107,   115,   343,   890,   820,  1088,\n",
      "          211,   388,   216, 10786,   110,   163,   221, 17692,  8840,   111,\n",
      "          162,   112,   424,   115,   390, 23747,   174,   115,  8400,   388,\n",
      "          198,   210,   207,   110,  5648,  1023,   117,   221, 24656,   348])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 554, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4748 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   220,  3554,   210,   207, 11831,   115,   223,   216,   213,\n",
      "          229,   207,   496,   261,  2008,   373,   207,  1655,   210,   207,\n",
      "          425,   210,   207,  1023,   117,   105,   774,   165,   117,   163,\n",
      "          117,   147,   117,   142,  5552,   126,   113,   113,  1144,   128,\n",
      "          111,   153,   112,   120,   321, 10742,  2202, 10964,   117,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 508, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4557 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3746,   216,   223,  2872,   280,   215,   601,   126,   113,\n",
      "          113,  1123,   128,   217,   355,   213,   207,  3746,   212,   223,\n",
      "          227,   145,  7969,   234,   215,  4672,   210,  2525,  2784,   217,\n",
      "          370,   116,  6971,   235,   355,   558,  5552,   111,   150,   112,\n",
      "          111,   199,   112,   117, 10786,  2680,   213,   207,  8552,   347])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 509, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4038 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  4370,   221,   220,  3704,   211,  1240,   207,   221, 17692,\n",
      "         8840,   111,   162,   112,  9832,  2888,   115,   237,  2530,   210,\n",
      "          207,  2965,  6971,   235,  2439,   189,   115,   292,   280,   281,\n",
      "          207,   354,   265,   115,   820,   110,  1151,   223,  2661,   211,\n",
      "         1688,   211,   207,   680,   210,  1023,  1354,   126,   113,   113])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 345, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3590 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1254,  2783,   189,   217,   355,   213,  6197, 13461,   189,\n",
      "          211,  1915,   207,  4289,   210,  7177,   117,   442,   820,  5214,\n",
      "          216,   228,  9832,   223,   407,   213,  2965,  6971,   235,  4593,\n",
      "         5862,   356, 10899,  2490,  1915,   145,   683,   210,   774,   165,\n",
      "          117,   163,   117,   147,   117,   142,  5552,   111,   150,   112])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 415, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4102 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   266,   117,   213,   308,   211,  2859,   145,   388,   115,\n",
      "          820,   297,  2575,   247,   211,  2394,   460,   216,   207,  2965,\n",
      "        12479,  1412,   246,   227,  1377,   115,   967,   231,  1877,   210,\n",
      "          207,   388,   117,  3135,   179,   115,   521,   117,   166,   117,\n",
      "          288, 11815,  8355,   115,   521,   117,   115,  9927,   150,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 846, 210, 846, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 562 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   254,   212,  1003,   210,   446,   119,   371,   213,   295,\n",
      "          225,   207,   308,   210,   728,   254,   212,   835,  5602,   301,\n",
      "         1438, 14215,   115,   233,   223,   457,  1677,   119,   111,   198,\n",
      "          112,   216,   226,   347,   219,   115,   212,   207,   376,   457,\n",
      "          223,   115,  1629,   212,  8935,   715,   238,   207,  1594,  4523])\n",
      "Original length: 787 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,  2231,   226,   223,   237,  5859,   347,   213,   229,\n",
      "          595,  3503,   216,   207,   894, 30228,   252,   213,   207,  3237,\n",
      "          174,   235,   736,   217,   145,  7514,   235,   726,   115,   211,\n",
      "          690,   216,   588,  3417, 28692,   189,   115,   521,   117,   642,\n",
      "          207, 14485,  3237,   117,   213,   635,   210,   239,  1372,   115])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4190 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  4890,   179,  3704,   275,   528,   105,   216,   207,  7682,\n",
      "          571,   213,   245,   126,   113,   199,   128,   210,  1094,   217,\n",
      "          207,   665,   116,  7514,   235,   726,   292,   480,   212,   370,\n",
      "          116, 22908,   117,   105,   111,  2945,   117,  5483,   117,   115,\n",
      "         8427,   117,   148,   236,   198,   117,   112,   936,   373,   207])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4597 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1743,   223,   227,  2137,   213,   817,   115,   105,   444,\n",
      "         5630,  2044,   245,  4000,   306,  1038,  1002,   215,  2196,   115,\n",
      "          105,   221,  5152,   211,   105,  2137, 10284,   117,   105, 14346,\n",
      "         4738, 11001,   115,  7994,   165,   117,   163,   117,   236,  2273,\n",
      "          115,  2925,   163,   117,   756,   117,   236, 23663,   117,   152])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3681 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  4575,  4354,   211,  2745,   275,   528,   216,   330,   246,\n",
      "          230, 10520,   117,   152,   184,  4214,   126,   128,   207,  1909,\n",
      "          126,   113,   258,   128,   311,  3412,   207,   358,   218,   229,\n",
      "          300,  2318,   275,  2314,   115,   212,   228,   358,   311,  1790,\n",
      "          236,   684,   289,   210,   207, 18467,   190,  1674,   210,  5630])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 625 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   308,   116,   835,  2618,   116,   201,   634,   212,   936,\n",
      "          226,   200,   188,   174,   414,   210,   463,   115,   634,   115,\n",
      "          306,   126,   113,   337,   128,   731,   210,   207,   867,   213,\n",
      "        16587,   175,   210,   595,  2838, 19203, 20886,  4107,   469,   111,\n",
      "          105,  2838, 19203,   105,   112,   211,  2990,   207,  1909,  1743])\n",
      "Original length: 1224 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  7860,   128, 14899,   149,   117,  2838,\n",
      "          115,   881,   865,   119,  8869,  2278,  6646,   189,  1409,   145,\n",
      "          371,   221,   145,   691,   210,   266,   111,   105,   154,   117,\n",
      "          157,   117,   156,   117,   105,   112,   213,  2009,   210,   207,\n",
      "         1985,  1102,   210, 13159,  4436, 12990,   189,   190,   189,   111])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3874 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2278, 6646,  189,  110,  163, 6083,  189,  211,  207,  145,  171,\n",
      "         185,  115, 3388,  216, 2278, 6646,  189,  110,  163,  818, 1219, 2488,\n",
      "         318,  210,  216, 1445,  110,  163,  473,  210, 1882, 1550,  117, 2278,\n",
      "        6646,  189,  832,  216,  207, 1318, 4255, 5581,  275, 6083,  189, 3160,\n",
      "         213,  207])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3698 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   257,   117,   260,   210,   207,   768,   238,   207,   671,\n",
      "         1661,   218,  2278,  6646,   189,   221,   460,   210,   145,  3854,\n",
      "         4678,   222,   536,  6295,   117,   200,   126,   113,   113,   203,\n",
      "          128,   420,   115,   207,   671,   223,   115,   236,  1024,   115,\n",
      "        11945,  6774,   460,   210,   145,  3854,   117,   201,   126,   113])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3972 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   205,   128,  2278,  6646,   189,  2042,\n",
      "          115,   390,   115,   216,  3304,   173,   246,  5706,   225,   353,\n",
      "          268,  3036,  2062,  1840,   117,  2388,   115,   300,  1069,   189,\n",
      "          115,   207,   145,   171,   185,  1092,  2145,   145,  5052,  3409,\n",
      "        21215,   211, 15776,  3304,   173,   211,   711,   239,  1110,  1350])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3498 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  616,  117,  105, 1609, 2065,  338,  115,  521,  117,  166,  117,\n",
      "        1824,  117, 3288,  864,  213,  189,  190,  117,  115, 8797,  150,  117,\n",
      "         199,  174, 5635,  115, 5908,  116, 1001,  111,  202,  475,  682,  117,\n",
      "        1474,  112,  117,  213, 1609, 2065,  338,  115,  532, 1653,  145,  388,\n",
      "         210, 3854])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4220 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  242,  226, 1972,  115,  237, 1355,  223,  885,  253,  233, 1505,\n",
      "         301, 3243,  207,  105,  620,  115, 2194,  115,  212, 8283,  210,  145,\n",
      "        6124,  105,  213, 5169,  239, 1113,  117,  447,  117,  236, 9709,  117,\n",
      "         213, 2015, 5672,  115,  207,  240, 2255,  216,  145, 2102,  222, 2796,\n",
      "         497,  211])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1354 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   246,  9788,   115,   310,  2278,  6646,   189,  2042,   216,\n",
      "          233,   305,   247,   255,  2680,   242,   207,  1887,   217,   768,\n",
      "          280,   218, 26197,   189,   117,   152,   184,  4019,   126,   128,\n",
      "          242,   207, 26197,  1887,   115,  9788,   460,   223,  2335,   307,\n",
      "          253,   207, 17115,  9262,   218,   145,  9074,  1303,   210,   207])\n",
      "Original length: 1759 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  2938,   128, 11802,   115,   881,   865,\n",
      "          119,   226,   615,  1379,  1143,   211,  1025,   331,   207,   264,\n",
      "          116,   347,  3459,  3497,  6452,   189,   145,  4268,   212,   145,\n",
      "         1018,   469,   238,   832,   522,  5859,   116,   266,  2581,  1318,\n",
      "          238,   207,  4268,   110,   163,  8270,   757,   306,   207,   469])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4276 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  532,  753,  225,  207,  483,  240,  216,  217,  504,  210,  207,\n",
      "         264,  116,  347, 3459, 3497,  115,  207,  264,  755,  207, 4258,  211,\n",
      "        2898,  207, 3527,  216, 2629,  207,  515, 7012,  225,  267,  211,  207,\n",
      "         579,  210, 9706,  235, 3601,  213, 2565,  179,  171,  117,  532, 6710,\n",
      "         115,  126])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4464 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1864,   473,   117,  9601,  5972,  5198,   117,   160,   186,\n",
      "          201,   115,   282,   117,  2064,  2364,   350,   871, 10660,   547,\n",
      "          247,   601,  1302,   216,   115,  1486,   207,  4258,   110,   163,\n",
      "          115,  2834,   960,  1801,  2438,   211,  1150,   941,   210,   739,\n",
      "          238,   207,   515,   117,   447,   117,   160,   279,   117,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4397 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207, 11286,   235,   257,   117,   105, 11938, 20564,   171,\n",
      "        11624,   117, 10964,   117,   535,   117,   166,   117, 16773,   178,\n",
      "         2582,  1610,   117,   115,  6333,   165,   117,   163,   117,  6277,\n",
      "          115,  8799,   115,  1076,   156,   117,   149,   174,   117,   199,\n",
      "          174,  7941,   115,  2220,   163,   117,   756,   117, 15516,   111])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5218 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  2138,   126,   128,   522,   986,   115,   372,\n",
      "         1455,   264,   116,   347,  3459,   217, 10660,   547,   115,   311,\n",
      "          105,  1643,   145,  2699,   210,   434,  2499,   577,   268,  1067,\n",
      "          223,   740,   211,  1025,   207,   384,   658,   210,   207,  4268,\n",
      "          110,   163,   347,   242,   264,   266,   117,   105,   447,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4378 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  146,  117,  207,  350,  871,  325,  212,  207, 4258, 3527,  207,\n",
      "        4258, 2042,  216,  239,  872, 2834, 1446,  235,  207,  595,  238,  207,\n",
      "         327,  217, 3228, 3601,  281,  239, 8206,  292,  755,  218,  207,  325,\n",
      "         212,  497,  697,  561,  126,  113,  113,  441,  128,  218,  207,  350,\n",
      "         871, 1634])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4846 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   199,   117,  2393,   428,   119,   434,   211,  7226,  1016,\n",
      "          117,   532,   323,  2283,   216,   207,   325,   212,   207,  1634,\n",
      "          110,   163,   697,  1204,   207,   450,  2562,   210,   434,   216,\n",
      "        10660,   547,   311,  1596,   242,  8284,   117,   207,   599,  1395,\n",
      "        13408,  2984,   216,   207,   595,  3503,  1132,   238,   207,  4258])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3473 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   595,   832,   213,   239,   396,   759,   216,   207,\n",
      "          515,  2863,   207, 10104,   325,   218,  3007,   301,  2041,   212,\n",
      "         3331,  5508,   547,   213,  1338,   350,   871, 10660,   547,   115,\n",
      "          212,   218, 25354,   239,  5508,   195,   643,   218,  1283,  1857,\n",
      "          116,  1050,   446,   212,   218,  7137,   211,  4987,   239,  1974])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4713 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1016,  117,  447,  117,  111, 5377,  212,  888, 2871, 2023, 1396,\n",
      "         112,  117,  450,  115,  233,  311,  126,  113, 3413,  128, 1596,  216,\n",
      "         239,  818,  246,  105, 5557, 6056,  218,  207,  264, 1092,  117,  105,\n",
      "         447,  117,  111, 5377,  212,  888, 2871, 2023, 1396,  112,  117,  353,\n",
      "        3531,  115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4784 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  389,  712,  755,  218,  207,  264,  211,  500,  228,  347,  117,\n",
      "        1590,  296,  273,  213,  226, 4473, 1383,  246, 5054, 1383,  115,  145,\n",
      "         256,  229,  115,  221, 3124,  213,  332,  290,  117,  145,  117,  115,\n",
      "         348,  115, 1325, 1350,  218,  145,  264,  116, 1920,  712,  115,  207,\n",
      "         165,  174])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5022 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  165,  117,  163,  117,  236, 8326,  115,  809, 1790,  235,  207,\n",
      "        2161,  216,  207,  450, 3441, 6886, 8376,  116,  116, 1594, 2557,  116,\n",
      "         116,  246, 1733,  211,  785,  117, 5054, 1383,  212, 5041,  171, 6611,\n",
      "        1386,  188,  465,  227, 3518,  217,  207, 5696,  216, 1018,  276,  443,\n",
      "         438,  225])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4550 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  4258,   115,  9016,   216,   207,   296,  8376,   210,   207,\n",
      "         3441,  6886,   893,   217,   504,   210,   392,   872,   297,  4654,\n",
      "          222,   207,   434,   210,   392,   231, 10660,   547,   211,  3193,\n",
      "         5508,   195,  1482,   306,   207,   515,   117,   222,   615,   115,\n",
      "          390,   115,   207,   595,   249,   227,  1810,  1689,  5964,   392])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 433, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1728 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   595,  2913,   307,  4162,   212,  6010,   195,  1322,   351,\n",
      "          207,  4258,   117,   237,  2411,  1219,   219,   237,   489,  2551,\n",
      "          217,   126,   113,   113,   541,   128, 12811,  3418,  1251,  3065,\n",
      "          189,   216,   519,   213,  1106,  1395, 13408,   347,   218,  1018,\n",
      "          276,   212,   216,   725,  1569,   353,   268,   145,   105, 15959])\n",
      "Original length: 1864 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113, 18073,   128,   308,   226,   602,   223,\n",
      "          373,   207,   240,   222,   588,   110,   163,   867,   217,  1152,\n",
      "          371,   111,  3192,   117,   230,   117,  2720,   115,   678,   557,\n",
      "          498,   115,   634,   112,   212,   595,   110,   163,  1598,   111,\n",
      "         3192,   117,   230,   117,  3016,   115,   678,   557,   498,   115])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3209 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1430, 2053,  215,  944,  211, 4231,  145, 1090, 5653,  115, 6210,\n",
      "         252,  613,  115,  238,  145, 1849, 3020,  443,  223,  227, 1884,  852,\n",
      "         218,  207, 6805,  116,  515,  215,  472,  852,  218,  207, 5616, 2289,\n",
      "         117, 6149, 2221,  808,  211,  241,  207, 5653,  189,  110, 7552,  189,\n",
      "         211, 2434])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4189 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1071,   259, 10534,  7552,   189,   207,   276,   110,  1059,\n",
      "          213,   226,   256,  2044,   207,   222,   116,  4473,  1383,  1279,\n",
      "          210,   105,  1071,   116,   259,   105, 10534,  7552,   189,   117,\n",
      "         1071,   116,   259,  7552,   189,   115,   221,   207,   425,  4965,\n",
      "          115,   244,  7552,   189,   216,   244,  3691,  1394,  1797,  5691])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4055 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2262,   115,   207,   160,  4313, 11197,   249,   280,   230,\n",
      "         3569,   213,   226,  1641,   210,   239,  2598,   211,  1069,   145,\n",
      "          993,  2237,   213,   207,   327,   217,  1809,  1071,   116,   259,\n",
      "        10534,  7552,   189,   212,   249, 16119,  2236,   252,   239,   319,\n",
      "          211,  1839,   126,   113,   113,   282,   128,   239,  2524,   721])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4125 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2644,  2963,   117,   207,   160,  4313, 11197,  2329,   211,\n",
      "         8388,   226,   485,   117,   233,  1908,  1502,  6149,   216,  2644,\n",
      "        16375,   189,   297,   307,   219,   271,   222,   207,   827,   216,\n",
      "        18320,   235,   286,  2719,   238,   207,   222,   116,  1934,  2644,\n",
      "         2963,   219,   407,   307,   213,  1279,   189,   281,   207,  6149])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4226 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1579,  3117,   189,  1382, 10932,   383,   277,   115,   229,\n",
      "        11373,   195,  1907,   207,   232,   597,  3571,   211,  2977,  5947,\n",
      "         2642,   207,  1835,   405,   117,   279,   207,  1835,   907,   217,\n",
      "         1071,   116,   259,  7552,   189,   245,  2389,   145,  2078,  2941,\n",
      "          126,   113, 22431,   128,   217,   353,   459,   116,   211,   116])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3566 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  8013,   115,  8170,   111,   282,   475,   682,   117,  1154,\n",
      "          112,   115,   212,  3248,   241,   480,  4016,   213,   216,   257,\n",
      "          110,   163,  2009,   117, 12464,   166,   117, 20708,   115, 10780,\n",
      "          150,   117,   199,   174,  5111,   115,  5365,   111,   282,   475,\n",
      "          682,   117,  1464,   112,   117,   809,   115,   253,   145,   480])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3506 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   217,   677,   116, 12882,   235,   216,   223,   278,   307,\n",
      "          213,   649,   225,   207,  4487,   116,   350,   189,  1887,   115,\n",
      "          229,   261,   219,  3124,   213,  1706,   188,  2653,   688,   117,\n",
      "          353,  1014,   301,   115,   207,   286,   216, 11881,   407,   211,\n",
      "         1915,   239,   424,   246,   213,   207,   408,  4113,   115,   361])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4280 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   728,  6410, 10158,   189,   217, 10460,   212,  6077,   145,\n",
      "         1714,   452,   857,  1693,   225,   207,   595,   314,   145,   316,\n",
      "         3483,   117,   321,   221,  8202,  8208,   235,   535,   117,   166,\n",
      "          117,   221,  8202, 14820,   189,  8208,   235,  1610,   117,   115,\n",
      "         7769,   165,   117,   163,   117,  7938,   115,  8791,   116,   962])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3914 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,  5419,   166,   117,   350,   871,  3488,   477,   115,\n",
      "         4250,   165,   117,   163,   117,  8465,   115,  1047,   156,   117,\n",
      "          149,   174,   117,  3872,   115,  1100,   163,   117,   756,   117,\n",
      "         6449,   111, 11891,   112,   115,   595,  1637,   237,  2411,   217,\n",
      "          173,   235,   588,   211,  2616,   595,   225,   145, 18301,   577])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3946 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,  1043,   128,  1015,   115,   207,   240,\n",
      "         1723,   216,   207,   160,  4313, 11197,   223,  1948,   213,   239,\n",
      "         1265,   552,   111,   198,   112,  6149,   677,   116, 19101,   189,\n",
      "          222,   207,   160,  4313,   126,   113, 22726,   128, 11197,   110,\n",
      "          163,  1350,   115,   111,   199,   112,   207,   160,  4313, 11197])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4192 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,  2129,  2025,   213,  1455,   331,   145,   683,   210,\n",
      "          142,   199,   249,  1402,   117,   105,   112,   120,   321,   323,\n",
      "         3441,   116,  2348,  1798,  1175,   115,   521,   117,   166,   117,\n",
      "         1985,  2106,   109, 10556,   535,   117,   115,  7427,   150,   117,\n",
      "          199,   174, 22388,   115, 24895,   111,   202,   475,   682,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3907 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1623,   211,   207,  1068,   223,   227,  7487,   175,   117,\n",
      "          105,   112,  4620,   115,   231,  5004,  4008,   115,   228,   221,\n",
      "         6174,   193,   117,   207, 16067,   176, 30352,   117,  1871,   115,\n",
      "         6174,   193,   117, 16330,   117,  1871,   115,   212,  6174,   193,\n",
      "          117, 10534,   383,   825,   117,  1871,   115,   443,   465,   227])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4297 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  3875,   126,   128,  3750,  5508,  3064,   249,\n",
      "          584,  1877,   119,   198,   112,   207,   588,   110,   163,   599,\n",
      "         1870,   211,  2574,  5508,   195,   643,   555, 16822,   195,   215,\n",
      "        17430,   818,   115,   199,   112,   207,   588, 10175,   213,   228,\n",
      "          818,   115,   212,   200,   112,   330,  2431,   145,  3676,  6727])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 50 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 13626,   195,   149,   117, 19011,  7004,   354,   265,   483,\n",
      "          865,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 1265 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  7458,  1463,   218,  1808,   676,  8695,   117,  4409, 10700,\n",
      "          166,   117, 12900,  6576,  1111,   213,   189,   117,   535,   117,\n",
      "          115,   641,  3000,   117,  2153,   117, 13648,   189,  1433,   111,\n",
      "         3000,   117,  2153,   117,   199,   174,  6394,   117,   115,  3772,\n",
      "          117,   404,   115,   641,   112,   709,  1463,   218,  1808,   676])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 199, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2324 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5859,   109,   616,   266,   123,  2078,   644,   123, 12497,\n",
      "          109,  3427,   616,  1748,   123,   336,  9283,   152,   184,   973,\n",
      "          126,   128,  2078,   644,   115, 12497,   109,  3427,   616,  1748,\n",
      "          321,  3000,   117,  5471,   117,   109, 12758,   117,   473,   142,\n",
      "        23962,  2311,   117,  5859,   109,   616,   266,   123,   117,   117])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 200, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2862 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5859,   109,   616,   266,   123,  2078,   644,   123, 12497,\n",
      "          109,  3427,   616,  1748,   123,   336,  9283,  5859,   109,   616,\n",
      "          266,   123,  3984,  1748,   123,   616,  1748,   109,  3427,  1016,\n",
      "          123,   336,  9283,   152,   184,  2450,   126,   128,  2078,   644,\n",
      "          115, 12497,   109,  3427,   616,  1748,   221,   407,   213,  3000])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1672 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   321,  3000,   117,  5471,   117,   109, 12758,   117,   473,\n",
      "          142,  6963,  2567,   117,  5859,   109,   616,   266,   123,  2078,\n",
      "          644,   123, 12497,   109,  3427,   616,  1748,   123,   336,  9283,\n",
      "          152,   184,  4290,   126,   128,  2078,   644,   115, 12497,   109,\n",
      "         3427,   616,  1748,  4897,  3072,   215,  3312,   768,   280,   225])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2924 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5859,   109,   616,   266,   123,  3984,  1748,   123,   616,\n",
      "         1748,   109,  3427,  1016,   123,   336,  9283,   152,   184,  5531,\n",
      "          126,   128,  3984,  1748,   115,   616,  1748,   109,  3427,  1016,\n",
      "         3000,   117,  5471,   117,   109, 12758,   117,   473,   142,  3454,\n",
      "         3384,  5826,  3405,   548,   117,  5859,   109,   616,   266,   123])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1875 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   588,   110,   163,  2857,   115,   212,   207,   588,   110,\n",
      "          163,   517,   115,   982,   115,   212,   761,  4054,   117,  3000,\n",
      "          117,  5471,   117,   109, 12758,   117,   473,   142,   142,  3454,\n",
      "         4221,   115,  3792,  5939,   117,  5859,   109,   616,   266,   123,\n",
      "          117,   117,   117,   123,  1018,   872,   123,  1126,   123,   336])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3114 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  7154,   126,   128,  1018,   872,   115,  1126,\n",
      "         5427,   245,   219,  1677,   213,   207,  1345,   210,  1683,   210,\n",
      "         1591,   210,  1002,   213,   308,   211, 10417,  1149,  2581,   210,\n",
      "         3000,   117,  5471,   117,   109, 12758,   117,   473,   142,  3454,\n",
      "         3384,   149,   190,  2830,   117,   115,   207,   105,  3427,  1016])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3712 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2121,  111,  198,  171,  112,  126,  128,  111,  198,  171,  112,\n",
      "        2121,  111,  198,  172,  112,  126,  128,  111,  198,  172,  112, 2121,\n",
      "         111,  198,  173,  112,  126,  128,  111,  198,  173,  112, 3427, 1016,\n",
      "         142,  258,  100,  872,  242, 3427, 1016,  266,  100,  949,  100,  966,\n",
      "        2339,  547])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3845 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   266,   111,  5471,   117,   109, 12758,   117,   473,   115,\n",
      "          142,   142,  6963,  2567,   115,  3454,  3384,   112,   115,   218,\n",
      "         3177,   235,  3072,   212,  3441, 11563,   174,   235,   768,   213,\n",
      "         1110,  1044, 13801,   189,   559,   338,   111,  6580,   117,   473,\n",
      "          115,   142, 23962,  2311,   112,   115,   212,   218,  5690,   213])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4433 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,  6628,   128,  2791,  1421,  9578,   216,  1436,\n",
      "          460,   635,   189,   207,  1601,   210,   207,   729,   240,   117,\n",
      "          207,   298,  2238,  1152,   223,   487,   222,   207,   513,   210,\n",
      "          273,   117,  1421,   212,   207,  5590,   217,  6212, 12224,   111,\n",
      "         1824,   171,   112,   105,  1845,   213,  1338,  6557,  5981,   211])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3786 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,   775,   211,  6021,  4162,  1322,   115,   207,   729,\n",
      "          240,  1146,   145,   966,  2408,   210,   107,  3805,   217,   260,\n",
      "          683,   210,   248,  3454,  3384,   212,   217,   260,   683,   210,\n",
      "          248, 23962,  2311,   115,  5977,   218,   237,   536,   126,   113,\n",
      "          113,   113,   203,   128,   107,  3805,   558,   683,   552,  1122])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5144 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2121,  111,  200,  172,  112,  126,  128,  111,  200,  172,  112,\n",
      "         221, 3407,  174,  213,  207, 1932,  115,  248, 3454, 3384,  265,  213,\n",
      "         332,  119,  152,  184, 2450,  126,  128,  105,  221,  407,  213,  226,\n",
      "         746,  115, 3427, 1016,  224,  819,  212,  698,  220, 2093,  115, 3427,\n",
      "         215, 3399])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4360 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   113,   279,   128,  1700,   115,   152,\n",
      "          184,  2967,   126,   128,   207,  2067,  8376,   210,   248,  3454,\n",
      "         3384,   105,  4981,  3612, 26289,   211,   346,   266,  2067,   215,\n",
      "         2618, 17735,  1485,   117,   105,   111,   264,  2703,  2486,   109,\n",
      "         5206,   535,   117,   166,   117,  3638,   240,   115,  2559,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5046 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 16668,  4099,   780,   399,   115,   521,   117,   115,  2559,\n",
      "          115,   416,  3000,   117,  2153,   117,   201,   475,   236,   160,\n",
      "          117, 18917,   115,   150,   184,   117,   203,   112,   212,   323,\n",
      "          221,   237, 18574,  1656,   454,   213,   145,   847,  2387,   232,\n",
      "          111,  1808,   166,   117, 18175,  1656,   115,  2559,   115,   442])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4096 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  9717, 13071,   323,  2522,   216,   207,  6910,   663,   105,\n",
      "          249,  1750,  1026,  2347,  1162,   212,   569,   145,   915,  2039,\n",
      "          210,   259,   117,   105,  6910,  1302,  6242,  2659,  2347,  1162,\n",
      "          222,  1466,   115,   310,  1421,   110,   163,  6910,  1302,   503,\n",
      "          227,   117,   532,  2283,   216,  1436,   460,   635,   189,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3737 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   221,  1421,  1137,   189,   115, 14136,   166,   117,  4277,\n",
      "          115,  2559,   115,  1585,   343,   145,  1119,   386,   227,  1232,\n",
      "          220,  1045,   213,  1455,   207,   291,   210,   207,  2408,   117,\n",
      "          111,   321,   115,   149,   117,   151,   117,   115,  1808,   676,\n",
      "         8695,   117, 10802, 10270,   166,   117,  3638,   240,   111,  1099])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4414 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1714,  1421,  1013,   238,   239,  6910,   126,   113,   113,\n",
      "         7859,   128,   316,   117,   111,   203,   112,  2858,   210,   107,\n",
      "          199,   117,   202,  1105,   213,  2339,   547,   223,   227,   369,\n",
      "          221,   145, 11547,   117,   532,   296,   499,   152,   184,  5045,\n",
      "          126,   128,   207,   165,   173,   182,  1379,   216,   207,  1126])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4183 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   105,   404,   117,   117,   117,   117,   207,   804,  1243,\n",
      "          210,  1824,   171,  1512,   950,   207,  2530,   210,  7550,   301,\n",
      "         1455,   253,   207,   804,  6932,  1519,   217,   237,  1044, 13801,\n",
      "          189,   559,   115,   221,   253,   207,   804, 16764,   292,  3644,\n",
      "          211,   468,   145, 16576,   212,  1882, 13277,   273,   222,   216])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4484 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   113,   774,   128,  1421,   110,   163,\n",
      "          811,   223,   227,  8521,   117,   207,   729,   240,   503,   227,\n",
      "         3673,  1421,   110,   163,   818,   221,   105, 28272,   175,   105,\n",
      "          213,  5169,   239,   273,   115,   212,  1696,   210,   207,   649,\n",
      "         1421,  5343,   189,   213,   635,   210,   239,   811,   222,   226])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4355 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   394,   210,   972,  1275,   117,   111,   447,   117,   236,\n",
      "          160,   117,  8232,   117,   112,   233,   207,   184,   265,   119,\n",
      "          105,   126,   166,   128,   153,  5168, 15069,  1429,   213,   226,\n",
      "          812,   625,  2434,   211,   207,  2417,   215,  1588,   217,  2417,\n",
      "         2747,   218,   207,  2093,  1275,   212,   356,  4285,   219,   657])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4140 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  240, 1139,  216,  207,  695,  210,  324,  652,  247,  255, 3859,\n",
      "        4543,  225,  218,  231,  318,  210,  207, 2538, 1322,  117,  117,  117,\n",
      "         117,  105,  809,  115,  207,  563,  394,  210, 2581,  306,  229,  233,\n",
      "        1146, 2339,  547,  246,  206,  115, 3856,  117,  152,  184, 6336,  126,\n",
      "         128,  207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4759 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3427,   616,  1178,  1119,   212,   211,  8693,  3169,   218,\n",
      "          207, 24427,   210,   239,  1775,   116, 15829,   715,  2417,   189,\n",
      "          117,   105,   111,   447,   117,   236,   160,   117,  7099,   117,\n",
      "          112,  1421,  2042,   216,   145,   983,   256,   249,  4272,  1866,\n",
      "          222, 14935,   115,  3189, 26483,   166,   117, 19913,   195,   567])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 516, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3507 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   105,   207,  2015,  1194,   336,   115,   441,   483,  1883,\n",
      "          115,   199,  1148,  1883,   212,   207,  1254,   210,   604,  1125,\n",
      "          145,   966,   347,   351,   126,  1421,   128,  3442,   211,   207,\n",
      "          496,   210,   239, 16279,   547,   218,   126,  1824,   171,   128,\n",
      "          117,   222,   126,  2688,   254,   128,   207,  4714,  7026,  3638])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 498, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 744 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   113,  1291,   128,   207,   671,   207,\n",
      "          729,   240,  1677,   223,  3265,   212,  1097,  3673,   189,   207,\n",
      "         1926,   126,   113,   113,  7780,   128,  1136,   213,   145,  5994,\n",
      "        14808,   812,   117,   233,   323,  7972,   207,   126,   113,  8100,\n",
      "          128,  2327,   210,  2347,   235,   145,   663,   117,   213,   226])\n",
      "Original length: 59 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,  9580,   145,   117,   199,   174,  2891,   115,   113,\n",
      "         2891,   120,   634,  2324,   117,  6911,   193,   117, 13648,   189,\n",
      "        15378,   115,   113,   113,   198,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 200, 210, 203, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4196 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,  8231,   128,   528,   218,   707,   865, 24901,\n",
      "          189,  4265,  2786,  1687,  1409,   207,   308,   210,   207,   240,\n",
      "          210,   346,  4270,   210, 10311,   189,  1484,   216,  9150,   239,\n",
      "         1366,  1328,   189,   211,   207,  6010,   195,   371,   347,   678,\n",
      "          218, 11123,   395,  7120, 19794, 21656,   185,   212, 11494,  9964])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 203, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3699 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  207, 2218,  399,  469,  110,  163,  721,  210,  239, 1388,  189,\n",
      "         212, 4036,  117,  221,  763,  213,  207,  266, 7367, 2218,  399, 4271,\n",
      "         115,  199,  152,  184,  973,  126,  128,  207, 1632, 1670,  211, 7492,\n",
      "         218, 2218,  399, 4271,  115,  241, 1632,  210,  593,  211, 2218,  189,\n",
      "         280,  218])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 203, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3956 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5659,   957,  1632,   211,  1403,   449,  1669,   967,   749,\n",
      "          210,   145,   687,   210,   449, 11279,   189,   212,   211,  3698,\n",
      "          207,  2120,   210,   207,  1251,   784,   221,   449,   284,  3729,\n",
      "          115,   153,   117,   149,   117,   115,   207,  5104,   210,   237,\n",
      "          784,   116,   957,   449,   223,   370,  9503,  5361,   452,   117])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 203, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 215 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   308,   212,   936,   115,   226,   339,   475,   414,   210,\n",
      "          463,   634,   115,   207,   308,   210,   207,   240,   210,   346,\n",
      "         4270,   210, 10311,   189,  1484,   213,   207,   348,   116,  9108,\n",
      "          252,   691,   223,  5133,   115,   212,   207,   759,   223,  1629,\n",
      "          117,  3097, 14176, 24901,   189,   115,   707,   865,   102,     0])\n",
      "Original length: 2755 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113, 22726,   128,   222,  1497,   217,  7458,\n",
      "          218,   207,   240,   119,   207,   240,   361,   255,  8273,   252,\n",
      "          236,   207,   485,   210,   289,   210,   207,   749,   210,   207,\n",
      "          240,   212,   145,  1375,   210,   207,   881,  1716,   443,   244,\n",
      "          213,  1943,  1594,   446,   227,   361,  7896,   213,  2009,   210])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 200, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3121 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   325,   210,  1099,   115,  6819,   117,   156,   117,  2206,\n",
      "          116,  2206,   115,  2425,  3496,   117,  1185,   111,  9304,   174,\n",
      "          236,  1323,   165,   117,   163,   117,   147,   117,   142,  3923,\n",
      "          149,   190,  2830,   117,   112,   117,   207,  1053,  3820,   211,\n",
      "        10485,  1016,   213,   207,   454,   210,   958, 12213,   117,  2353])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2941 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   165,   117,   163,   117,   147,   117,   142,  5089,   111,\n",
      "         9829,   235,   162, 26422,   126,   113,   113,   203,   128,   258,\n",
      "         1003,   288,   207,  8498,  1718,   316,   598,   689,   112,   117,\n",
      "         1285,   244,  2577,   307,   293,   153,  1656,   173,   189,  2587,\n",
      "          216,   335,   247,  3152,   270,   357,   242,   207,  1099,   325])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3730 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   147,   117,  9283,   213,   332,   290,   210,   226,   528,\n",
      "          115,   153,  3412,  2715,   207,   838, 23812,   174,  2913,   211,\n",
      "         2768,   116,  2076,   115,   207,   838,   211,  2907,   289,   110,\n",
      "          163,  2381,   116,   223,   329,   218,   207,  1099,   325,   310,\n",
      "          227,   207,  5859,   534,   117,   226,  5696,   223,  2536,   218])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5423 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  8208,   235,   535,   117,   166,   117,   221,  8202, 14820,\n",
      "          189,  8208,   235,  1610,   117,   115,  7769,   165,   117,   163,\n",
      "          117,  7938,   115,  2721,   116,   527,   115,  2367,   163,   117,\n",
      "          756,   117, 13211,   115,  5559,  2837,   115,  1288,   156,   117,\n",
      "          149,   174,   117,   199,   174,  6803,   111,  1658,   112,   111])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4747 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   575,   211, 14197,   239,   915,  3251,  1410,   225,   236,\n",
      "          109,   164,   110,   163,   958,  6152,   117,   207,   240,   545,\n",
      "          216,   552, 12543,   246,  2041,   211,  4310,   225,   236,   109,\n",
      "          164,   213,   207,   915,   116,  3251,   327,   115,   233,   246,\n",
      "          227,   575,   211,  2357,   222,   236,   109,   164,   110,   163])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4217 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   225,   207,  3366,   881,   216,   105,   207, 14934,   529,\n",
      "          210,  4804,   608,   212,  1177,   632,   218,   207,  1099,   325,\n",
      "          297,   219, 15371,   252,  3091,   218,   220,   668, 11942,   257,\n",
      "          225,   207,  2959,   995,   210,   237,  5859,   347,   117,   105,\n",
      "          321,  3764, 13393,   115,  4835,   150,   117,   200,   174,  6074])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2879 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2578,   353,   481,   219,   786,   802,   207,  2980,   528,\n",
      "          115,   228,   221,   111,   198,   112,   372,   233,   246,   561,\n",
      "          115,   428,   210,   207,  2129,  1041,   233,  2022,   222,   111,\n",
      "         4782, 18714,   441,   212,  1609,  1186,   112,   263,   255,  6672,\n",
      "          120,   111,   199,   112,   207,  2980,   284,   459,   145,  7413])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1722 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   261,  1708,   220,   350, 11163,   211,  9912,   207,   517,\n",
      "          210,   239,   111, 28840,  1161,   112,  4809,  2381,   253,   207,\n",
      "          517,   244,   105,  1754,   105,   217,   207,   350, 11163,   211,\n",
      "         4310,   117,   226,  2499,   968,   249,  2081,   255,   207,   266,\n",
      "          117,   213,   207,   376, 14541,   115,   207,  2980,   273,   261])\n",
      "Original length: 1420 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  7527,   128,  3045,   577,   115,   881,\n",
      "          865,   119,   117,   820,  8727,  8588,  1522,   115,   149,   190,\n",
      "         1742,   117,   115,   615,   238,   207,   463,   282,   115,   722,\n",
      "          115,   371,   210,   207,   354,   265,   483,   240,   217,   207,\n",
      "         3426,   483,   210,   350,   871,   111,  2162, 13172,   665,  4427])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4257 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1309,   115, 13376, 10502,   177,  3244,   320,   907,   211,\n",
      "        22801,   116,  4431,  7442,   116,   212,   293, 12418,   804,  1265,\n",
      "          244,   145,   432,   210,   405,  9783,   860,   117,   623,   211,\n",
      "          820,   115,   320,   216,  3953,   822,   189,  1536,   238,   207,\n",
      "          376,   408, 11684,   223,   227,   309,   211,   293, 12418,   804])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4868 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 5859,  215, 2010, 2044,  115,  212,  207, 1786, 2081,  249, 2666,\n",
      "         215,  545,  216,  894,  110,  818,  246, 3563,  117,  820,  323, 3194,\n",
      "         216,  552,  894,  110,  818,  223, 1395,  116, 2010,  115, 1799, 5859,\n",
      "         534,  297, 6205,  207,  592,  210,  207,  477,  325, 1625,  268,  309,\n",
      "         894,  211])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4843 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   248,   478,   111,   150,   112,   211,  1232,   392,   212,\n",
      "          578,  2803,   608,   225,   207, 10104,   325,   110,   163,  1444,\n",
      "          216,   335,   244,  1975,   558,  1131,   115,   105,   728,  1917,\n",
      "          207,  1786,   263,   227,  1280,   957,   207,  2803,   608,   117,\n",
      "          447,   117,   236,  6641,   116,   345,   117,   213,  9088,   184])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5457 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   350,   697,   117,   447,   117,   236,   200,   115,   368,\n",
      "          117,   207,  1786,   323,  2442,   303,   216,  9783,   860, 17667,\n",
      "          211,  6369,   207,  1684,  2906,   210, 13376, 10502,   177,   117,\n",
      "          447,   117,   236,   202,   117,   213,  6729,   212,  4734,   115,\n",
      "          207,  1786, 22186,   252,   207,  9783,   860,   550,   212,  1453])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1909 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   240,  1560,   213,   239,   308,  4197,   820,   110,   867,\n",
      "          217,  7005,   117,   222,   216,   867,   212,   222,   615,   115,\n",
      "          820,   126,   113,   113,   339,   128,  3194,   216,   207,   483,\n",
      "          240, 28310,   207,   105,  3647,  1695,   105,   210,   248,   206,\n",
      "          111,   145,   112,   111,   203,   112,   212,   105,  1736,   252])\n",
      "Original length: 762 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1734, 23320,  2121,   111,   198,   112,   126,   128,   111,\n",
      "          198,   112,  3517,   709,   142,  3434,   100,  1064,   210,   709,\n",
      "          100,  5839,   222, 15128,   189,   117,   116,   116,   222,   615,\n",
      "          238,   145,   371,  6214,   237,   347,   293,   207, 14040,   210,\n",
      "          145, 15128,   314,  1516,   211,  1836,   115,   207,  4624,   240])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4092 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   212,   233,   223,   237,  2287,   210,  1045,   211,  4696,\n",
      "          145, 15128,   314,  1516,   211,  1836,   253,   207,   595,  2777,\n",
      "          330,   223,   145,   480,  1814,   216,   220,  4055,  1541,   218,\n",
      "          207,   588,   356,   219,  2764,   174,   218,   543,   117,   207,\n",
      "          376,  1075,   549,   211,   237,   615,   238,   145,   371,   222])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2908 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   555,   207,   355,   210,   207,  3427,  1016,   266,   211,\n",
      "         1049,   145,  1090,  7105,   213,   145, 10277,   235,  2963,   117,\n",
      "          323,   115,   207,  2148,   503,   227,   264,   145,  1630,   225,\n",
      "         5859,   534,   215,  1302,   117,   126,   321,   282,  9390,  5551,\n",
      "          115,  1152,   210,  3000,   117,   266,   111,   206,   475,   149])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4699 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   228,  1503,   117,   105, 11137,  2879,   110,   163,   936,\n",
      "         1931,   207,  7105,   302,   242,   145,   990,   216,   105,  1884,\n",
      "         2495,   189,   555,   207,   398, 17598,   115,   283,  1722,  1604,\n",
      "          117,   105,   213,   760,  1094,   115, 11137,  2879,   110,   163,\n",
      "         3231,   145,  1794,  2458,  1068,   236,  1698, 20449,  2289, 10277])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5663 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2121,   111,   199,   112,   126,   128,   111,   199,   112,\n",
      "          152,   184,   973,   126,   128,   207,  3427,  1016,   266,   111,\n",
      "         5471,   117,   109, 12758,   117,   473,   115,   142,  3454,  3384,\n",
      "          149,   190,  2830,   117,   112,   246,   105,   289,   210,   207,\n",
      "          340,   116,  1640,   110,  3612, 10318,  1275,   110,   210,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5039 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,  1808,   166,   117,  2987,   171, 14210,   171, 19802,\n",
      "        12064, 10998,  7635,   115,   521,   117,   111,  1751,   112,  3999,\n",
      "         3000,   117,  2153,   117,   200,   174,  7107,   115,  7218,   126,\n",
      "         4597,  3000,   117, 10900,   117,  4217,   128,   115,   207,   240,\n",
      "         1468,   216,   152,   184,  2450,   126,   128,   207,   354,   265])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4329 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1016,   119,   152,   184,  2967,   126,   128,   105,   145,\n",
      "          316,  1178,  1539,  3427,  1016,   253,   233,   223, 11446,   715,\n",
      "          218,   220,   266,   115,   110,   219,   233,   966,   215,   797,\n",
      "          115,   522,   115,   264,   115,   215,  3418,   115,  1701,   115,\n",
      "         1251,   115,   215,   240,   116,   280,   110,   111, 21884,   166])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4914 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1130, 1590, 4000,  223,  222,  331,  215,  227, 1421,  249, 3859,\n",
      "         763,  145,  602,  210,  347,  217, 3427, 1016,  115,  532,  499,  216,\n",
      "         207, 1249,  741, 2913,  297, 2768,  145, 2835, 1874,  222,  207,  240,\n",
      "         117,  207, 1249,  216, 1421, 2913, 2107,  395,  705, 1886,  207, 1249,\n",
      "         210, 4774])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1156 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2121,  111,  201,  173,  112,  126,  128,  111,  201,  173,  112,\n",
      "         532,  247, 2022,  115,  390,  115,  222,  237, 1355,  210,  207, 2191,\n",
      "         663,  210,  780,  212, 1218,  473,  248, 5188, 8577,  126,  113,  113,\n",
      "         113,  498,  128,  213, 5932,  207, 3880,  210, 3427, 1016,  487,  222,\n",
      "        8630,  210])\n",
      "Original length: 1716 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  6393,   128,  2231,   588,   168,  6437,\n",
      "         4298,  1360, 10423,   115,   521,   117,   126,   105,   588,   105,\n",
      "          215,   105, 27373,   105,   128,  3620,   189,   211,  2310,  1483,\n",
      "          210,   207,   759,   210,   595, 26537, 15510, 10277,   235,  2963,\n",
      "          115,   521,   117,   126,   105,   595,   105,   215,   105, 26537])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3703 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3242,   189,   210,  2346,  5179,   391,   228,   683,   117,\n",
      "          207,   563,   291,   210,   228,  5348,   809,  2365,   224,   219,\n",
      "          596,   211,   219,  4415,   126,   113,   113,   200,   128,   949,\n",
      "          100,   117,   222,   713,   345,   115,  1138,   115,  2678,   403,\n",
      "         1246,   239,   990,   211, 27373,   225,   207,   582,   210, 26537])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5079 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   166,   117,   153,   117,   147,   117,   142,   142, 10986,\n",
      "          116, 17877,   117,   152,   184,   866,   126,   128,   213,  2793,\n",
      "          588,   110,   163,   867,   211,  2310,   242,   525,   279,   111,\n",
      "          146,   112,   111,   203,   112,   115,   207,   240,   105,   245,\n",
      "         2310,   126,   207,   128,   759,   253,   233,  1763,   411,   207])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5528 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  111,  198,  112,  207, 4831,  445,  317,  207, 5859,  683,  212,\n",
      "         207, 2984,  211,  207,  595,  212,  207, 1870,  218,  207,  588,  211,\n",
      "         602,  207, 2984,  115,  225, 1073, 2025, 2490, 8270,  235, 1566,  120,\n",
      "         111,  199,  112,  331,  207,  595,  110,  163,  832, 1563,  223,  210,\n",
      "         207,  816])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4344 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   327,  1438,   223,   207, 16687,   195,  2458,   327,   117,\n",
      "          221, 26537, 15510,   223,   227,  1325,   213,   226,   327,   213,\n",
      "          220,   887,   115,   233,   223,   227,   145,  1549,   257,   117,\n",
      "          321,   447,   117,   236,  7941,   126,   113,   113,   404,   128,\n",
      "          111,   227,   235,   216,  2353,  4042,   252,   207, 10104,   325])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4132 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   339,   128,   147,   117,  2625,   235,\n",
      "         1064,   210,  2803,  1999,  2321,   239,  4098,   115, 27373,   249,\n",
      "          787,   216,   153,  1042,  1357,   709,   210,   207,  3956,   839,\n",
      "          852,   218,   207,  2803,  1999,   211,   207,   595,   110,   163,\n",
      "        10277,   235,  2963,   117, 26537, 15510,   115,   236,  1744,  1151])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2775 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   391,   126,   113,   113,   498,   128,   207,   425,   210,\n",
      "          226,   990,   212,   220,  1722,   375,   207,  2155,   224,   227,\n",
      "          355,   696,  1232,   211,   219,   407,   220,   231,   332,   210,\n",
      "          207, 10277,   235,  2963,   215,   220,   231,   359,   656,   215,\n",
      "         1419,  1056,   215,  1765,   218,   207,  2155,   281,   145,  7118])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 284 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   391,   207,   425,   210,   226,   990,   212,   220,  1722,\n",
      "          375,   207,  2155,   224,   227,   355,   696,  1232,   211,   219,\n",
      "          407,   220,   231,   332,   210,   207, 10277,   235,  2963,   217,\n",
      "          207,   496,   210,  1360,   217,  1879,   383, 30518,  1543,   117,\n",
      "          835,   226,   339,   475,   414,   210,   463,   115,   634,   117])\n",
      "Original length: 1097 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,  4176, 29246,  1125,   145,   316,   212,  4866,   189,\n",
      "          473,   248,  3454,  3384,   347,   351,   428,  4271,   212,  1298,\n",
      "         2070,   115,  3187,   894,  1752,  2257,   266,   212,  5859,  2581,\n",
      "          117,   293,  1848,  1152,  6042,   222,   207,  5859,   470,   212,\n",
      "          145,   370,   189, 11689,   222,   207,  2257,   266,   470,   115])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 199, 210, 204, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4018 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 29144,  1740,   120,   212,   111,   199,   112, 15651,  8952,\n",
      "          212,  2987,  1529,  2104,   213,  2093,   405,  2080,   212,  1096,\n",
      "          211,   462,  1549,  1019,   210,   907,   212,   477,  1632,   117,\n",
      "        29246,  1125,   207,   347,  2566,   242,   316,   212,  4866,   189,\n",
      "          473,   248,  3454,  3384,   111,   248,  3454,  3384,   112,   552])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 200, 210, 204, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4237 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1395, 13408,  6563,   117,   111,  5119, 20137,   174,  1610,\n",
      "          117,   166,   117,  4265,  3574,  1610,   117,   111,  1751,   112,\n",
      "         6803,   165,   117,   163,   117,  8167,   115,  9439,   116,  9582,\n",
      "          115,  1293,   156,   117,   149,   174,   117,   199,   174,  8335,\n",
      "          115,  2206,   163,   117,   756,   117,  5669,   866,   111,  5119])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 204, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5020 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1152,   371,   115,  1750,   213,   226,   256,   343,   894,\n",
      "          280,   307,   145,  7378,   212, 13283,  2238,  2419,   117,   532,\n",
      "          753,   207,  1182,   712,   550,   245,   219,  1423,   218,  3865,\n",
      "         1674,   213,   775,   211,   145,   469,   110,   163,   384,   494,\n",
      "          210,   239,   734,   115,   228,   221,   207,  1035,  1389,   317])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 204, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3578 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 29246,  1096,   211,  1362,   275,  1874,   211,  1596,   145,\n",
      "        19337,  4333,   550,   210,   482,   222,   226,   550,   117,   126,\n",
      "          113,   368,   128,   809,   115,   207,   729,   240,  1758,   614,\n",
      "         1152,  6042,   222,   226,   388,   117,   199,   146,   117,   832,\n",
      "         1019,  2581, 29246,  1595,  2042,   207,   729,   240,  4342,   213])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 204, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4298 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1130,   532,   753,   225,   226,   968,   115,   233,   223,\n",
      "          227,  2907,  7286,   211, 29246,   110,   163,   811,   213,   226,\n",
      "          256,   117,   213,  3782,   217,  1152,   371,   115,   894,   503,\n",
      "          227,  2357,  1199,   222,   207,   482,   216,   207,   278,  1251,\n",
      "          784,   263,   957,   270,   449,  6184,   115,  2388,   335,  1564])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 204, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2459 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   533,  9189, 16204, 22611, 14548,   189,   246,   493,   217,\n",
      "          502,   630,   117,   558, 21894,  5084,   105,   218, 10199,  1876,\n",
      "          117,   105,   207,   240,   614,   894,   110,   867,   211,  4106,\n",
      "          216,  1804,   487,   222,   145,  1591,   210,  4099,   117, 29246,\n",
      "          110,   163,   661,   207,   184,  2017,   558, 21894,   331, 22611])\n",
      "Original length: 2022 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,  2234,   128,   558,  5449,   183,   119,   595,\n",
      "          116,  1421, 19449,  1517,   115,   521,   117,   111,   105, 19449,\n",
      "         1517,   105,   112,  1409,   238,   207,   371,   210,   207,   354,\n",
      "          265,   483,   240,   217,   207,  3426,   483,   210,   350,   871,\n",
      "          111, 11928,  9107,   145,   117,   961,  5701,   115,   483,   865])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4580 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6311,   211,  2728,  4348, 14812,  6366,  3272,  2278,   253,\n",
      "          174,   189,   115,   310,   372,   233,  1637,   211,   465,   340,\n",
      "          115, 17058,   116, 12400,  4051,   211,  1960,   207,   340,   116,\n",
      "         1640,   105, 10722,   105,   215,   105,  1630,   210,   302,   105,\n",
      "          663,   679,   213,   239,   608,   225,   253,   174,   189,   115])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4261 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   145,   117,   207,   444,   327,   152,   184,  2837,   126,\n",
      "          128,   221,   237,   914,   691,   115,   233,   223,   369,   211,\n",
      "         3822,   207,   444,   424,   212,  3956,   327, 17058,   116, 12400,\n",
      "          223,   832,   211,   219,  5508,  7370,   117,   321, 30517,   118,\n",
      "         7959,   166,   117,  1615,  2882,   115,  3643,   150,   117,   200])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5323 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   236,  5047,   115,   217,  1455,   207,  1373,   210,   145,\n",
      "         1182,  1063, 12418,   297,   635,   145,  1408,   216, 14812,  6366,\n",
      "         3272,   618,   218,   253,   174,   189,  1539,   145,  1182,  1063,\n",
      "        12418,   117,   324,   213, 15094,   244,   119,   111,   198,   112,\n",
      "         4776,   664,  1551,   120,   111,   199,   112, 12516,   174,  7149])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4190 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2562,   210, 13817,   145,  5508,  3064,   388,   117,  1831,\n",
      "          145,   444,   327,   223,   657,   115,   207,   588,   110,   163,\n",
      "          617,   213,   216,   327,   356,   219,   407,   221,   145,  3292,\n",
      "          217,   327,   643,   117,   126,   113,   113,   404,   128,   353,\n",
      "          211,   207,   564,   115,   390,   115, 19449,  1517,   249,  1096])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5159 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   339,   128,  1015,   115,   532,  1231,\n",
      "          216,   207,   483,   240,  1758,   614,  1152,   371,   213,  2009,\n",
      "          210, 17058,   116, 12400,   222, 19449,  1517,   110,   163,   248,\n",
      "          199,  5508,  3064,   212,  3750,  5508,  3064,   470,   117,   426,\n",
      "          117,   248,   198,   210,   207, 10104,   325,   221,   207,   483])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2237 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   216,   207,   460,   481,   219,  5911,   591,   710,   115,\n",
      "          126,   113,  2841,   128,   310,   216,   207,   240,   246,  3236,\n",
      "          211,  4129,   552,   207,   278,   847,   210,   709,   246,   207,\n",
      "         3825, 18672,   105,  1436,   460,   105,   210,   784,   709,   117,\n",
      "          447,   117,   236,  9585,   115, 10379,   116,   774,   117,  9603])\n",
      "Original length: 2089 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  6802,   128,   528,   212,   308,   116,\n",
      "         1366,  2411,   207,   276,   115,  5719,   116,  6603,  2783,   189,\n",
      "          115,   521,   117,   115,  5719,   116,  6603,  7340,   185,  5372,\n",
      "          115,   521,   117,   115,  4298,  5514, 19418,   189,  1824, 15943,\n",
      "          115,   521,   117,   111,  1462,   105,   820,   105,   112,   115])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4373 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,  7354,   128, 10726,   148,   117, 10962,   196,\n",
      "         3226,   171,   111,   105, 10962,   196,   105,   112,   115,   145,\n",
      "          629,  1580,   443,   249,  3222,   236,   207,   159,  3621,   626,\n",
      "         1094,   115,   246,   207,  1173,  5012,  4436,   180,   900,  1292,\n",
      "          222,   226,   256,   212,   246,  4518,   217,   207,  1024,   332])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4492 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 14513,   185,   111,   105, 14513,   185,   105,   112,   115,\n",
      "          207,   243,   216,  6011,   207,  1245,  1824, 15943,  2783,   189,\n",
      "         4774,   252,   117,   391,   226,  1085,   207,  1276,   210,  1824,\n",
      "        15943,  1661,   211,   462,   207,   159,  3621,   225,  3205,   286,\n",
      "          335,  2468,   212,   335,  1956,   207,  1085,   225,   207,  7067])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5450 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  603,  216,  263,  255, 9048,  218,  207, 5012, 4436,  180,  294,\n",
      "        2566,  211,  207,  408,  663,  210,  207,  997, 1269,  210,  207,  346,\n",
      "        4598,  210, 7340,  185, 5372,  117,  222,  557,  201,  115,  634,  145,\n",
      "         954, 1314,  210,  207, 5027,  246, 1329,  218,  820,  110,  661,  211,\n",
      "         207, 5012])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4569 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  979,  634,  116,  732,  634,  116,  207, 8034,  293, 7439,  189,\n",
      "         317,  241,  276,  802,  207,  640,  216,  263, 1582,  207, 1024,  302,\n",
      "         213,  207, 1245, 8034, 2783,  189,  115,  222,  979,  339,  115,  634,\n",
      "         145,  671,  210, 1870,  222,  767,  210, 5717, 1360, 2458,  235,  115,\n",
      "         521,  117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4651 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1824, 15943,  2522,   117,   200,   207,  2331,  4051,   211,\n",
      "         5913,   459,   222,   588,   117,   218,   226,   259,   115,   588,\n",
      "          263,   571,  1970,  1855,   115,  2303,  1513,   189,   115,   212,\n",
      "         1038,  5308,   210,   560,   276,  1618,   235,   270,   635,   215,\n",
      "         3281,   211,   207,   619,   117,   226,   408,  2331,   216,  2833])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4999 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   775,   211,   241,   207,   286,   335,   263,   571,   948,\n",
      "          207,  8034,   211, 14513,   185,   115,   207,   316,   115,   580,\n",
      "         3064,   212,   567,  1447,   221,   705,   221,   207,   405,   493,\n",
      "          212,   231,  1885,   115,   335,   323,  2468,   145,  5678,   528,\n",
      "          671,   216,   297,  7115,   207,   365,  1355, 14040,   126,   113])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4639 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  671,  386,  227,  126,  113, 4542,  128,  785,  327, 1540,  215,\n",
      "         327,  644,  117,  145,  316, 2927,  210,  145, 2427,  619,  213,  207,\n",
      "         327,  297, 1432, 2433,  211, 1025,  331,  145,  496,  405,  223,  281,\n",
      "         207, 4994,  210,  207, 1071,  453,  213,  126,  113,  113,  508,  128,\n",
      "         207,  327])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4793 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   550,   210,  1021,   235,   207,  1476,   210,   207,\n",
      "          997,   630,   539, 13597,   189,   207,   997,  2005,   210,   630,\n",
      "          539,   246,  1125,   459,   213,  2912,   713,   221,   237,   543,\n",
      "          211,   207,  5027,   213,   237,  2732,   218,   207,  5012,  4436,\n",
      "          180,   211,  1839,   207,   408,   663,   210,  7340,   185,  5372])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4843 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   514,  1789,   669,   222,   207,  7346,  1705,   210,\n",
      "        18224,   115,   463,   202,   115,   634,   117,   222,   216,   414,\n",
      "          115, 11265,   116, 21832,   252,   171,  2017,   820,   110,  1883,\n",
      "          211,  9610,   207,  4026,   210,   207,   514,   598,   200,   119,\n",
      "          362,   160,   117,   157,   117,   216, 15469,   340,   216,   588])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5350 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1534, 25756,   213,   207, 30326,   995,   210,   145,   264,\n",
      "         5859,  4179,   213,   237,  3386,   211, 15776,   820,   211,  3291,\n",
      "          211,   207,  1634,   110,   163,  3161,   117,   126,   113,  5997,\n",
      "          128,   532,   296,  3212,   236,   207,   264,   240,   759,   212,\n",
      "         2617, 20718,   189,   211,   760,   115,   634,   117,   207,   759])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5009 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3503,   216,   231, 28840,  1161,  2044,   115,   210,   229,\n",
      "          230,   460,   246,  2452,   236,   207,   886,   115,   432,   207,\n",
      "          421,   210,   502,   759,   115,   283,   324,   741,  1637,   211,\n",
      "        13190,   213,  2968,   145,  5678,   528,   671,   117,   226,  3108,\n",
      "         3615,   212, 22242,   467,   207,  4435,   210,  1534,   216,   249])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4956 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  550,  598,  207,  716,  923,  210, 2399,  115,  212,  293,  207,\n",
      "        2191, 4227, 1789,  669,  115,  372, 2762, 2785,  105, 1897,  117,  105,\n",
      "         111,  321, 4523,  230,  117,  556,  112,  117,  207, 1814,  210, 2926,\n",
      "        1252,  189,  212, 1378,  213,  207,  630,  539, 3733,  626,  414,  289,\n",
      "         210,  207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3864 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   241,   324,   443,   292,  8617,   235,  1897,   212,   408,\n",
      "         2331,   189,   222,   502,   117,   626,   230,   460,   246,  2452,\n",
      "          211,   207,   363,   216,   578,   506,   244,  3137,  1146,   222,\n",
      "          231,   918,   115,   331,   242,   207,   620,   210,   145,   880,\n",
      "          215,   227,   115,   820,   110,   674,   644,   277,   244, 11933])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4995 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   111,  3189, 12866,   166,   117,  9714,  5183,   115,   613,\n",
      "          165,   117,   163,   117,  7294,   115,  6059,   115,   442,   156,\n",
      "          117,   149,   174,   117,  8788,   115,   198,  6204,   117,   156,\n",
      "          117, 10900,   117,  4448,   111, 24093,   112,   112,   117,   372,\n",
      "         7852,   228,   636,   644,  5960,   115,   207,  1327,   217,  5775])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5006 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1547,   189,   217,   216,   691,   117,  8313,   175,   540,\n",
      "          115,  1392,   117,   166,   117,  3501,   109, 18153,   115,  7107,\n",
      "          165,   117,   163,   117,  4494,   115,  4692,   115,  2531,   156,\n",
      "          117,   149,   174,   117,   199,   174,  4076,   115,  2623,   163,\n",
      "          117,   756,   117,  5365,  2758,   111,   751,   112,   117,   152])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3383 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  9016,   216,   221,   915,   221,   741,   126,   113,   113,\n",
      "         1449,   128,   503,   227,   468,   207,   364,   332,   210,   502,\n",
      "          759,   741,   297,   219,   281,   207,  8206,   210,   207,   266,\n",
      "          213,  8129,   235,   225,   820,   212, 11222,   270,  1927,   211,\n",
      "          629,   364,   373,   741,   297,  3042,   270,   880,   117,   228])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 516, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5264 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,  7179,   128,   152,   184,  4019,   126,   128,\n",
      "          211,  3337,   213,   145, 30326,  1177,   388,   145,   595,   311,\n",
      "         2587,   216,   111,   198,   112,   300,   215,   741,  2031,   145,\n",
      "         1787,   319,   120,   111,   199,   112,   207,  1580,   215,   264,\n",
      "          524,   263,   145,  5021,   213,   207,   542,   210,   216,   319])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 498, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4894 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 4048,  115,  207,  256,  373,  226,  240,  223,  227, 3901,  802,\n",
      "        5859, 1540,  115,  215,  207, 2525,  849,  115,  215,  466,  736,  215,\n",
      "         674,  644,  117, 2388,  115,  233,  223,  802,  145,  264,  524,  443,\n",
      "         249, 3750,  211, 5518,  212, 2287,  264,  643,  211, 2488,  820,  110,\n",
      "        1515,  277])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 442, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4800 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,  1590,  2671,   189,   212,   529,   210,   534, 24382,\n",
      "          117,   233,   223,   226,   408,   302,   213,  1021,   235,   466,\n",
      "          736,   212,  7922,   235,   207,   559,   213,   207,   525,   210,\n",
      "          266,   216,   207,   240,   435,   189,   225,   226,  1366,  2411,\n",
      "          117,   207,   276,   216,   835,   288,   226,   619,  2255,   216])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 556, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 118 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   233,   340,  1677,   117,   340,  1677,   117,  2254, 10389,\n",
      "          115,  7340,   185,  5372,   115,   126,   113,   113,   896,   128,\n",
      "          463,   556,   115,   634,   117, 10389,   157,   117, 10403,   116,\n",
      "        23291,  5519,   196,   165,   117,   163,   117,   483,   865,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 2652 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  528,  126,  113, 7295,  128,  126,  113,  113, 6504,  128, 3983,\n",
      "        6416, 6441,  115,  154,  117, 1532,  211, 1754,  189,  115,  207,  461,\n",
      "         213,  226,  256,  223,  331,  151,  117,  156,  117,  147,  117,  951,\n",
      "         171,  115,  142,  282,  115, 2916,  207,  595,  115,  145, 7818, 6809,\n",
      "         115,  211])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1574 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1198,  4478,   215,  3642,   593,   117,   200,   207,  1119,\n",
      "          783,   216,  2581,   210,   142,   442,   171,   356,  1761,   211,\n",
      "         2319,   210,   207, 24427,   110,   163,  1214,   115,   151,   117,\n",
      "          156,   117,   147,   117,  3604,   115,   142,   516,   115,   126,\n",
      "          113,  9383,   128,   215,   211,  2858,   210,   797,  2339,   547])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3055 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1590,  1355,  7168,   225,   207,  6004,   117,   152,   184,\n",
      "         2138,   126,   128,   145,   759,   356, 10899,   219,  1629,   294,\n",
      "          211,  2418,   117,   162,   117,  3195,   117,   160,   117,   279,\n",
      "          111,   146,   112,   111,   203,   112,   105,   621,   233,  1763,\n",
      "          411,   216,   207,  9350,   257,   223,   227,   575,   211,  1322])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3369 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5490,  4677,   577,  2521,   189, 13233,   175,   115,   521,\n",
      "          117,   166,   117,   336,  6395,  1610,   117,   115,  6402,  2418,\n",
      "          117,  7960,   115,  7031,   158,   117,   149,   117,   199,   174,\n",
      "         6305,   111,  1851,   112,   120, 23424,   190,  1610,   117,   166,\n",
      "          117,  3078,  4436,  2304,   115,  7278,  2418,   117,  6757,   115])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4046 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  2967,   126,   128,   236,   207,  1699,  4473,\n",
      "         1383,   115,   390,   115,   405,  1669,   223,   237,  3427,   616,\n",
      "         1178,   307,   253,   207,  1669,   249,   237,  1160,  1576,   222,\n",
      "         1016,  1092,   115,   227,  2641,   237,  1160,  1576,   222,  2381,\n",
      "          189,   117,   368,   221,   289, 22103,   249,  2494,   115,   126])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2211 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   236,   207,  1699,  4473,  1383,   115,   307,   405,  1669,\n",
      "          126,   113, 10459,   128,  1733,   211,  1915,   145,  5508,   195,\n",
      "          223,   237,  3427,  1178,   215,   237,  3427,  1243,   210,  1016,\n",
      "          117,   126,   113,   113,   113,   422,   128,   154,   117,   109,\n",
      "          154,   117,  3034,   115,   521,   117,   166,   117,  6603, 20334])\n",
      "Original length: 1527 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  6569,   126,   128,   616,  9431,   115,  1877,\n",
      "        10029,   115,   211,  4696,   145,   388,   210,   993, 10947,  1346,\n",
      "          242,  3587,   266,   115,   145,   595,   263,   211,  5818,   584,\n",
      "         1877,   119,   111,   198,   112,   216,   207, 17377,   235,   513,\n",
      "          210,   482,   223,  3072,   215,   216,   207, 17377,   235,   513])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2602 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  6905,   126,   128,   316, 12931,   115,   616,\n",
      "         9431,   207,  3587,  1313,   240,   249,  3531,   987,  4664,   111,\n",
      "          450,   112,   210, 12931,   142,  8596,   111,   145,   112,   117,\n",
      "        12931,   123,   117,   117,   117,   123,  3588,  2237,   123,  4702,\n",
      "         2295,   123,  1877, 12931,   123,   316, 12931,   123,   993,  2295])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2991 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  6868,   126,   128,   731,   115,  3808,   195,\n",
      "         4706,   213,  3587,   115,   213,   308,   211,  3337,   222,   145,\n",
      "         3808,   195,  4706,  3880,   115,   145,   257,   311,  1204,   216,\n",
      "          111,   198,   112,   207, 16485,  1242,   280,   145,  3117,   216,\n",
      "          300,   305,   247,   625,  1387,   297,  4750,   347,   215,  6892])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4046 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  324, 6158,  117,  321, 8266,  332,  426,  117,  552,  207,  397,\n",
      "         642,  218,  207,  276,  223,  227, 3044,  115,  233,  261,  227,  219,\n",
      "         633,  213, 1455,  207,  384, 3768,  210,  207,  396,  759,  117,  198,\n",
      "         126,  113,  113,  201,  128,  290,  117,  585,  212,  384, 1327,  226,\n",
      "         240,  249])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4492 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   447,   117,   595,  4142,   216, 11911,   383,   110,   163,\n",
      "         9685,  1479,   105,  1404,   261, 29213,   220,   231,   529,   552,\n",
      "          210,   239,  3638,  3342,   212,  5630,   117,   105,   447,   117,\n",
      "          160,  1036,   117,   623,   211,   207,   396,   759,   115,   367,\n",
      "          211,   588,   110,   163,   645,   210,  9685,   115,   789,   712])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4044 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   595,   110,   163,   396,   759,  1305,  4987,   252,   306,\n",
      "          207,   856,   759,   117,   227,   307,   503,   595,  2704,   211,\n",
      "          239,   396,   759,   350,  4196,   210,  2208,   212,  3808,   195,\n",
      "         4706,   115,   310,   595,   323, 14147,   252,   239,  5859,  2148,\n",
      "          117,   400,   207,   856,   759,   679,   307,   145,  1090,  5859])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4743 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  2450,   126,   128,   207,   393,   881,   249,\n",
      "          915,   545,   216,   145,   248,   198,   595,   105,   311,  5818,\n",
      "          207,   889,  3257,   207,  3854,   115,   239,  2412,   212, 17263,\n",
      "          117,   105,  2709,   109,   169,  5498,   166,   117, 13735, 24879,\n",
      "          195,   221,   189,   110,   158,   115,  3453,   150,   117,   199])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3537 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 3017,  225, 7834,  189,  443, 2778,  238,  215, 1240,  211,  231,\n",
      "        7834,  189,  120, 7219,  474,  210, 7834,  189,  443, 2789,  213,  228,\n",
      "         804,  120,  212, 2696,  804,  211, 7834,  189,  211,  207,  291,  207,\n",
      "        7834, 2221, 1199,  217,  239,  639, 2458,  804,  115,  213,  308,  211,\n",
      "        1403,  216])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4828 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   265,   115,  5489,   165,   117,   163,   117,  3872,   115,\n",
      "         8167,   115,  9051,   115,   456,   156,   117,   149,   174,   117,\n",
      "         3978,  2450,   115,  1431,   163,   117,   756,   117, 17929,   111,\n",
      "         8349,   112,   111,  1108,   115,   213,   370,   116,  5859,   797,\n",
      "         3854,   256,   115,   216,  1336,  3262,   305,   247,  2494,   105])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3815 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1917, 11938, 20564,   171,   246,  1287,   213,   207,  1158,\n",
      "          210,   145,   867,   217,  1152,   371,   115,   239,  3066,   115,\n",
      "         2262,   115,   249,   255,   407,   211,  1988,  3883,   211,  2310,\n",
      "          212,  4965,   216,   595,   110,   163,   248,   289,   388,   305,\n",
      "          219,  1629,   117,   202,   207, 24293,   889,   284,   126,   113])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4454 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 20875,   111,  2142,   112,   111,   227,   235,   105,   207,\n",
      "         2613,  3282,   174,   218,   207,   525,   210,   744,   223,   331,\n",
      "          207,  3648,   232,   223,   289,   216,  2675,   189,  1016,   215,\n",
      "          289,   216,  7226,   395,  1016,   105,   112,   117,   152,   184,\n",
      "         5484,   126,   128,   213,   308,   211,  5818,   145,   388,   216])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 516, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4211 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2381,  189,  115,  105,  396,  759,  160,  433,  115,  595, 2081,\n",
      "         126,  113,  113,  875,  128, 2421,  115, 5102, 2490,  783,  220,  329,\n",
      "        2194,  115,  216,  207, 2381, 2289,  189, 2258,  878,  211,  485,  167,\n",
      "         177,  190,  193,  110,  163, 2819,  117,  203,  126,  113,  113,  912,\n",
      "         128,  552])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 498, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4295 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 10002,   117,   623,   211,   207,   396,   759,   115,   160,\n",
      "          186,   342,   116,   416,   115, 11911,   383,   110,   163,  9685,\n",
      "         6158,   292,   145,   105,   893,   105,   210,   207,   350,  1479,\n",
      "          117,  2558,   216, 11911,   383,   503,   115,   213,   482,   115,\n",
      "         3972,   211, 27793,   307,   207,  5090,  1410,  2289,   189,   217])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 442, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4993 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  152,  184, 6336,  126,  128,  211,  264,  145,  388,  210, 5508,\n",
      "        3064,  242,  248,  126,  113,  113,  673,  128,  199,  115,  145,  595,\n",
      "         311, 4327,  105,  111,  198,  112,  207, 1470,  210, 5508,  195,  643,\n",
      "         213,  207,  444,  327,  212,  111,  199,  112,  207, 3007, 1128,  215,\n",
      "        1476,  210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 556, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4623 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2814,   547,   115,   521,   117,   166,   117,  9059,   190,\n",
      "         2814,   547,   115, 10316,   150,   117,   199,   174,  1164,   115,\n",
      "         2556,   111,   200,   174,   682,   117,  1167,   112,   112,   117,\n",
      "          390,   115,   207,   396,   759,  2127,   230,  2148,  2176,   497,\n",
      "          211,   220,   210,   392,  1674,   117,   207,   393,   881,   249])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 554, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3899 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   227,  1451,  2858,   210,  5859,   548,   222, 11911,   383,\n",
      "          117,   207,  2339,   547,   217,  5859,  2581,   244,  4341,   115,\n",
      "          212,   233,   223,   667,   913,   663,   221,   705,   221,   145,\n",
      "          384,   126,   113,   113,  1048,   128,  2738,   211,   960,   599,\n",
      "         2148,   115,   229,   969,  2607,  1222,   115,   373,  3429,   237])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 508, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3856 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1917, 15495,   175,   246,  1287,   213,   207,  1158,   210,\n",
      "          145,   867,   217,  1152,   371,   115,   239,  1108,   223,   278,\n",
      "          211,   207,   530,   867,   211,  2310,   117,   207,   889,   832,\n",
      "          465,   227,  4218,   216, 11911,   383,   249,  7633,   494,   569,\n",
      "          237,  7618,   175,   850,   210,   207,   327,   217,  3332,   631])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 509, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4064 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   199,   117,   207,   817,   210,   207,  1104,   115,   212,\n",
      "          207,  1345,   210,   220,  6104,   636,  3880,   211,   635,   595,\n",
      "          110,   163,   759,   115,  2147,   216,   233,   297,   219, 11939,\n",
      "          175,   217,   595,   211,   468,   435,   543,   117,   321,   150,\n",
      "          117,   162,   117,  3195,   117,   160,   117,   342,   117,   213])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 345, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4990 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6851,   175, 10456,   117,  3182,   117,   115,   521,   117,\n",
      "          166,   117, 24522,   264,   116, 23456,   181,   541,   115,   521,\n",
      "          117,   115,  4078,   150,   117,   200,   174,  4830,   115,  5284,\n",
      "          111,   200,   174,   682,   117,  1020,   112,   111,  3189,  1615,\n",
      "         3048,   117,  1028,   189,   210, 23645,   117,   115,   521,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 415, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4125 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1822,   211, 10154,   110,   163,   126,   113,  8806,   128,\n",
      "          586,   222,   207,  6953,   171,   327,   115,   212,   297,   227,\n",
      "          115,  6104,   301,   115,   219,  8559,   213,  1017,  3502,   235,\n",
      "         1041,   218,   207,  1345,   210,   167,   177,   190,   193,   238,\n",
      "        11911,   383,   110,   163,  9685,  2359,   117,   221,   595,   110])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 846, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4729 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3513,   235,   207,   393,   881,   110,   163, 17173,   115,\n",
      "          152,   184,  6905,   126,   128,   207,  3587,  1313,   240,   249,\n",
      "         3531,   987,  4664,   142,  8596,   171,   117,   321,  1481, 10534,\n",
      "        11525,   117,   166,   117, 20081,   175,  3884,   117,   350,   189,\n",
      "          186,   117,   535,   117,   115,  6876,  2324,   117,  5073,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 875, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4901 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1942,   280,   115,   521,   117,   166,   117,  1111,  3389,\n",
      "         5866,   115,   521,   117,   115,   527,   116,  4984,   116,  5502,\n",
      "         2138,   115,   634,   167,   182,  4788, 13553,  5618,   115,   236,\n",
      "          113,   279,   111,   149,   117,   148,   117,  2324,   117,  6391,\n",
      "          117,   206,   115,   634,   112,   117,   221,   211,   207,   296])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 912, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3060 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  151,  117, 2481, 1420,  116,  116, 3808,  195, 4706, 1700,  115,\n",
      "         595, 5404,  145,  388,  242,  207, 2125, 3497,  210, 3808,  195, 4706,\n",
      "         117,  152,  184, 6868,  126,  128,  213,  308,  211, 3337,  222,  145,\n",
      "        3808,  195, 4706, 3880,  115,  145,  257,  311, 1204,  216,  111,  198,\n",
      "         112,  207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 774, 210, 774, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2782 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113, 20110,   128,  9153,   115,   881,   865,\n",
      "          117,  2500, 28751,   195,  8063,  8108,   243,   111,   105, 28751,\n",
      "          195,  8063,   105,   112,  1125,  1333,   351, 11028,   469,   212,\n",
      "         1298,   497,  1499,   111,  1462,   105, 11028,   105,   112,   212,\n",
      "          154,   117,   162,   117,  7871,   185,   190,   243,   115,  2703])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3440 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  8108, 16442,   316,   246,  1545,   225, 11028,   110,   163,\n",
      "        19149,   235,  5695, 16442,   316,   212, 16442,   708,   529,   117,\n",
      "          213,  6066,   210,   226,   399,   115, 11028,  1536,   237,  1029,\n",
      "         8108,  4637,  1690,  3026, 19858,   182,   115,  7651,   117,   552,\n",
      "          233,   246, 18765,   178,   301,  7705,  9234,  9707,   317,  9763])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3055 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   217,   207,  1595,  2391,   637,   115,   207,  5104,   210,\n",
      "          207,  1583,  1256,   189,   222,   207,  8108,  4502,   246,  9468,\n",
      "          174,   373,   207, 10565,   212,   239,  1228,   784,   115,   207,\n",
      "         2645,  3115,   515,   111,   105,   719,   172,   105,   112,   117,\n",
      "          200,   934,   207,   126,   113, 11189,   128,   691,   246,  1282])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4989 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  146,  117,  483,  240,  308,  293,  207,  719,  172,  561,  239,\n",
      "        1099,  308,  212,  207,  483,  240, 1319,  216,  239, 1571, 1146,  126,\n",
      "         113,  113,  279,  128, 2549,  263, 3809,  115,  163,  176,  678,  145,\n",
      "         867,  217, 1152,  371,  117,  213,  239,  867,  115,  163,  176, 2621,\n",
      "         216,  233])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4249 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   483,   240,   110,   163,   126,   113,   113,   404,   128,\n",
      "         1374,   210,   163,   176,   110,   163,  1152,   371,   867,   297,\n",
      "          323, 11932,  2957,   210,   239,   470,   351, 11028,   117,  2388,\n",
      "          115, 28751,   195,  8063,  1785,   216,   207,   483,   240,   110,\n",
      "          163,   308,  1848,  1152,   371,   211,   163,   176,   246,  7591])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4360 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 28751,   195,  8063,   470,   216, 11028,   110,   163,   212,\n",
      "          163,   176,   110,   163,  2965, 17430,  1256,   189,   217,   207,\n",
      "          355,   210,   207,  8108, 19262,   195,  4502,  1351,   237,  5859,\n",
      "         1563,   211,   606, 28751,   195,  8063,   110,   163,   316,   212,\n",
      "          359,   117,   152,   184,  2758,   126,   128,   237,  5859,  1563])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3394 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   483,   240,  4085,  1653, 28751,   195,  8063,   110,\n",
      "          163,  4040,   216,   233,   246,   227,   329,   211,  2859,   220,\n",
      "          435,  5187,  2226,   211,   824,   207,  8108,  3675,   327,   552,\n",
      "          207,   872,   210, 11028,   212,   163,   176,   263,  2705,   228,\n",
      "         2226, 11939,   175,   117,  1130,   227,   235,   216,   227,   241])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4541 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1636,  279,  120,  111,  258,  112, 3203, 5974,  395,  948, 1087,\n",
      "         337,  120,  111,  282,  112, 3506,  145,  469,  211, 1518,  222,  207,\n",
      "         316,  117,  392, 1393, 1366, 2226,  244, 2661,  211, 1596, 1612, 1268,\n",
      "         211,  824,  207,  327,  117,  126,  113,  113,  509,  128,  296,  115,\n",
      "         221, 1468])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4985 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   150,   117,   199,   174,  3668,  2837,   115,  3866,   973,\n",
      "          111,   282,   475,   682,   117,  1344,   112,   111,   105,   126,\n",
      "          145,   128,   588,   356, 10899,   568,   218,   207,   352,   210,\n",
      "          207,  1566,  3497,   238,   207,   482,   216,   233,   223,  4589,\n",
      "          175,   211,  1403,   207,   595,   238,   219,  3423,   145,   126])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3743 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   327,   115,   233,   297,   219,   211,   185,  8116,   212,\n",
      "         5739,   211,  5138,   126,   113,   113,   910,   128,   207,  7105,\n",
      "          302,   221,   472,  3580,   117,   207,  4063,  2428,   307,  1231,\n",
      "          453,   253,   207,  6398,   356,   219, 15846,   301,  4637,   174,\n",
      "          115,  4093,   252,   115,   212,  1165,   117,   314,  2419,   216])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4715 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   217,   324,   853,   284,   303,   348,   115,   226,   240,\n",
      "         3119,   216, 28751,   195,  8063,   249,  1096,   211, 11459,   460,\n",
      "          238,   229,   145, 11003,   210,   482,   481,  2283,   216,   233,\n",
      "          249,  2150,   237,  5859,  1563,   211,   591,   239,   316,   215,\n",
      "          359,   117,   532,   420,  4129,   207,   483,   240,   110,   163])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5110 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   865,  8723,  6385,  1507,   614,  1152,   371,   115,   221,\n",
      "          284,   387,   348,   115,   211,   163,   176,   212, 11028,   222,\n",
      "        28751,   195,  8063,   110,   163,  2189,   522,   470,   120,   300,\n",
      "         2296,   207,  5318,   210,   207,   240,   105,   211,   824,   371,\n",
      "         1015,   212,   207,   184,  1686,   207,   256,   117,   105, 28751])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 516, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4005 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   606, 11028,   212,   163,   176,   678,  3883,   211,  2310,\n",
      "          226,   615,   222,   207,  1300,   216,   233,   223,  7515,   117,\n",
      "          335,  4327,   216,   207,   483,   240,   110,   163,  1230,  2957,\n",
      "          210,   207,  3757,   189,   314,  1678,  7515,   252,   239,  1435,\n",
      "         3053,   210, 28751,   195,  8063,   110,   163,  3883,   211,  2310])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 498, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5577 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,  1039,   128,   126,   113, 19163,   128,\n",
      "          213,   226,   256,   115,   233,   297,  1868,   216, 28751,   195,\n",
      "         8063,   110,   163,   615,   210,   207,   483,   240,   110,   163,\n",
      "         3053,   210,   239,   867,   211,  2310,  3279,   189,  8007,  9038,\n",
      "          281,   207,   525,  4561,   213, 10591, 10254,   117,   207,   483])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 442, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4229 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,  1291,   128,   331, 11028,   110,   163,\n",
      "          212,   163,   176,   110,   163,  3757,   189,   244,  1758, 26394,\n",
      "          174,   221, 15175,   213,  2287,   210,   736,   215,  4797,   355,\n",
      "          210,   966,   417,  1804,   189,   213,   332,   207,   461,   331,\n",
      "        28751,   195,  8063,   110,   163,   615,  5144,   189,   207,  1038])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 556, 210, 556, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3072 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  111,  145,  112,  417,  373,  207, 7651,  264, 2091,  115,  207,\n",
      "        7651,  393,  483,  240,  212,  207, 7651, 1313,  240, 2041, 8165, 2889,\n",
      "         210, 7651,  264, 5310,  189,  217,  207, 4502,  569,  264, 1116,  212,\n",
      "         249, 2852,  241,  210,  228,  417,  117,  111,  146,  112,  417,  373,\n",
      "         207, 1860])\n",
      "Original length: 1208 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113, 22693,   128,   273,   212,   308,   226,\n",
      "          347,  3335,   303,   210,   207, 11792,   210,  7846,  6091,  1416,\n",
      "          117,   145,   243,   216,  3237,   222,   207,  2656,   235,   115,\n",
      "          166, 12733,  2759,   212,  1180,  6775,   438,   310,   246,   227,\n",
      "         2225,   207,   630,   249,  6047,   207,  1499,   213,  1136,   210])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4214 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   111,   105,  6653,   105,   112,   115,  1225,   222,   767,\n",
      "          210,   207,   483,   115,   438,   252,   217,   207,  1650,   212,\n",
      "         1134,   210,   207,  7846,  6091,  1416, 11792,   189,   117,   588,\n",
      "         9426,  1134,   243,   223,   207, 20209,   212,   336,  1028,   210,\n",
      "          207, 11792,   189,   117,  6927,  4142,   216,   207,  7846,  6091])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5044 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  2138,   126,   128,   934,   142,   198, 15695,\n",
      "         5019,  1425,   232,   213,  6124,   210,   616,   115,   207,   354,\n",
      "          265,  1313,   240,   249,  1672,   142,   198,   211,  3430,   307,\n",
      "        12498,   126,   113,   113,   203,   128,   216,  2488,   207,   105,\n",
      "          525,   210,   744,   115,   105,   153,   117,   149,   117,   115])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4770 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2948,  115,  213, 4613, 5600, 3540,  535,  117,  115,  207,  595,\n",
      "         115,  237,  972, 2458,  327,  577,  210, 8954,  115,  832,  216, 4613,\n",
      "        5600, 3540,  263,  835,  288,  145, 4622,  115,  930,  405, 2080,  232,\n",
      "         225,  239, 3942,  115,  443,  292,  595,  110,  163, 2381,  189,  117,\n",
      "         595,  832])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5496 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,   901,   210,   392,  1075,   115,   233,   223,  2908,\n",
      "          216,  6927,   249,   832,   230,  5859,  1563,   117,  6927,   110,\n",
      "          163,  2148,   213,   230,   710,  3456,  1395, 13408,   818,   117,\n",
      "          207,   759,  4142,   216,   207,   832,  3237,   174,   235,   736,\n",
      "          246,   105,  1733,   211,  4224,  1016,   212,   378,  2372,  1049])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5176 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   323,  1645,  2282,   189,   115,   521,   117,   115,  8458,\n",
      "          150,   117,   199,   174,   236, 12877,   111,   105,   207,  5818,\n",
      "          577,   245,   227, 10961,   392,   126,   525,   279,   111,   146,\n",
      "          112,   111,   203,   112,   128,   506,   218,  2001,  3187,   145,\n",
      "        10597,   384,  1113,   120,   253,   207,   889,   110,   465,   227])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2968 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 23796,   383,  6231,   189,   115,   521,   117,   166,   117,\n",
      "         4001,  6377, 14124,   115,   521,   117,   115,  3931,  7573,   117,\n",
      "          199,   174,  5410,   115,  7727,   115,  7943,   158,   117,   167,\n",
      "          117,   199,   174,  8209,   111,  1138,   112,   120,  1824,   117,\n",
      "         3911,   117, 17140,   117,   210,  7573,   117,   115,   521,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 89 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   354,   265,   483,   865,   777,   226,   200,   188,   174,\n",
      "          414,   210,   609,   641,   115,   236, 14640,   175,   175,   115,\n",
      "         6861,   117,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 616 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  528,  126,  113, 4217,  128,  396,  594,  212, 2376,  585,  226,\n",
      "         691,  246,  385,  115,  294,  211,  508,  165,  117,  163,  117,  147,\n",
      "         117,  142, 8189,  111,  146,  112,  111,  198,  112,  111,  145,  112,\n",
      "         109,  111,  146,  112,  115,  211,  207, 2090,  126,  113, 4078,  128,\n",
      "         222,  692])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  368,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4282 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2791,  213,  270,  450,  396,  759,  115, 1125,  294,  211, 1557,\n",
      "         201,  212,  404,  210,  207, 8913, 1386,  184,  325,  115,  342,  165,\n",
      "         117,  163,  117,  147,  117,  142,  342,  111,  105,  248,  201,  105,\n",
      "         112,  115,  342,  165,  117,  163,  117,  147,  117,  142,  556,  111,\n",
      "         105,  248])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  342,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4066 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113,  113,  203,  128,  820,  110, 2231,  210,  266,  213,\n",
      "         635,  210,  207,  867,  217,  687, 2122,  246,  678,  222,  760,  498,\n",
      "         115, 1077,  111, 3192,  117,  230,  117, 1047,  112,  117,  236,  894,\n",
      "         110,  485,  115,  488, 2779,  246,  702,  115,  218,  308,  777,  647,\n",
      "         508,  115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  404,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4115 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   111,   105,  2218,   588,   111,   163,   112,   105,   112,\n",
      "          115,   450,   396,   759,   160,   160,   201,   115,   345,   115,\n",
      "          415,   115,   912,   115,   675,   146,   115,   203,   212,   207,\n",
      "          312,   210,  2847, 16019,   195,   778,   111,   105,  6789,   105,\n",
      "          112,   115,   237,  1102,   210,   778,   210, 16019,   195,  1190])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  416,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4676 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1220, 8860,  117,  450,  396,  759,  160,  404,  117,  820, 4293,\n",
      "         216, 2218,  894, 1013,  771,  238,  207, 3854,  555,  207, 1703, 3101,\n",
      "         210, 3013, 2847, 5024, 8860,  211, 1005,  270,  841, 2847, 3362, 1190,\n",
      "         189,  236,  145, 4477, 1812,  828,  117,  447,  117,  160,  126,  113,\n",
      "         113,  342])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  422,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4723 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  222,  270, 1687,  867,  211, 2310,  115,  894, 4293,  115,  487,\n",
      "         222,  207, 2148,  679,  213,  207,  450,  396,  759,  221,  705,  221,\n",
      "         460, 2268,  213,  445,  225,  894,  110, 3281,  211,  820,  110,  867,\n",
      "         217,  687, 2122,  115,  216,  820, 1591, 5859, 1566, 2424, 2957,  210,\n",
      "         207,  347])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  433,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4594 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,   973,   126,   128,   145,   240,   296,   115,\n",
      "          105,   311,  2351,  1566,   211,  4691,   487,   222,   207,  1566,\n",
      "          210,   207,  2615,   595,   126,   163,   128,   212,   227,   306,\n",
      "          207,  1566,   210,  1541,   687,   749,   117,   105, 13025,   116,\n",
      "         9735,   180, 27264,   115,  2559,   115,   236,  4210,   111,  3189])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  339,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5417 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213, 10926,   189,  6943,  1610,   117,   166,   117, 27373,\n",
      "        17644,   116,   159,   116,  8329,   115,   521,   117,   115,  2559,\n",
      "          115,   820,  1430,   221,   949,   536,  2606,   335,  2852,   552,\n",
      "          588,   263,  5981,  1405,  1298,  2947, 17644,   235, 16925,   189,\n",
      "          213,   820,   110,   327,   229,   115,   310,   217,   588,   110])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  441,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5600 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  9936,   211,   145,   485,   217,  4162,  1322,   294,   211,\n",
      "          248,   404,   210,   207,  8913,  1386,   184,   325,   246,   632,\n",
      "          213,  1645, 11164,   115,   521,   117,   166,   117,  3889,  5326,\n",
      "          190,   210,  5229,   115,  7239,   165,   117,   163,   117,  2206,\n",
      "          115,  2623,   115,   951,   156,   117,   149,   174,   117,   199])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  478,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3988 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 13648,   189,  3926,  2837,   115,   751,   167,   182,  9228,\n",
      "        12452,   111,   158,   117,   148,   117,  1775,   117,  6404,   117,\n",
      "          279,   115,   751,   112,   111,  1071,  1471,  7008,   189,  2041,\n",
      "          629,  1952,  1903,   238,   588,   126,   113,  4188,   128,   616,\n",
      "         1445,   280,   459,   210,  4320,  7008,   189,  1096,   211,  4327])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  516,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4686 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   435,  2405,   216,   228,  5977,   748,   306,  5001,   235,\n",
      "         3701,   175,   183,   515,  2122,   487,   222,  1178,  4231,  3300,\n",
      "          115,   221,   207,  5774,   210,   207,  3107,  1641,   115,   223,\n",
      "          271,   218,   231,   820,   117,   217,  1363,   115,   595,  1704,\n",
      "          117,  6908, 19593,   189,  2522,   216,   253,   300,   292,  3701])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  498,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3705 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1566,   487,   222,  1345,   210,  2419,   595,   263,  2150,\n",
      "         5859,  1563,  3382,   112,   120,  7973,   189,  7947,   115,  2559,\n",
      "          111,   376,   112,   120, 17413,   383,   115,  2559,   111,   376,\n",
      "          112, 28492,  5993,   115,  2559,   111,   376,   112,   117,   321,\n",
      "          323, 16649, 12857,   193,   115,  2559,   111,  1848,  1152,   371])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  442,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5410 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   519,   210,   207,  3854,   211,  4182,   593,   211,   207,\n",
      "        12351,   189,   221,  2381,   189,   210,   207, 22363,   117, 11394,\n",
      "        19460,   195,   115,  7184,   165,   117,   163,   117,   236,  7666,\n",
      "          116,  1244,   117,   230,   578,   830,  2984,   211,   820,   298,\n",
      "          238,   237,  4338,  2102,   222,  1016,   223,   832,  1438,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  556,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5736 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1280,   115,  2254,  7549,   905,  1931,   216,   145,  2381,\n",
      "          443,   105,  2913,   211,  2417,   145, 16375,   216,   261,  2907,\n",
      "          603,  1136,  1835,   907,   105,   221,  2381,   189,   443,  3897,\n",
      "          207, 16375,   115,   105,   247,  6605,  2149,   303,   210,   240,\n",
      "          222,   207,  5859,   388,   117,   105,  2254,  7549,   905,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  554,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5043 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,  1323,   128,   236,  1744,  1151,   115,\n",
      "          820,  4530,   216,   207,   240,   305,   625,  8247,   238,   820,\n",
      "          110,  2148,   210, 28099,  2716,  3079,   301,  1835,   907,  3127,\n",
      "          174,   218,   207,  3854,   216,   306,  8490,   210,   207,  1178,\n",
      "         4231,   115,   207,  1318,   536,  3701,   175,   183,  1853,  8860])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  508,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5078 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   820,   435,  3194,   216,   207,  1794,   394,   210,  1498,\n",
      "          350, 11163,   189,   288,   207,   444,   327,   115,   216,   820,\n",
      "         2743,   261,   219,   207,   519,   210,   270,  4179,   253,  2973,\n",
      "          115,   323, 14052,   189,   226,   256,   238,   324,  2022,   306,\n",
      "          218,   894,   117,   820,   110,  2072,  2231,   236,   279,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  509,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5620 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113,  113, 1431,  128,  213,  775,  115,  820,  244,  227,\n",
      "        3420, 1960,  188,  189,  210,  207, 5859,  534,  212, 1591, 1566,  217,\n",
      "         226,  744,  221,  705,  117,  152,  184, 4019,  126,  128,  217, 1566,\n",
      "         211, 4691,  242,  248,  201,  215,  248,  203,  210,  207, 8913, 1386,\n",
      "         184,  325])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  345,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4193 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   435,   115,   217,   207,   504,   210,  1566,   115,   207,\n",
      "         1498,  2984,   211,   820,   223,  7129,  8116,   221,   728,   253,\n",
      "          820,   292,  1622,   211,  6379,   217,   207,  3701,   175,   183,\n",
      "         2122,  1163,   115,   233,   356, 10899,   219,  7273,   252,   225,\n",
      "         3680,  1459,  1970,   210,   207,   820,   212,   954,   687,   749])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  415,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5556 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  525,  516,  323, 1379,  216, 1667,  326,  219,  271,  211,  207,\n",
      "         687,  749,  212,  213,  207,  256,  210, 2122,  294,  211,  525,  516,\n",
      "         111,  146,  112,  111,  200,  112,  115,  237, 1588,  211,  485,  237,\n",
      "         110, 6248,  303,  110, 2819,  238,  207,  687,  117, 1954,  117,  162,\n",
      "         117, 3195])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  846,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5171 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   435,   115,   126,   113,   113,  1289,   128,   152,   184,\n",
      "         4934,   126,   128,   213,  9416,  8187,   177,   331,   207,   832,\n",
      "          346,  1540, 15835,   569,   750,  1540,   115,   525,   516,   111,\n",
      "          146,   112,   111,   200,   112,   386,   227,   960,   216,  1425,\n",
      "          550,   213,   207,   256,   219,   578,   211,  1425,   231,   550])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  875,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5589 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5979,  5415,   452,   117,   447,   117,   236,  1195,   117,\n",
      "         1280,   115,   894,  3456,   216,   820,   110,  1229,   210,   207,\n",
      "          954,   687,   223, 20173,  1724,  4527,  5324,   175,   495,  9193,\n",
      "          487,   222,   207,   944,   217,   213, 19830,   452,  4886,   210,\n",
      "          260,   687,   262,   110,   163,  3300,   211,  6379,   217,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  912,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5060 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3865,   346,  1491,   210,   482,   212,   266,   244,  1564,\n",
      "          218,   207,   450,   396,   759,   283,   331,   894,   212,   231,\n",
      "          832, 26197,   189,   244,   749,   210,   207,  1670,  3854,   115,\n",
      "          450,   396,   759,   160,   160,  1185,   116,  1323,   115,  2623,\n",
      "          115,  3725,   116,   912,   120,   239,  1064,   212,   363,   306])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  774,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5814 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   435,   115,   894,   388,   216,   221,  1704,   117,  9159,\n",
      "         9292,  1386,   184,   115,   820,   110,  1909,   115,   249,   307,\n",
      "         2268,   145,   954,  2972,  4354,   211,  1204,   207,  2312,   210,\n",
      "          687,  3409,  1576,  2472,   235,   238,   207,  3854,   115,   207,\n",
      "          954,  4354,   356, 10899,  1790,   207,   506,   210,   525,   516])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  910,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5226 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   820,   110,   801,   128,   117,   105,   159, 10719,\n",
      "         7586,   184,   179,   166,   117,   264,  3406,   210,   350,   871,\n",
      "          236,   350, 12431,  5416,   115,  7835,   150,   117,   199,   174,\n",
      "         4345,   115,  5941,   116,  1456,   111,   199,   174,   682,   117,\n",
      "         1464,   112,   111,   888,  5453,  1396,   112,   111,  2969, 19558])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  984,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5455 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   894,   110,  1909,   115,  1704,   117, 24593,   182,   115,\n",
      "         5809,   211, 11956,   207,  5630,   210,  1704,   117,  9292,  1386,\n",
      "          184,   110,   163,  4354,   552,   233,  2048,   211,   500,   288,\n",
      "          366,   668, 18999,   452,  2063,   210,  2847,  5024,   228,   221,\n",
      "          105,  5758,  1050,   105,   212,   115,   809,   207,   748,   210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1043,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5578 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3480,   117,   105,  9292,  1386,   184,  3629,   160,   282,\n",
      "          117,  1704,   117,  9292,  1386,   184,   323, 15478,   174,   216,\n",
      "          207,  3854,   263,   145,   105,   346,  1160,  1576,   222,   749,\n",
      "          210,   207,   687,   117,   105,   447,   117,   160,   126,   113,\n",
      "         5092,   128,   160,   337,   116,   342,   117,  1296,   115,  1704])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  909,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5435 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   949,   223,  5001,   452,   555,   145,   346,  3297,   117,\n",
      "          105,  3501,   115,  2559,   115,   236,   200,   117,  2144,   115,\n",
      "          207,  2312,   210,   750,   949,  1540,   356, 10899,   115,   213,\n",
      "         1092,   115,  7643,   687,  2122,  1750,   213,  5859,   872,   117,\n",
      "         4014, 13233,   115,   774,   150,   117,  1772,   117,   199,   174])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  673,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5553 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  832,  218,  207,  687,  297,  219, 1387,  211,  247,  237, 1160,\n",
      "         363,  222,  260,  687,  262,  117,  153,  323, 2283,  216,  233,  297,\n",
      "         219,  798,  211, 2526, 1387,  949, 2150,  218,  260,  687,  262,  225,\n",
      "         145, 3297,  216,  481,  219,  740,  211,  207, 1458,  687,  117,  105,\n",
      "        9292, 1386])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  895,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 5179 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  854,  388,  230, 3491,  211,  687, 2122,  112,  111, 3189,  213,\n",
      "         665, 4400, 1876,  118, 1510, 9965,  175,  195,  115, 2559,  212,  231,\n",
      "         649,  112,  117, 1438,  115,  207,  240, 1723,  216,  687, 3409, 1540,\n",
      "         210,  207, 1373,  126,  113,  113, 2047,  128,  210,  207, 3854,  115,\n",
      "         239,  363])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898,  925,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 4055 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 15835,   115,   894,   110,  2231,   236,  1456,   116,  1317,\n",
      "          115,   145,   811,  1653,   218,   226,   240,   487,   222,  2243,\n",
      "          450,   881,   434,   117,  2366,   115,  2559,   115,   236,  1431,\n",
      "          116,  1047,   117,   221,  1468,   115,   894,   465,   227,  1059,\n",
      "          216,   820,   110,   661,   244,  1519,   211,  2277,   207,   687])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1027,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 3844 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1025,   220,   350,   215,  1106,   384,   215,   482,   126,\n",
      "          113,  4448,   128,  1540,   117,   979,   890,   166,   117, 20530,\n",
      "          182,   383,  6089,   115,   689,   165,   117,   163,   117,  6394,\n",
      "          117, 13648,   189, 18076,  2837,   115,   689,   167,   182,  8017,\n",
      "        15613,   111,   149,   117,   148,   117,   158,   117,   169,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  898, 1007,  210, 1007,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 563 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113, 4597,  128,  650,  211,  938, 1328,  189,  281,  207,\n",
      "         612,  259,  215,  211,  485,  237, 1722,  210,  228,  259, 2650,  207,\n",
      "         319,  211,  615,  207,  483,  240,  110,  163,  308,  117, 3324,  166,\n",
      "         117, 4586,  184,  115, 7681,  165,  117,  163,  117, 3146,  115, 1174,\n",
      "         156,  117])\n",
      "Original length: 3319 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113, 19527,   128,   308,  1848,  3883,   211,\n",
      "         2310,   373,   207,   240,   223,   588,   643,  3469,   469,   110,\n",
      "          163,   211,  2310,   212,   867,   211,  4106,   111,   756,   117,\n",
      "         4947,   117,   200,   112,   212,   894,   110,   867,   211,  2310,\n",
      "          759,   111,   756,   117,  6063,   117,   279,   112,   117,   198])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3946 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   327,   263,   211,  5881,  4842,  1092,   211,   902,   225,\n",
      "          324,  1256,   189,   117,   233,   223,   207,   832,   115, 10190,\n",
      "        10690,   210,   226,   350,   406, 18210,  1286,   216,  1764,   207,\n",
      "          421,   210,   820,   759,   117,   146,   117,  2148,   213,   207,\n",
      "          759,   820,   759,  2913,  4392,   217,   832,  3427,   316,  1748])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4207 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 20553,   195,  1251,  1158,   115,   229,  3704,   805,   350,\n",
      "        14200,   189,   211,   239,   378,  1625, 14114,   352,   117,   221,\n",
      "         1468,   348,   115,   207,  2129,  6136,   210,  2015,   110,   163,\n",
      "         3531,   406, 18210,  4551,  1197,  1472,   223,   207,  1472,   110,\n",
      "         2664,   222,   105,   327,   116,   487,  1632,   117,   105,   392])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4548 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   305,   549,   207,  3497,   211,  2310,   207,   820,   470,\n",
      "          552,   115,   213,   308,   211,  3248,   324,   470,   115,   207,\n",
      "          240,   297,   247,   211,   465,  4471,  1067,   207,  3497, 14680,\n",
      "          189,   115,  2076,   115,   284,   145,   449,   870,   238,   216,\n",
      "         3918,   218,  8939,   117,   126,   113,   113,   337,   128,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5199 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1637,  218,  595,  297, 2768, 1263, 6419,  210,  241,  894,  110,\n",
      "         872,  115, 1724, 2834, 5659,  207,  409,  632,  218, 8939,  217, 2167,\n",
      "         213,  392, 1472,  117,  207,  678,  449, 3497, 5429,  226,  519,  117,\n",
      "         321, 3020,  210,  696, 3664,  115, 2418,  117,  166,  117,  350, 5329,\n",
      "         643,  535])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4464 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   356, 10899, 21688,   207,  1694,   210,   207,   251,   632,\n",
      "          218,  2353,   213,   207, 21245,   117,   105,   126,   113,   113,\n",
      "          478,   128,  4587,   150,   117,  1772,   117,   199,   174,   236,\n",
      "          156,  8782,   117,  8939,   249,   207,   434,   211,  4168,   207,\n",
      "         6419,   233,  4163,   211,  3177,  4122,   222,   207,   816,   210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4753 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   894,  1069,   216,  1416, 14862,   383,  5341,   211,  2061,\n",
      "          595,   110,   163,   470,   552,   115,   555,   207,   522,   643,\n",
      "          325,   115,  2353,  5710,   207,  4551,   850,   210,   207,  1416,\n",
      "          210,  1864,   658,   449,   251,   117,  1015,   115,   894,  3456,\n",
      "          216,   595,   110,   163,   470,   126,   113,   113,   556,   128])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4682 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113,  113,  345,  128, 8245,  643,  109,  901,  166,  117,\n",
      "        8245,  115, 7672,  165,  117,  163,  117, 5737,  115, 6286,  115, 1973,\n",
      "         156,  117,  149,  174,  117,  199,  174, 5536,  115, 2628,  163,  117,\n",
      "         756,  117, 5073, 2758,  111, 1474,  112,  111, 1108,  264,  417, 1935,\n",
      "        1044, 4058])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4039 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   599,   592,   210,   145,   264,  1119,   223,   211,\n",
      "         6679,   213,   237,   839,  2815,   211,  8939,   223,  8625,  4885,\n",
      "          117,   126,   113,   113,   912,   128,   207,  4201,   881,   110,\n",
      "          163,   273,   213, 11509,  1197,  1907,   226,   564,   969,   117,\n",
      "          221,  1468,   115, 11509,  1197,   560,   207,  2015,  6225,   110])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2413 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5859,   325,   116,   212,  4296,   223,   227,   145, 14862,\n",
      "          383,   256,   117,   894,   244,   227,  2041,   145,  2499,   115,\n",
      "         9099,  2886,   210,  5859,   534,   213,   270,   867,   211,  2310,\n",
      "          115,   310,  1625,  1968,   145,  4533,   577,   904,   210,   331,\n",
      "          595,   110,   163,  5859,   388,   111,   212,   239,   165,   173])\n",
      "Original length: 59 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5233,   150,   117,  1772,   117,   199,   174, 15838,   115,\n",
      "          113, 15838,   120,   641,   165,   117,   163,   117,  6394,   117,\n",
      "        13648,   189,  3164,   115,   113,   113,   198,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 342, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4510 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113, 18076,   128,  2231,   528,   212,   308,\n",
      "          595,  2572,  9820,   171,   469,   111,   105,  2572,  9820,   171,\n",
      "          105,   112,  1125,   226,   347,  3187,   216,   894,   350,   189,\n",
      "         1760,  1110,   213,   116,  2783,   115,   521,   117,   212,   350,\n",
      "          189,  1760,  1110,   213,   116,  2783,   410,   115,   521,   117])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 342, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4505 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5508,  3064,   213,   683,   210,   207,  3465,  5859,   325,\n",
      "          111,  4196,   166,   212,   953,   112,   115,  6124,   210,   616,\n",
      "          213,   683,   210,   207,  3465,  5859,   325,   111,  2481,  1420,\n",
      "          112,   115,   105,   213, 13939,  2043,   189,   211,  3017,   105,\n",
      "          213,   683,   210,   207,  3465,  5859,   325,   111,  2481,  2014])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 342, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4720 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3883,   213, 16587,   175,   350,   189,  1760,   212,  2572,\n",
      "         9820,   171,   247,   678,   466,  3696,  3883,   211,  2990,   207,\n",
      "          231,   110,   163,  1909,   528,  5415,   235,   207,   444,  5859,\n",
      "          327,   213,   226,   256,   117,   207,   240,   261,  4580,   225,\n",
      "          350,   189,  1760,   110,   163,   867,   211,  2990,   207,   594])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 342, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4612 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1909,   528,   487,   306,   145,  3619,   311,  2859,   216,\n",
      "          207,  1909,   110,   163,  2664,   222,   228,   586,   223,   480,\n",
      "          117,   321,   211,   195,   189,   105,   162,   105,  1143,   115,\n",
      "          521,   117,   166,   117,   356, 13462,   179,   175, 14652, 11041,\n",
      "         6406,   115,   521,   117,   115,  8848,   150,   117,  1772,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 342, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4442 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1152,   371,   311,   530,   460,   216,  6368,   189,   211,\n",
      "         2990,   207,  1814,   216,   207,   588,   110,   163,   818,   246,\n",
      "          221,  1545,   225,  1016,   221,   225,  1975,   818,   117, 11938,\n",
      "        20564,   171, 11624,   117, 10964,   117,   535,   117,   166,   117,\n",
      "        16773,   178,  2582,  1610,   117,   115,  6333,   165,   117,   163])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 342, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4987 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2572,  9820,   171,   110,   163, 10104,   325,   470,   152,\n",
      "          184,  2967,   126,   128,   552,   105,   237,  5859,   663,  7358,\n",
      "          174,   238,   327,  2511,   297,  1591,   220,   931,  5304,   189,\n",
      "          105,  5504,   757,   164,   117,   166,   117,   115,   521,   117,\n",
      "          166,   117,   151,   190,   175, 26937,   179,   171,   115,   521])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 342, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4591 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   266,   218,   207,   522,   986,   221,   145,  4990,   213,\n",
      "         9018,   235,   226,   325,   117,   105,  8365, 20386,   189,   258,\n",
      "          118,   282,   117,  2572,  9820,   171,   110,   163,   470,   242,\n",
      "          248,   200,   210,   207,  3465,  5859,   325,   217,  5508,  3064,\n",
      "          111,  2481,   166,   112,   115,  3750,  5508,  3064,   111,  2481])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 342, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3574 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 11628,  2295,   126,   113,   113,   509,   128,   152,   184,\n",
      "         5045,   126,   128,   213,  3465,   115,   207,  1877,   210,   145,\n",
      "          388,   217, 11628,  2295,   225,  3588,   316,  2718,   244,   119,\n",
      "          111,   198,   112,   145,   480,  3653,   210,  2902,   288,   145,\n",
      "         1179,   316,  1389,   120,   111,   199,   112,   207,   588,   110])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 342, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 145 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  233,  223,  457, 1677,  212, 8739,  174,  216,  207,  867,  217,\n",
      "        1152,  371,  218,  350,  189, 1760,  223,  614,  126,  113,  113,  846,\n",
      "         128,  212,  207,  256, 1629,  117,  254,  119,  198,  118,  204,  118,\n",
      "         641,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "Original length: 59 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5110,   150,   117,  1772,   117,   199,   174, 16139,   115,\n",
      "          113, 16139,   120,   641,   165,   117,   163,   117,  6394,   117,\n",
      "        13648,   189,  7294,   115,   113,   113,   198,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3683 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113, 17943,   128,  2231,   212,   308,   226,\n",
      "         5271,   687,   347,  3335,   303,   210,   237,   832,   793,  3854,\n",
      "          211,  4034,   207,   907,   210, 11420,  5498,   115,   283, 12517,\n",
      "          189,  2172, 11420,  1990,   212, 11420,  1161,  1953,   115,   229,\n",
      "          244,  1821, 15772,  9023,   407,   213,  1360,   338,   117,   820])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3631 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  820, 4293,  216,  207,  542,  210,  585,  569,  894,  223,  489,\n",
      "         242, 2069,  189,  111,  146,  112,  111,  198,  112,  115,  111,  146,\n",
      "         112,  111,  199,  112,  115,  212,  111,  146,  112,  111,  204,  112,\n",
      "         126,  113,  113,  203,  128,  210,  207, 5879,  915,  116, 3659, 1119,\n",
      "         117, 6667])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4397 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   206,   128,   152,   184,  2758,   126,\n",
      "          128,   105,   213,  3470,   235,   207,   889,   369,   211,  1204,\n",
      "          585,   115,   207,   483,   240,   311,  1599,   221,  1377,   207,\n",
      "         2148,   284,   387,   213,   207,   759,   211,   207,   464,   335,\n",
      "          244, 21881,  3730,   218,   588,   110,   163,  6378,   117,   105])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3765 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6563,   117,   200,   552,   820,  1769,   207,  1874,   211,\n",
      "         1204,  1038,   585,   569,   894,   115,  1954,   117,  1101,   115,\n",
      "        11352,   150,   117,   199,   174,   236,  4434,   115,   212,   247,\n",
      "          227,  1564,   434,   211,   635,   270,  1151,   115,   207,   240,\n",
      "        11742,   211,  1800,   216,   820,   110,  2148,   210,  5879,  5859])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3899 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1782,  115,  207,  240, 1723,  820,  247, 1096,  211,  468,  145,\n",
      "        4191, 4474, 2419,  216,  207,  542,  210,  585,  242,  142,  743,  116,\n",
      "        5209,  111,  146,  112,  111,  204,  112,  223,  489,  213,  226, 5859,\n",
      "         683,  256,  117,  588,  165,  715,  185, 2977,  110,  163,  867,  223,\n",
      "         614,  222])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4232 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3985,   211,  1790,   207,   998,  5517,   118, 19697,  5707,\n",
      "         1346,   893,   117,   321,  1727,   116,  2554, 12879,  1610,   117,\n",
      "          166,   117, 10894,   383,   115,  7072,   165,   117,   163,   117,\n",
      "         6097,   115,  5297,   115,  1342,   156,   117,   149,   174,   117,\n",
      "          199,   174,  6996,   115,   613,   163,   117,   756,   117,  8848])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4574 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1374,   210,  9546,   395,   115,   212,   111,   202,   112,\n",
      "          207,  2922,   302,   210,   207,  1298,   265,   213,   435,   235,\n",
      "         1621,  2865,   913,  1302,   117,   159,  3905,  1554,   115,   521,\n",
      "          117,   166,   117,  4557,   213,   189,   117,   535,   117,   210,\n",
      "         2795,   115,  4043,   150,   117,   200,   174, 23256,   115, 12084])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4861 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   159,  1113,   207,   240,  1723,   216,   115,   728,   343,\n",
      "          145,  3425,   210,   207,  5104,  1674,  3922,   213,  2009,   210,\n",
      "          820,   115,   392,  1674,   244,   227,  2592,  3985,   211,  7095,\n",
      "          207,  1591,   210,   885,  2148,   210, 19697,  2317,   117,   152,\n",
      "          184,  7154,   126,   128,   330,   223,   237,   105, 28313,   105])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3073 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   159,   466,   736,  2511,   207,  4877,   881,   110,   163,\n",
      "          584,   332,   847,  3743,   189,   207,   240,   110,   163,  1355,\n",
      "         1438,   117,  7979,   172,   185,   115, 10499,   150,   117,   199,\n",
      "          174,   236,  3724,  4214,   158,   117,   203,   117,   467,   207,\n",
      "          578,   658,   210,   392,   894,   110,  1689,   211,   324,  2621])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 416, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1968 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  3604,   128, 21964,   189,   175,   115,\n",
      "          881,   865,   119,   820,   213,   392,   687,   347,  3728,   115,\n",
      "          229,   292,  1609,   217,  7795,   504,   213,   207,   354,   265,\n",
      "          483,   240,   217,   207,  3426,   483,   210,   350,   871,   115,\n",
      "          615,   238,   145,   371,   210,   216,   240,   115,  4176, 23393])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4828 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  207,  530, 1641, 3684,  207, 1457,  210,  800, 1604,  222,  126,\n",
      "         113,  113,  203,  128, 1338,  320, 6152,  117,  207,  889,  397,  211,\n",
      "         207,  483,  240, 5839,  216,  244,  207,  309,  210,  226,  615,  244,\n",
      "         227,  213, 1059,  117,  145,  117,  207, 1840,  115,  207, 3883,  211,\n",
      "        2310,  115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4537 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   331,   211,  1088,   239,   997,   663,   210,  2876,   235,\n",
      "         2969,   126,   113,   113,   258,   128,  1457,   213,   477,   116,\n",
      "         4470,  1604,   215,   331,   211,  1232,   145,   353, 20151,  2010,\n",
      "         1834,   213,   229,   237,  1604,   477,   297,   219,   677,   211,\n",
      "          616,   220,  1220,  1604,   687,   115,   309,   211,   207, 15198])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4727 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  4176,   147,   117,  4519,   715,   211,   147,   172,   185,\n",
      "          175,  2776, 12837,   577,   146,   117,  8319,  1522,   115,   777,\n",
      "          589,   415,   115,  1344,   115,   236,   199,   111,   105,   221,\n",
      "          799,  1996,   115,   153,   247,  5617,  2017,   207,  1604,  6152,\n",
      "          227,   211,  3148,  2969,  3122,   242,   525,   433,   173,   116])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4343 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   240,   305,  1231,   216,   330,   223,   230,  2666,\n",
      "         2886,   210,   207,  5859,   534,   225,   267,   211,   832,   818,\n",
      "         2003,   218,  1786,   525,   117,   111, 17937, 13023, 15201,  2259,\n",
      "          211,   207,   483,   240,   115,   777,   557,   198,   115,   722,\n",
      "          115,   236,   279,   116,   337,   117,   112,   207,   483,   240])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4046 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  207,  371, 1092, 1319, 2001,  216, 1152,  371,  246,  614,  115,\n",
      "         207,  483,  240,  213,  239,  760,  368,  528,  263,  763,  119,  226,\n",
      "         240,  249,  230,  585,  211, 1025,  331,  126,  820,  110,  128, 2148,\n",
      "         247,  220, 2865, 4991,  117,  552,  207, 3122,  212, 1457,  210, 1604,\n",
      "        3610, 3052])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4468 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,   973,   126,   128,   105,   233,   223,   145,\n",
      "         9240,   968,   210,  1134,   216,  2886,   189,   218,  4839,   244,\n",
      "          227, 17511,   117,   105,   354,   265,   166,   117, 27353,   535,\n",
      "          117,   115,  5209,   165,   117,   163,   117,  4862,   115,  5092,\n",
      "          115,  1244,   156,   117,   149,   174,   117,  3643,   115,   743])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4587 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  270,  639, 5859,  417,  297, 1630,  225,  207, 1251, 1286,  755,\n",
      "         218, 2353, 1625,  268, 1184,  216, 1286,  117,  117,  117,  117,  117,\n",
      "        1130, 1786,  347,  213,  207, 2012,  637, 1763,  211,  247,  255, 7378,\n",
      "         115,  233,  223,  969,  216,  626, 6370,  207, 1786,  249,  255, 2104,\n",
      "         213, 5791])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4915 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 12521,   115,  5973,   165,   117,   163,   117,   236,  9053,\n",
      "          116,   774,   111,  2561,   189,  1396,   112,   111,  2937,  1061,\n",
      "          112,   120,   321,   323,   447,   117,   236,  9208,   111,  1408,\n",
      "          105,   230,   710,   211,  9257,   207,   250,   110,   163,   643,\n",
      "          211,  2612,   392,  1265,   225,   207,  4320,  3282,   210,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5342 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   608,   217,  7012,   213,  1604,  3122,   115,   212,   115,\n",
      "          221,   735,   115,   207,   250,   249,   484,  7893,  3183,   225,\n",
      "          267,   211,   207,   489,  1268,   210, 25466,   195,   115,   213,\n",
      "          332,   552,   242,   207,   477,   325,   233,   223,   560,   225,\n",
      "          353,   268,  1791,   207,   644,   210,  1016,   115,   229,   223])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5378 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,  1140,   115,   152,   184,  4564,   126,   128,   207,\n",
      "         1786,   249, 10259,  1701,   434,   115,   229,   233,   249,  5617,\n",
      "         2031,   115,   211,  6679,   207,  3122,   212,  1457,   210,   800,\n",
      "         1604,   117,   233,   249,   236,  1729,  7210,  2969,  3122,   126,\n",
      "          113,   113,   895,   128,   212,   236,  1729, 10830,   174,   210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5480 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  213, 5501,  115,  207,  240, 2081,  385,  211,  522,  986,  110,\n",
      "         105, 5859,  585,  105,  215,  407,  207, 3795,  105,  309,  691,  585,\n",
      "         117,  105,  207, 5501,  240,  407,  207,  425,  105,  585,  105,  307,\n",
      "         213,  445,  225,  207,  643,  210,  207, 1786,  211, 6679,  207, 3648,\n",
      "         818,  117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2617 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,   207,   530,   256,   115,   820,   110,   470,   242,\n",
      "          207, 10104,   325,   292,  1073,  9811,   696, 26104,   252,  2001,\n",
      "          211,  1915,   522,   585,   117,   696,   292,   270,   470, 13053,\n",
      "          218,   367,  1041,   210,   207,  1313,   240,   115,   626,   152,\n",
      "          184,  5618,   126,   128,   145,   904,   210,   331,   126,   113])\n",
      "Original length: 1562 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  528, 2231,  210,  273,  222,  894,  110,  867,  217, 1152,  371,\n",
      "         889,  207,  595,  115, 4452,  110,  163, 2697, 3831,  243,  115,  156,\n",
      "         117,  156,  117,  147,  117,  115,  223,  145,  488,  548,  243, 1690,\n",
      "         213, 1205, 3692,  115, 7123,  212,  207,  706,  210,  145, 8661,  195,\n",
      "         116, 2391])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2837 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  207, 1245,  116, 2481,  759, 3503, 2581,  210, 1557,  774,  116,\n",
      "         556,  111, 6124,  210,  616,  112,  115,  774,  116,  554,  111, 5508,\n",
      "        3064,  112,  115,  774,  116,  508,  111,  558, 1131, 2093, 1275,  112,\n",
      "         212,  774,  116,  509,  111, 1975,  164,  195,  235, 1236,  112,  210,\n",
      "         207, 7123])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4586 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  421,  217,  270,  388,  223,  216,  335, 2075, 1205, 6195,  547,\n",
      "         294,  211,  212,  213,  295,  225,  145, 1393, 3392, 1286,  210,  958,\n",
      "         115, 1166,  212,  264,  251,  117,  350, 3475,  110,  163,  914,  434,\n",
      "        8407,  189,  225,  145,  629,  325,  601,  218,  207, 3666,  222,  589,\n",
      "         202,  115])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4581 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  213,  207, 1631, 4619,  401,  179,  256,  115,  207,  588,  503,\n",
      "         227,  629,  301, 5818,  264,  347, 3459,  212,  207,  550,  246,  227,\n",
      "        1810,  391,  207,  729,  210,  207,  256,  117,  207,  550, 2645,  174,\n",
      "         213,  207, 1158,  210, 1273,  116,  729, 7223,  117,  293, 3189,  211,\n",
      "         212, 3284])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4958 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   522,  5859,   534,   115,   207,   894,   110,   704,   297,\n",
      "         1528,   219,  2700,   238,   548,   117,   865, 19992,  8178,   115,\n",
      "          213,   207,  5041,   171,  6611,  1386,   188,   256,   115,   783,\n",
      "          145,  1152,   210,   522,   256,   266,  5964,   264,   347,  3459,\n",
      "          117,   105,   207,  1313,   240,   296, 10329,  1067,   249,   863])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1232 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  243,  115,  207,  708,  210,  216, 1205,  212,  207,  704, 2935,\n",
      "        2723,  244, 1280, 2296,  218,  207,  264,  117,  435,  115,  207,  240,\n",
      "        1723,  216,  207,  894,  110,  704, 2260,  281,  207, 1286, 5644,  213,\n",
      "         324,  264, 3031,  212,  697,  117,  217,  392,  853,  115,  207,  894,\n",
      "         110,  704])\n",
      "Original length: 152 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  4323,   128,  2231,   212,   308, 16976,\n",
      "          147,   117,  8528,   189,   115,  1509,   354,   265,   483,   865,\n",
      "          117,  4922,   150,   117,  1772,   117,   199,   174,  2760,   115,\n",
      "          113,  2760,   120,   641,   165,   117,   163,   117,  6394,   117,\n",
      "        13648,   189,  5967,   866,   115,   113,   113,  5967,   866,   102])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4549 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3054,   212,  6406, 15249,   243,   111,   105,  3054,   109,\n",
      "         6406,   105,   112,   212, 27935,  3417,   535,   117,   115,   521,\n",
      "          117,   111,   105, 27935,   189,   105,   112,  1125,   226,   347,\n",
      "          294,   211,   248,   198,   210,   207, 10104,   325,   115,   342,\n",
      "          165,   117,   163,   117,   147,   117,   142,   198,   120,   248])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4147 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   705,  6956,   110,   163,  3237,   246,  1812,   268,  9824,\n",
      "         5217,   110,   163,   310,  2786,  1687,  1682,   805,  4781, 22879,\n",
      "          225,   606,  7682,   212,  2629,   207,   428,   160,  8666,   189,\n",
      "          237,  1588,   211,  1399,  1453,  7682,   117,   213,   463,  1094,\n",
      "          115,   293,  1843,   207,  1453,  7682,   115,  2786,  1687,  1287])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3345 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2481,   153,   210,   207,   396,   759,  3503,   216,   207,\n",
      "          894, 14142,   174,   211,  3019,  7349,   616,   213,   683,   210,\n",
      "          142,   198,   210,   207, 10104,   325,   218,  1446,   235,   207,\n",
      "          820,   238,  4320,   213,   207, 13026,  2565,   327,   217,   207,\n",
      "         2458,   496,   210,  6734,  3447,   338,   216,   244,  4329,   218])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3821 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  2138,   126,   128,   237,  1382,  2775,  1693,\n",
      "          223,   289,   213,   229,   145,   640,   694,   211,   427,   241,\n",
      "          215,   145,  1106,   850,   210,   239,   506,   210,   145,   424,\n",
      "          215,   446,  1199,   238,   145,   382,   430,   215,  2007,   117,\n",
      "         3655,   147,   117, 13109,   189,   115,   126,   113,  4801,   128])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3467 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   769,   190,   183,   110,   164,   117,   521,   117,   166,\n",
      "          117, 14699,  1315,  3717,   115,  8408,   145,   117,   199,   174,\n",
      "        21994,   115, 22230,   158,   117,   156,   111,   162,   117,   153,\n",
      "          117,  1094,   112,   111, 13026,  2565,   336,   534,   142,   203,\n",
      "          116,   910,   116,   199,   111,   146,   112,  1379,   264,  5859])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4724 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1687,   118,  9824,  5217,  1410,   212,   859,   145,  2242,\n",
      "         1717,   399,   117,  1489,   115,   330,   223,   460,   216,   207,\n",
      "         2734,   210, 11083,   288,   207,  2786,  1687,   118,  9824,  5217,\n",
      "         1410,   246,  2529,   306,   207,  2734,   210,  4984,   189,   288,\n",
      "          207,   165,   178,   173,   118,  4383,   189,  1410,   212,  1771])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4848 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  4437,   105,   261,   219,  9116,   252,   218,   207,   105,\n",
      "          525,   210,   744,   117,   105,  6221,   166,   117,  8989, 22075,\n",
      "         1484,  3911,   117, 12885,   110,   169,   115,  7184,   165,   117,\n",
      "          163,   117,  6173,   115,  6597,   115,  1321,   156,   117,   149,\n",
      "          174,   117,   199,   174,  1048,   115,  2377,   163,   117,   756])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4857 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   393,   115,   211,   207,   464,   216,  2786,  1687,   212,\n",
      "          165,   178,   173,  7492,   244, 19670,   252,   238, 14187,  7370,\n",
      "          370,   116,  1410, 14373,   395,   552,   270,  1447,  1219,   960,\n",
      "          603,   211,   490,   597,   217,  6734,  3447,   189,   236,  1410,\n",
      "        14373,   395,   115,   207,  1252,   213,   405,   245,   219,   145])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4686 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  152,  184, 6897,  126,  128, 5859, 1563,  245,  219, 3690,  213,\n",
      "         591,  210,  428, 6557,  117,  233,  245,  219, 5108,  656,  218, 7702,\n",
      "         460,  210, 1035, 1563,  211, 1016,  213,  207,  444,  327,  215,  233,\n",
      "         245,  219, 5108, 1419,  218, 2419,  207, 3986,  210, 1684, 1563,  211,\n",
      "        1016,  117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3929 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   907,  1670,   236,  1410, 14373,   395,  1212,   207,   907,\n",
      "         1670,   236,   370,   116,  1410, 14373,   395,   201,   212,   216,\n",
      "          207,   680,   210,   446,   271,   236,  1410, 14373,   395,   249,\n",
      "          255,  1532,   117,   126,   113,   113,   774,   128,   331,   207,\n",
      "          820,   356,  2587,   392,  2762,   120,   331,   335,   356,  2587])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 478, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2458 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 4014,  116,  218,  116, 4014,  211,  195,  189,  109, 7665, 4821,\n",
      "         115,  521,  117,  115, 4501,  150,  117,  200,  174, 5700,  115, 6520,\n",
      "         111,  199,  174,  682,  117,  722,  112,  111, 2424, 1683,  216,  207,\n",
      "         588, 3350,  225,  207,  996,  592,  210, 2984,  235,  207,  595,  215,\n",
      "         407, 9842])\n",
      "Original length: 1896 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  1133,   128,  6784,   115,   881,   865,\n",
      "          117,  1704,   117, 15825, 18015,   189,   116, 10165, 17153,   185,\n",
      "          189,   115,   237,   237,  1500,  3712,  7341, 15421,   115,  1125,\n",
      "         5859,   470,   242,   342,   165,   117,   163,   117,   147,   117,\n",
      "          142,   198,   111,   722,   112,   111,   248,   198,   210,   207])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 200, 210, 202, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5081 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2558,   207,  2238,  2148,   211,   219,  1377,   212,   213,\n",
      "          174,  3249,  8187,   177,   211,   145,   480,  2030,   145,   595,\n",
      "          443,   249,   227,  1951,   263,   237,  1588,   211,   818,  2779,\n",
      "          117,   105,   148,   183,  1261,   115,   521,   117,   166,   117,\n",
      "        13765,   117,   210,  1824,   117, 28878,   189,   115,  3715,   150])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 202, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4665 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 18015,   189,   116, 10165, 17153,   185,   189,  3503,   216,\n",
      "          741,   249,   255,  8241,   238, 16509,   235,   237,  1500,  3712,\n",
      "         7341, 22568,   213,   244,   173, 14861,   115,   502,  1702,  6962,\n",
      "          115,   212,   216,   741,   249,   323,  2150,   949,   211,   502,\n",
      "         1882,  4401,   221,   145,   519,   210,   502,   474,   218,  6222])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 202, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1185 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  210,  971,  115,  207,  894,  245,  219, 1387,  211, 3194,  216,\n",
      "        1050,  494,  212,  231, 2332, 2044, 2636, 3205, 1382, 2775, 1236, 2086,\n",
      "         115,  221,  705,  221,  207, 2819,  210,  207,  595,  238, 6222,  117,\n",
      "         198,  310,  392,  244, 1000,  216, 2107,  211,  207,  461,  210,  331,\n",
      "         207, 1236])\n",
      "Original length: 1867 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3195,   117,   230,   117,   527,   116, 17535,  4214, 12349,\n",
      "          118,  2725,   181,  4301,  5611,   150,   117,  1772,   117,   199,\n",
      "          174, 23256,   113,   120,   641,   165,   117,   163,   117,  6394,\n",
      "          117, 13648,   189,  4434,  2837,   113,   113,  6891,  2565,   356,\n",
      "        11041,   189,   115,   521,   117,   115,   145, 10510,   469,   115])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 199, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2592 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1152,  371,  224,  219,  614,  372,  207, 4098,  115, 9290,  115,\n",
      "        1804,  189,  211, 8070,  547,  212, 2734,  189,  222,  938,  115,  836,\n",
      "         225,  207, 6378,  115,  253,  220,  115, 1596,  216,  330,  223,  230,\n",
      "        3037,  550,  221,  211,  220,  397,  482,  212,  216,  207, 3782,  257,\n",
      "         223,  575])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 200, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2341 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3245,   266,   123,   117,   117,   117,   123,   522,  3427,\n",
      "         1016,   266,   123,   616,  8432,   644,   123,   336,  9283,   152,\n",
      "          184,  2567,   126,   128,   522,  3427,  1016,   266,   115,   616,\n",
      "         8432,   644,   616,  8432,   644,  7296,   307,   211,  1650,  3970,\n",
      "          216,   244,   370,   176, 22600, 20393,   117,   207,   257,  3614])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2333 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   321,   342,   165,   117,   163,   117,   147,   117,   163,\n",
      "          117,   142, 20789,   111,   149,   112,   111,   202,   112,   117,\n",
      "          316,   109,  1215,   739,   123,   117,   117,   117,   123,   522,\n",
      "         3427,  1016,   266,   123,   616,  8432,   644,   123,  3244,   210,\n",
      "          347,  3245,   266,   123,   117,   117,   117,   123,   570,   920])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2480 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   966,   492,   123,   117,   117,   117,   123,  1598,   189,\n",
      "          123,  1372,   189,   115, 15128,   189,   109,  1328,   189,   123,\n",
      "          933,   109,  5841,   210,  1372,   189,  3245,   266,   123,  6844,\n",
      "          123,   336,  9283,   152,   184,  4564,   126,   128, 17185,  6844,\n",
      "          115,   826,   210,  5122,  1303,  6844,   210,   145,  1233,  3244])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2376 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2525,   120,   111,   199,   112,   207,  6902,   547,   210,\n",
      "         1044,  4058,  2525,   115,   215,   652,   212,  2762,   213,  1044,\n",
      "         4058,  2525,   115,   728,  1917,   207,  6616,   245,  2326,   307,\n",
      "          238, 18261,   704,   120,   212,   111,   200,   112,   324,   704,\n",
      "          216,  1305,  1049,  1044,  4058,  2525,   117,   242,   216,  1229])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2854 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  3875,   126,   128,   522,  3427,  1016,   266,\n",
      "          115,   616,  8432,   644,  5609,  1268,   223,   145,  2061,   211,\n",
      "          145,  1408,   216,   145,   382,   832,   616,  8432,   223,  1787,\n",
      "          117,   661,   119,   217,   595,   119,  4176,   154,   117,  8083,\n",
      "          185,   189,   111,  1785,   112,   115, 15406, 20719,   109, 17845])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3404 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 16671,   213,   722,   215,   689,   115,  5961,   179,   175,\n",
      "         6922,  4051,  1809,   239,   639,  7002,  4566,  1739, 18372,  5961,\n",
      "          179,   175,   225,   105, 10817,   116,   230,   189,   175,   105,\n",
      "        15936,   115, 22442,   174, 20132,   301,   213,  9756,   111,   207,\n",
      "          105,  5961,   179,   175,  6922,  5961,   179,   175,   105,   112])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4761 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   606, 15655,   212,  5961,   179,   175,  6922,   247,  3754,\n",
      "          217,  1680,  1152,   371,   222,   207,   550,   210,   207,  4025,\n",
      "          658,   210,   207, 15655,  5961,   179,   175,  1650,   117,   552,\n",
      "          330,   223,   145,  3037,   550,   210,   397,   482,   221,   211,\n",
      "          331,   207,  1650,   210,   207, 15655,  5961,   179,   175,   223])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4032 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   163,   117,   145,   117,   166,   117,   154,   117,  3289,\n",
      "         3034,   115,   521,   117,   115,  9726,   150,   117,   199,   174,\n",
      "         9051,   115,  9568,   111,   206,   475,   682,   117,  1755,   112,\n",
      "          117,  1625,   115,   105,   207,   663,  1682,   213,  9698, 13857,\n",
      "          185,   212,   207,   649,  1287,   242,   233,   223,  3820,   236])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3781 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   233,   223,  5220,   216,  1623,  5961,   179,   175,  1650,\n",
      "          189,   244,   537,   117,   606,   276,   247,  1564,   460,   216,\n",
      "         1739, 18372,  5961,   547,   245,   219,  1514,   213,   145,  2554,\n",
      "         3987,   210,  4193,   189,   115,   283,   159,  9595,   212,  6878,\n",
      "        11364,  4193,   189,   115,   212,   216,  2969, 22684,   235,  4435])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4241 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  8521,   115,   728,  2558,   216,   145,  7002,  4566,  5961,\n",
      "          179,   175, 22442,   174,  2512,   213,  9756,   223,   378,  4025,\n",
      "          115,   221,   233,   223,   227,   969,   216,  1107,   116, 22684,\n",
      "          235,   223,  2959,   188,   215,   597, 10629,   268,  1260,   145,\n",
      "         3467,   117,  1296,   115, 15655,  2596,  5961,   179,   175,  6922])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5018 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  207, 3366, 1372,  265,  115,  105,  894, 4353,  211, 2357,  222,\n",
      "         126,  113,  113,  478,  128,  207, 1372,  210, 2767,  367,  355,  117,\n",
      "         105,  152,  184, 5484,  126,  128,  213, 3245,  266,  115, 1210,  277,\n",
      "         213,  145, 3245,  245,  219, 1405,  555, 2767,  367,  355,  117, 5961,\n",
      "         179,  175])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4508 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,  9524,   475,  1372,  4142,   216,   105,   595,   110,\n",
      "          163,   264,   116,   487,   470,   244,  7542,   252,   218,   522,\n",
      "          266,   117,   105,   207, 11856,   475,  1372,  4142,   216,   105,\n",
      "          595,   110,   163,  2621,   277,   213,   207,   889,   210,   226,\n",
      "          256,  4895,   207,   931,   189,   210,   165,   117,   163,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3836 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213, 10510,   115,   239,   704,   297,   247,   145,  1436,\n",
      "          363,   222,  1044,  4058,  2525,   555,   239,   363,   222,   207,\n",
      "         3534,   210, 15655,   110,   163,   832,   616,  8432,   115,   229,\n",
      "        15655,  3214,   213,  1044,  4058,  2525,   117,   207,  2253,   116,\n",
      "          296,  1372,   111,  1044,  4058,  2525,   112,   297,   420,  1253])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1425 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   217,   207,   788,   853,   115,   606,   207,   867,   212,\n",
      "          207,  1687,   116,   867,   217,  1680,  1152,   371,   244,  1463,\n",
      "          225,   267,   211,   207,   550,   210,  4025,   658,   117, 15655,\n",
      "          110,   163,   867,   217,  1680,  1152,   371,   223,   614,   213,\n",
      "          332,   212,  1463,   213,   332,   225,   267,   211,  5961,   179])\n",
      "Original length: 399 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  9471,   128, 14297, 10670,   115,   881,\n",
      "          865,   117,  7815,   793,   213, 16347, 14546,  4869,  8536, 15530,\n",
      "          213,   207,  5657,   189,   117,   226,  1258,   115,   145,  1186,\n",
      "          236,  3362,  3010,   115,   249,   667,   237,  1500,  3712,  4418,\n",
      "         1341,   117,   310,   233,   246,   211,   185,  2835,   212,   828])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5445 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2038,  115,  207, 1673,  210,  229, 6086,  213,  463,  638,  117,\n",
      "         310,  207,  237, 1500, 3712, 4418, 1186, 1568,  481,  227,  219, 1165,\n",
      "         213,  207,  354,  265,  621,  233,  296,  571,  207, 5295,  110,  163,\n",
      "         672,  115,  212, 7815,  246,  227, 5803,  211, 1769,  207,  418,  210,\n",
      "         207,  329])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5113 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 1960,  126,  113,  113,  203,  128,  793, 2436,  117,  242,  790,\n",
      "         266,  115,  221,  705,  221,  242,  207,  431,  115, 5379,  105,  247,\n",
      "        3989,  677, 9499,  211, 1988,  207,  266,  221,  705,  221,  207,  889,\n",
      "         212,  244,  227,  309,  211, 3517,  709,  117,  105,  346, 4598, 6848,\n",
      "         189, 1610])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4750 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   242,   207,  7815,   116, 25144,  4653,   179,   232,   117,\n",
      "         1951,   226,   223,   237,  5390, 16624, 11498,   173,   117,   207,\n",
      "         7815,   116, 25144,  4653,   179,   232,  3296,   296,   115,   212,\n",
      "          239,  7012,   525,   246,   145,  2542,  3132,  1181,   232,  1733,\n",
      "          211,  4750, 25144,  4653,   179,   212,   239,  6446,   211,   468])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4234 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 14546,  4869,  8536, 15530,   481,   219,  5645,   252,   552,\n",
      "          335,   297,  2059,  9059,   190,   110,   163,  1046,   189,   688,\n",
      "         5508,   195,  2005,   115,   728,  1917,   207,  3653,   210,  5609,\n",
      "         1016,   246,  1569,   350,   117,   207,   393,   262,   210,   207,\n",
      "         8487,  2980,   111,   207,   307,  1985,   112,  7446,   252,   626])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3434 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  8747,  5514,   166,   117,  1044,  4058,   118,  3489,  6787,\n",
      "         1610,   117,   115,  1272,   165,   117,   163,   117,   339,   115,\n",
      "          508,   115,  2938,   156,   117,   149,   174,   117,   199,   174,\n",
      "          556,   115,  2841,   163,   117,   756,   117,  4217,  2450,   111,\n",
      "         1344,   112,   111,   105,   207, 10104,   325,   115,   207,   334])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2669 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  4314,   252,   218,   207,  8487,   908,   117,   203,   226,\n",
      "          358,   216,   532,   247,   211,  1025,   126,   113,  9411,   128,\n",
      "          331,   115,  3644,  1613,   115,   207,  4809,  6124,   222,  7815,\n",
      "          110,   163,  4320,   225,  9059,   190,   213,   207, 14546,  4869,\n",
      "         8536, 15530,   327,  2488,   189,   207, 10104,   325,   117,  5026])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 258, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4039 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1479,  1615,   225,   239,   645,   210,   207,   289,   116,\n",
      "         2393,   736,   117,   206,   153,   465,   227,  3339,   216,   330,\n",
      "          481,  6104,   301,   247,   255,   220,  1481,   116,  2010,   363,\n",
      "          238,   207,  5977, 28099,   175,  2666,   218,   207,  5379,   117,\n",
      "         1434,   417,  3091,   115,   212,   467,   216,   145,  1999,   227])\n",
      "Original length: 2855 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,  2231,   528,   212,   308,  4986,   146,   117, 16514,\n",
      "          115,   483,   865,   119,   595, 14602,   194,   384,   410,   111,\n",
      "          105,  6668,   189,   105,   112,  1125,  1333,   351,   894,  3187,\n",
      "         2581,   210,   207,   384,   410,   469,   325,   115,   522,   212,\n",
      "          264,  1395,   116,   559,  2581,   115,  6127,   213,   683,   210])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 282, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4006 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 4072, 2041,  211, 1150,  156,  189,  173,  823, 2107,  555,  145,\n",
      "        2010, 3237,  174,  235,  736,  115, 3302, 1519,  742, 1399, 1196,  126,\n",
      "         113,  201,  128,  211,  156,  189,  173,  217, 2470,  212,  952,  117,\n",
      "         321,  899,  147,  117,  150,  117,  162,  117,  142, 4253, 2138,  117,\n",
      "         198,  117])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 282, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4554 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   595,  2042,   216,   894,  2863,   142,   199,   210,   207,\n",
      "        10104,   325,   218, 21322,   211, 24369,   207,   444,   327,   117,\n",
      "          716,   301,   115,   595,   126,   113,   205,   128,  2042,   216,\n",
      "          894,   818,  2488,   189,   207, 11750,   195,   325,   115,   350,\n",
      "          871,   110,   163,  5859,   266,   117,   201,   152,   184,  2138])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 282, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4508 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 14602,   194,   117,  1625,   268,   145,   316,   215,   993,\n",
      "          477,   115,   226,   978,   223,   353, 15109,   211,   145,   701,\n",
      "          210, 10155,   175, 11009,   238,   289,   257,   211,   789,   117,\n",
      "          435,   115,   595,   249,   227,   832,   115,   212,   481,   227,\n",
      "         5283,  4327,   115,   216,  1898,   802,   207,  1389,   317,   207])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 282, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4576 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  216,  207,  798, 1148,  116, 2554, 2134, 3124,  213,  207,  156,\n",
      "         189,  173,  671,  246, 2081,  213, 2649,  333,  115,  212,  216,  156,\n",
      "         189,  173, 1956,  233,  211,  207, 1045,  210,  156,  189, 2374,  221,\n",
      "         211, 1459,  211, 2397,  888, 1378,  117,  809,  115,  728, 2597,  241,\n",
      "         480, 6295])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 282, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4399 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  207, 1540, 1810,  213,  595,  110,  163,  245,  722, 4179,  117,\n",
      "         207, 5096,  210,  207,  713,  689, 4026,  115,  420,  115, 2048,  211,\n",
      "         635,  595,  110,  163, 1151,  117,  595, 2641,  249,  227,  832,  220,\n",
      "         284,  210,  889,  213,  207,  759,  229,  481,  625, 1761,  211,  145,\n",
      "        4831,  445])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 282, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2291 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  333,  210,  207,  438,  465,  227, 1915,  145, 3077, 1389,  115,\n",
      "         212,  595,  249,  227,  832,  220, 4512,  620,  211, 7095,  207, 1412,\n",
      "         213,  207,  438,  117,  420,  115,  894,  110, 3883,  211, 2310,  207,\n",
      "         681,  210, 3077,  838,  388,  244,  614,  221,  156,  189, 2374,  263,\n",
      "         230, 3077])\n",
      "Original length: 1996 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   218,   119,  9405,   528,   126,   113,  5051,   128,\n",
      "         9405,   115,   881,   865,   119,   126,   113,   113,   199,   128,\n",
      "          207,   347,   213,   226,   256,   246,   678,   242,   248,   198,\n",
      "          210,   207, 10104,   325,   115,   342,   165,   117,   163,   117,\n",
      "          147,   117,   142,   198,   115,  1557,   201,   212,   404,   210])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5245 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1907,   233,   969,   216,  1590,   522,  5859,   534,  6679,\n",
      "         1093,   818,   307,   343,   216,   818,   249,   207, 16655,   105,\n",
      "         1692,   105,   222,   790,   215,  1093,   354,   265,  2525,   117,\n",
      "          212,   142,   203,   171,   111,   199,   112,   210,   150,  8766,\n",
      "          171,   783,   216,   207,  5859,   534,   244,  6985,   621,   207])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4435 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   420,  3738,   207,   483,   240,   110,   163,   273,   115,\n",
      "         6986,   207,   371,   351,  3128,   115,   212,  2698,   207,   256,\n",
      "          126,   113,   113,   205,   128,   217,   435,   417,  1545,   225,\n",
      "          226,   528,   117,   153,   117,  2791,   152,   184,  1570,   126,\n",
      "          128,   248,   198,   210,   207, 10104,   325,  1907,  2093,   105])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4970 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2525,   503,   227,   602,   207,  1093,   518,   189,   110,\n",
      "         1563,   115,   207,   240,   503,   227,   247,   585,   569,   207,\n",
      "         1093,   518,   189,   110,   470,   117,   321,   689,   165,   117,\n",
      "          163,   117,  6394,   117, 13648,   189,  4494,  2967,   115,   126,\n",
      "          167,   182,   128,   236,   199,   116,   201,   117,   552,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5372 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1151,  6408,   115,   236,   345,   116,   415,   117,   213,\n",
      "          901,   210,   226,  3773,   218,  5172,   115,   233,   223,  4495,\n",
      "          217,  1143,   211, 12141,   435,   207,  3543,  4994,   210,   207,\n",
      "         1229,   210,   105,   818,   115,   105,   626,   330,   223,   230,\n",
      "         1071,  1059,   216,   207,  7070,   918,   110,   818,   263,   105])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5214 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1020,   112,   115,   207,   240,   126,   113,   113,   339,\n",
      "          128,   633,   331,   150,  8766,   171,  2612,   189,   309,   691,\n",
      "          585,   569,   145, 11058,  6387,  2582,  2289,   110,   163,   388,\n",
      "          216,   145,  4320, 11058,  6387,  2582,  2289,  2863,   207, 10104,\n",
      "          325,   218, 15106, 14047,   165,   117,   163,   117, 15889,   189])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5266 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   325,   117,   105, 12004,  1522,   115,  5635,   150,   117,\n",
      "          200,   174,   236,  2396,   117,   221,   213,   207,  3107,   256,\n",
      "          115,   207,   105,   830,   115,  1436,   115,   212,   625,  5465,\n",
      "          363,   105,  8376,   210,   142,   203,   171,   111,   198,   112,\n",
      "          246,  1649,   213,   324,   428,   649,   115,   212,   207,   461])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4978 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1590,  1472,  4504,   372,   207,  1093,  1286,   223,   227,\n",
      "        19670,   252,   552,   207,   790,  1286,   245,   247,   145,  1706,\n",
      "          188,  5221,   210,  4486,   372,   233,   223,  2395,   218,   207,\n",
      "         1093,  1286,   117,   447,   117,   236,  5895,   117,   809,   115,\n",
      "          207,   450,   881,   765,   216,   105,  1590,  1472,   356,   568])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5038 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   234,   213,   105,   145,   388,   115,   105,   228,   216,\n",
      "          105,   145,   388,   105,  8001,   358,   105,   966,   347,   100,\n",
      "          218,   207,   522,   389,   211,  1960,   126,   113,   113,   875,\n",
      "          128,   215,  1403,   145,  2865,   683,   210,   207, 10104,   325,\n",
      "          294,   211,   342,   165,   117,   163,   117,   147,   117,   142])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5382 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 10104,   325,   117,   105, 12004,  1522,   115,  5635,   150,\n",
      "          117,   200,   174,   236,  2396,   117,   623,   211,   207,   450,\n",
      "          881,   115,   105,   145,   683,   210,   207, 10104,   325,   223,\n",
      "          227,  7435,   174,   222,   207,  1373,   210,   237,  1563,   211,\n",
      "          145,   595,   117,   213,   482,   115,   207,   307,   966,   347])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4829 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2191,  3176,   216,   223,  1545,   225,   207,  2803,   631,\n",
      "          386,   227,   111,   372,  1672,   213,  1158,   112,  4888, 29173,\n",
      "          190,   175,   215,  2990,   207,   597,  2803,   631,   115,   400,\n",
      "          207,   597,  2803,   631,  1676,   281,   233,   207,   631,   216,\n",
      "          820, 12526,   218,   207,   165,   117,   163,   117,  1692,   356])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5368 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1234,   105,   854,   388,   223,   487,   222,   237,  4139,\n",
      "         3729, 20717, 23199,   363,   105,   356, 10899,  1204,   585,   343,\n",
      "          207,   818,   249,   145,   105,  1652,   363,   281,   207,   354,\n",
      "          265,   115,   105,   207,  4662,  9774,  5315,   216,   145,   595,\n",
      "         1234,   388,   223,   487,   222,   145,  1093,   363,   356,  1204])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4609 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   323,  1072,   265,   216,   105,  1093,   518,   189,   305,\n",
      "         3897,   207,   644,   210,  1590,  5859,   534,   213,   207,   790,\n",
      "        11990,   105,   111,  2937,  1061,   112,   117,   226,  1091,  4965,\n",
      "          216,  1093,   820,   356,  4691,   242,   207,  5859,   534,   217,\n",
      "         1563,  2150,   213,   207,   790,   327,   117, 11373,   195,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4908 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   236,  7089,   116,   909,   117,  5808,  4103,   110,   163,\n",
      "         2161,   246,   216,  4197,   145,  1093,   595,  3580,   218,   237,\n",
      "         5859,   683,   126,   113,   113,  1291,   128,   145,  1249,   242,\n",
      "         1590,  5859,   534,   297,  1232,   145,   405,  4034,   577,   211,\n",
      "         6196,   576,   548,   217,   275,   818,   115,   229,   297, 16287])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5091 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   986,   213,   145,   602,   242,   126,   113,   113,  1306,\n",
      "          128,   709,   110,   100,   117,   105,   150,   193,   118,   160,\n",
      "          172,   189,   115,  6714,   165,   117,   163,   117,   236,  5033,\n",
      "          111,  3189, 24387,   166,   117,  3921,  4619,   839,  4595,   117,\n",
      "         6394,   117,   115,  6333,   165,   117,   163,   117,  8439,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5127 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113, 6329,  128, 5172, 1700, 3194,  216,  105,  330,  223,\n",
      "         145, 1794, 1207,  210,  110,  353,  489,  820,  115,  110,  105, 2076,\n",
      "         207,  105, 2200,  189,  210,  165,  117,  163,  117,  820,  115,  221,\n",
      "         705,  221,  145,  687,  210,  790,  518,  189,  115,  105,  443,  247,\n",
      "        6047,  207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 516, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3926 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   470,   117,   310,   207,   483,   240,  1568,  7851,  1045,\n",
      "          211,  5140,   211,   542,  2333,   585,   569,   207,  1093,   266,\n",
      "          470,   294,   211,   508,   165,   117,   163,   117,   147,   117,\n",
      "          142, 15579,   117,   426,   117,  1113,   217,   207,   853,   763,\n",
      "          348,   115,   532,  3738,   207,   483,   240,   110,   163,   273])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 498, 210, 498, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2328 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  291,  210,  949,  733,  307,  211, 3580,  126,  113,  113, 1047,\n",
      "         128,  790,  652,  117,  105,  152,  117,  162,  117, 4947,  117,  230,\n",
      "         117, 1191,  116, 8868,  115,  236,  258,  111, 3407,  174,  213, 9655,\n",
      "         117, 3436,  117,  236,  345,  112,  111, 2937, 1061,  112,  117,  207,\n",
      "         594,  386])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 199, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3537 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,  4859,  3298,   115,   154,   117,   820, 27336,   174,\n",
      "         7077,   189,   210,  8548, 14188, 11955,   210,  8531,   115,  7698,\n",
      "          195,   564,  2710,  3664,   189,   115,   521,   117,   115,   212,\n",
      "         7774,   116, 13468,   189,  2710,  3664, 11412,   535,   117,   115,\n",
      "         1979,   226,  5859,   347,   351,   588, 24244,  3646, 15332,   243])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 200, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4449 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,  7791,   115,  8886,   383,   487,   275,   528,   306,\n",
      "          275,  4663,  1038,  2196,   212,  1002,   210,   207,  6168,   327,\n",
      "          213,   226,  1805,   117,   242,   207,   620,   210,   226,   256,\n",
      "          115,   216,   386,   227,  2636,  1446,   235,   275,   528,   117,\n",
      "         3599,   117, 11483,   189,  3182,  8012, 12438,   110,   151,   115])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3738 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1151,   112,   117,   588,   323,  5455,   189,   216,   820,\n",
      "          503,   184,   110,   164,  5343,   207,   599,  4473,  1383,  3267,\n",
      "          210,   207,  1743,   335,  2357,   306,   115,  3189,   207,   898,\n",
      "         3267,  2388,   117,  2096,   115,   207,  4055,   111,   253,   220,\n",
      "          112,   223,  5994, 19117,  4333,   117,  1595,   115,   588,  2913])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3839 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1805,   249,  6069,   961, 10577,  2716, 19875,   213,  2243,\n",
      "          637,   115,  1917,   207,   276,  1059,   207,   853,   217,   111,\n",
      "          212,   384,  5122,  1303,   210,   112,   216,  5140,   117, 24244,\n",
      "         3646, 15332,   110,   163,   600,  1151,   217,  1152,   371,   223,\n",
      "          216,   233,   386,   227,  3557,   145,  5508,   195,   213,   207])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4554 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   588,   323,  4906,   306,   595,  7698,   195,   564,   110,\n",
      "          163,  1598,   211,   485,   217,  2734,   230,   117,   202,   115,\n",
      "          202,   310,   207,   461,   246,  8325,   117,   233,   223,  7680,\n",
      "          331,  7698,   195,   564,   246, 10106,   216,  2905, 26651, 11412,\n",
      "          356,  2433,   221,   145,  2291,   217,   231,  2710,  3664,   189])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3931 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2565, 2703,  189,  115, 6347,  165,  117,  163,  117,  236, 4844,\n",
      "         111,  105,  696,  223,  207,  291,  210,  207, 5052,  110,  163, 1508,\n",
      "        1104,  229,  207, 2015, 6392,  189,  494,  444,  115,  340,  915,  221,\n",
      "         494,  223, 2031, 2834,  213,  207,  839,  560,  117,  105,  112,  221,\n",
      "         588, 1137])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 205, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3843 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2381,   110,   163,   418,   115,   212,   239,   639,   418,\n",
      "          115,   213,   207,  1739,  2495,   115,   211,   219, 12882,   210,\n",
      "          324,  2381,   189,   213,   207,   915,  2495,   117,   216,   223,\n",
      "         3612,   870,   238, 16822,   195,  2960,   235,  3820,   236,  6342,\n",
      "          207,  1016,   303,   210,   316,   117,   213,   775,   115, 24244])\n",
      "Original length: 58 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,  2231,   212,   308,   641,   165,   117,   163,   117,\n",
      "         6394,   117, 13648,   189,  7202,   115,   113,   198,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3555 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   153,   117,  2500,   222,   463,   282,   115,   689,   115,\n",
      "          207,   820,   115,  1377, 25572,   115,   521,   117,   212,   155,\n",
      "          189,   179,   115,   521,   117,   111,  1462,   105,  1377, 25572,\n",
      "          105,   112,   678,   145,   759,   351,   207,   588,   115,  6303,\n",
      "         7517,   115,   521,   117,   111,   105,  6303,   105,   112,   117])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2596 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  2138,   126,   128,   207,  3593,   210,  2295,\n",
      "          225,   145,   438,  1379,   105,   237,  4702,   325,   216,   223,\n",
      "          145,  1106,  2025,   213,  4361,   207,   681,   210,   207,   438,\n",
      "          117,   105,   198, 29065, 12941, 15259,   115,   156,   117,   160,\n",
      "          117,   166,   117, 29065,   115,  9266,   145,   117,   199,   174])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5066 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  207,  307,  105, 3588,  316, 1588,  105,  236,  550,  213,  207,\n",
      "        3757,  223,  207,  236,  109,  164,  438,  117,  221,  763,  348,  115,\n",
      "         207,  588, 1137,  189,  145, 1666,  212, 1726,  438,  225,  236,  109,\n",
      "         164,  117, 6303,  110,  163, 1387,  316, 1588,  115,  809,  115, 1763,\n",
      "         211,  247])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3807 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  5979, 28342,   212,  6984,   301,  8462,   211,  3950,   126,\n",
      "          113,   279,   128,   225,   316,  1016,   218,  1260,   145,  1040,\n",
      "          736,   110,   221,   237,  1395, 13408,  8238,   117,   110,  1882,\n",
      "         1071,  1471,   822,   189,   166,   117,  3634,  8979, 10964,   117,\n",
      "          115,  7033,   165,   117,   163,   117,  1036,   115,  3116,   156])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3811 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 10442,  1641,  1887,   216,   233,   297,  7488,  5402,   211,\n",
      "         9990,   145,   444,   212, 11114,  1477,   317,   603,   117,   809,\n",
      "          115,  3757,   166,   245,  1253,   115,   211,  3337,   215,  2827,\n",
      "          221,   233,   245,   213,   207, 10190,  1641,   117,   203,   126,\n",
      "          113,   416,   128,   148,   117,  3515,  1372,   426,   221,   239])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4001 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   867,   211,  4106,   207,   393,  3515,  1372,   115,\n",
      "          212,   211,  2310,  3757,   153,   115,   229,   223,   236,   684,\n",
      "         1680,   301,  8561,   174,   222,   207,  1372,   210,   213, 14878,\n",
      "          818,   115,   223,  1463,   117,   149,   117,  3515,  1372,   953,\n",
      "         1700,   115,   207,   820,  3620,   211,  4106,  3515,  1372,   953])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 45 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  8922,   157,   117,   163, 13040,   190,   354,   265,   483,\n",
      "          865,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 1820 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   230,   117,   834,   116,  1164,   116, 11546,  2450,   116,\n",
      "         4984,  4301,   641,  5567,   117,  2153,   117, 13648,   189,  8502,\n",
      "          113,   120,   641,   116,   198,   616,  2987,   117,   111,  4693,\n",
      "          178,   112,   160, 15269,   115, 11310,   207,  1364,  1510,   189,\n",
      "          115,   521,   117,   115,   207,  1510,   189,   210,  1364,   212])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 199, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2070 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,   973,   126,   128,  1156,   479,  1681,   115,\n",
      "         2078,   989,   342,   165,   117,   163,   117,   147,   117,   163,\n",
      "          117,   142, 22147,   172,   115,   111,  1154,   109,  1772,   117,\n",
      "          634,   112,  6539,   189,   242,  1067,   620,   479,  1681,  2091,\n",
      "          245,   815,  2078,   989,   115,   142, 22147,   175,  3282,   189])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 200, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2533 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   217,  2924,   119, 14426, 13025,   383,  8368,   115, 12133,\n",
      "          109,  4697,   189,   115,   156,   117,   156,   117,   160,   117,\n",
      "          115,  6094,   115,  4635,   117,  1233,   167,   117,  6797,   115,\n",
      "         5522,  2509,   175,   109,  9454,   175,   115,  3358,   115,  7197,\n",
      "          115,  4635,   117, 14395,   171,   145,   117, 23414,   115,  7065])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4736 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   283,  5567,   171,  1517,   115,  5974,  2759,   802,   324,\n",
      "          105,  3810,   105,   212,  1417,   211,   490,   766,   964,   189,\n",
      "          213,   477,   217,   207,  1916,   238,   207,   594,   210,   207,\n",
      "         2193,   286,   117,   126,   113,   201,   128,   569,   145,   287,\n",
      "          210,   259,   115, 16514,   383,   326,   174,   207,   479,  1681])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2881 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 5567,  171, 1517,  323,  678,  606,  750,  212,  857, 3533,  212,\n",
      "         230,  460, 1152,  371, 3883,  222,  241,  210, 3128,  110,  470,  117,\n",
      "         207,  729,  240, 2752,  392, 3883,  463,  199,  115, 1020,  117,  236,\n",
      "         207,  259,  115,  207, 2505, 5766,  246, 3128,  110,  393,  396, 1497,\n",
      "         117,  428])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4609 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,   270,   296,   564,   210,  1675,   115,  3128,  4293,\n",
      "          207,   729,   240,   835,   145,   105, 28178,  5078,   126,   113,\n",
      "          282,   128,   627,   371,   117,   105,  3128,  1267,   270,  3147,\n",
      "          222,   207,   298,  4523,  1639,  1003,   119,   148,   110,   163,\n",
      "         1620,   180,   111,   123,   112,   117,  5567,   171,  1517,   614])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4689 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   552,   335,  1780,   252,   207,   308,   222,   207,  1338,\n",
      "         1152,   371,  3883,   117,   233,   223,   705,  2624,   216,   145,\n",
      "          257,   356, 10899,   615,   238,   215,  5463,   145,   371,   211,\n",
      "          229,   300,   249,   582,   252,   215,   878,  3803,   237,  3949,\n",
      "          210,  1683,   210,  2067,   115, 10520,   115,   215,  4611,   383])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4849 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1019,   210,  2078,   989,   213,   295,   225,   248, 22147,\n",
      "          172,   115,   212,  1379,  1681,  2091,   690,   207,   989,   244,\n",
      "          221,  3265,   221,   798,   117,   447,   117,   142, 22147,   175,\n",
      "          117,   126,   113,   433,   128,   213,   635,   210,   270,  1372,\n",
      "          115,   207,   479,  1681,  2091,   212,  5567,   171,  1517,  1564])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 206, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2920 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2443,   217, 11628,  2295,   212,  5859,   470,   218,  2419,\n",
      "          237,  1975, 21215,   117,   321,   165,   117,   163,   117,   166,\n",
      "          117,  7709,   195,  2186,   116,   916,   115,   521,   117,   115,\n",
      "         9130,   150,   117,   199,   174, 21994,   115, 20923,   116,  1433,\n",
      "          111,   202,   475,   682,   117,  1697,   112,   111,   207,   558])\n",
      "Original length: 1204 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  528, 2231,  528,  226,  691, 4647,  373,  207,  240,  222,  207,\n",
      "         595,  110,  163,  867,  217,  145, 1366, 2411,  117,  153,  117, 2500,\n",
      "         207,  595,  212,  207,  588, 7477,  110,  163, 2595,  115,  521,  117,\n",
      "         244, 2381,  189,  213,  207, 2458, 8954,  316,  213, 1326, 6411,  117,\n",
      "         207,  595])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3978 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 7477,  110,  163, 2595,  115,  521,  117,  115,  223,  237, 8669,\n",
      "         469,  361,  239,  600,  669,  210,  316,  213,  207,  264,  210, 8669,\n",
      "         117, 7477,  110,  163,  223,  145,  734,  210, 5719,  116, 6603, 2783,\n",
      "         189,  115,  521,  117, 7477,  110,  163,  386,  316,  221, 7477,  110,\n",
      "         163, 5577])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4512 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   117,  1186,  3732,  3707,   195,   210, 10277,   235,   126,\n",
      "          113,   203,   128,   212,   563,   427,   218,  1590,   749,   117,\n",
      "          213,   775,   211,  1186,   804,   115,   207,   749,  2778,   235,\n",
      "         1186,   323,  6406,   213,   207,  5577,   353,  5039,   212,   427,\n",
      "          353,   117,   117,  1186,  3732,   563,  2665,   804,   212,  2606])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5035 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   595,   110,   163,   675,   509,   112,   115,   407,  7477,\n",
      "          110,   163,   888,   301,  1905,  2239,   418,   117,   222,   207,\n",
      "          231,  1107,   115,  7477,   110,   163,  1155,   211,  3177,   237,\n",
      "         1972,   211,   828,  1725,   229,   246,   601,   217,   207,   592,\n",
      "          210,   226,  1641,   212,   229, 15072,  1797,   126,   113,   258])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4986 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   392,  3170,  1596,   216,   115,   217,   207,  2844,   720,\n",
      "         3307,   589,   115,   634,   115,  8954,   126,   113,   368,   128,\n",
      "          963,   236,   392,   584,  2783,   189,  2852,  5691,   107,  2364,\n",
      "          115,   304,   117,   200,  3212,   235,   236,   207, 10418,  1746,\n",
      "         2783,  2490,   115,   217,   760,   555,   589,   115,   634,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5334 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2966,   801,   117,   553,   117, 25346,   115,  7477,   110,\n",
      "          163,   921,   210,  2239,  1418,   635,   115,  1661,   588,   110,\n",
      "          163,  3170,   205,   555,   258,   221,   145, 12794,   210,   207,\n",
      "         1377,   828,   210,   963,   217,   260,  1912,   225,   207,  1887,\n",
      "          210,   207,   567,  1142,   111,   227,   211,   219, 10436,   174])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4842 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   223,  3124,   688,   115,   226,  1355,  1379,   230,   731,\n",
      "          210,  7477,   110,   163,  1974,   327,   643,   115,   210,   331,\n",
      "         7477,   110,   163,   249,   207,   643,   211,  3414,   595,   303,\n",
      "          210,   316,   115,   215,   210,   207,   464,   211,   229,  1016,\n",
      "          249,   255, 16287,   252,   117,  7477,   110,   163,  8954,  1551])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4558 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   213,   145,   256,   656,  1918,   207,  6411,  3427,   804,\n",
      "          325,   115,   207,   354,   265,  1313,   240,   249,  1266,   216,\n",
      "          207,  1809,   210,  2115,   737,   236,   145,   826,   213,   308,\n",
      "          211,  4156,  2349,  1636,   223,   227,   307,   145, 19572,   358,\n",
      "          210,  1016,   115,   233,   323, 13641,   222,   207, 25429,  7093])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4112 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  6535,   296,   211,   207,  2832,   537,   238,   207,  6411,\n",
      "         1313,   240,   115,  4194,   178,   182,   166,   117, 23304, 14237,\n",
      "        16687,   189,   221,   189,   110,   158,   115,  7192,   160,   117,\n",
      "          199,   174,  6550,   111, 20737,   117,  3992,   112,   212,  7272,\n",
      "          184,  2838,  1129,   535,   117,   166,   117,  1639,   189,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4880 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   912,   128,   361,  2027,   189,   252,   142,\n",
      "         8403,   117,   200,   221,   284,   387,   348,   115,   865,  9375,\n",
      "          207,   184,  1736,   252,   211,   142,  8403,   117,   202,   115,\n",
      "          212,  1280,   211,   207,   461,   210,   207,  1701, 11297,   284,\n",
      "          387,   213,   142,  8403,   117,   202,   119,   460,   210,   804])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4900 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   506,   210,   522,  5859,   266,   115,  1696,   210,   229,\n",
      "          244, 14894,   174,   218,   220,  6411,   649,   242,   207,  3427,\n",
      "          804,   325,   115,   244,   809,   961, 18803, 14710,   174,   117,\n",
      "          233,   223,   227,   369,   217,   207,   240,   211,   785,   207,\n",
      "          393,   212,  1567,  7878,   225,   267,   211,   105,   519,   105])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 404, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 1184 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  207,  683,  361,  255, 3690,  115,  105,  207,  240,  224, 5645,\n",
      "         212, 7349,  215,  378, 3430,  115,  228,  683,  117,  105,  342,  159,\n",
      "         117,  163,  117,  142, 8403,  117,  202,  111,  145,  112,  117, 1113,\n",
      "        4718, 2305, 6603,  189,  249, 1528,  280,  239,  256,  115,  242,  207,\n",
      "        6411, 3427])\n",
      "Original length: 68 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,  5073,   150,   117,  1772,   117,   199,   174, 20733,\n",
      "          115,   113, 20733,   120,   641,   165,   117,   163,   117,  6394,\n",
      "          117, 13648,   189,  3724,  2837,   115,   113,   113,   198,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3060 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   308,  1848,   894,   110,  3883,   211,  2310,   225,  1678,\n",
      "          126,   113, 22891,   128,   226,   602,   223,   373,   207,   240,\n",
      "          222,   207,  3883,   210,   588,   969,  5031,  1798,   115,   521,\n",
      "          117,   212,   588, 18798,  5322,   469,   211,  2310,   595,   110,\n",
      "          163,   396,   759,   117,   126,   148,   117,   149,   117,   516])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2313 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,  3532,  2138,   128,   239,   396,   759,   218,\n",
      "         4921,   216,   207,   444,   424,   246,   207,   496,   210,  2796,\n",
      "         1773,   211,  2550,   116,  1412,  2582,   213,   324,  1556,  1472,\n",
      "          117,   203,   213,   635,   210,   270,  1229,   210,   207,   444,\n",
      "          327,   115,   207,   595,  2442,   211,   207,   482,   216, 15889])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3969 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1389,   189,   225,   239, 15889,   189,   117,   282,  1296,\n",
      "          115,   207,   595,  4142,   216,  4693,  2834,  2292,   152,   172,\n",
      "          173,   552,  4693,  3907,   556,   108,   210,   152,   172,   173,\n",
      "          110,   163,   320,   212,   249, 16161,   643,   569,  4041,   152,\n",
      "          172,   173,   704,   117,   279,   207,  9324,   217,   226,  4179])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3529 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   166,   117,  6754,   643,   535,   117,   115, 10379,   150,\n",
      "          117,   199,   174, 10609,   115, 10986,   111,   282,   475,   682,\n",
      "          117,  1344,   112,   117,   126,   113,   113,   337,   128,   213,\n",
      "         1739,   115,   207,   759,   311,  4327,  3985,   889,   115,  1625,\n",
      "          268,  2314,   115,   211,  1596,   330,   223,   145,   384,   388])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4970 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   216,   207,   327,   323,   950,  9203,  1835,  1050,  8435,\n",
      "        10778,   189,   112,   120,  6966,  6318, 14943,   117,   115, 10744,\n",
      "          150,   117,  1772,   117,   236, 11196,   111,   105,   207,  2893,\n",
      "          210,   207,   444,   424,   327,   223,   145,   461,   210,   482,\n",
      "         4431,  3323,   218,   207,  1336,   117,   105,   112,   117,   126])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3637 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  2758,   126,   128,   207,  6116,   881,   249,\n",
      "          512,   327,   643, 10668,   221,   119,   105,   110,   207,  2053,\n",
      "          211,  2807,   405,  1106,   301,   348,   207,  2010,   680,   314,\n",
      "         4714,   235,   241,   210,   289,   110,   163,   316,   117,   110,\n",
      "          105,   321,  5368,  8576,   189,   117, 27175,   172,   117,   166])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4795 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   595,   110,   163,  1744,  1151,   117,   934,   163,   172,\n",
      "          189,  3503,  3865,  5704,   210,  1563,   211,  1092,   115,   233,\n",
      "          386,   227,  4327,   116,   116,  1886,   239,   289,  1113,  1668,\n",
      "          513,   116,   116,   105,   207,   322, 15852,   223,   207, 15889,\n",
      "          443,   249,   597,  1588,   211,  2745,   126,   113,   113,   442])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4296 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207, 10104,  5859,   325,  1907,   233,   223,   145,  2306,\n",
      "          217,   220,   152,   184,  5484,   126,   128,   105,   322,   126,\n",
      "          211,   128, 24369,   115,   215,  2732,   211, 24369,   115,   215,\n",
      "         8091,   225,   220,   231,   322,   215,   652,   115,   211, 24369,\n",
      "          220,   332,   210,   207,   616,   215,  2525,   967,   207,  1298])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4740 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   271,   220,   889,   211,  4327,   226,  2530,   117,   236,\n",
      "          226,   564,   115,   207,   240,   311,  2283,   330,   244,  1696,\n",
      "          117,   595,   110,   163,   650, 11932,   189,  2957,   210,   239,\n",
      "        10104,   325,   470,   351,   606,   894,   117,   146,   117,   969,\n",
      "         5031,   221,   370,   116,  2381,   213,   444,   327,   152,   184])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3757 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  426,  117, 2957,  225, 1678,  361,  633,  207,  276,  110, 1513,\n",
      "         189,  212, 4663, 1744, 1151,  115,  207,  240,  311, 2310,  226,  347,\n",
      "         225, 1678,  117, 1130,  207, 6116,  881,  249,  763, 3531,  216,  152,\n",
      "         184, 6336,  126,  128,  105,  525,  279,  111,  146,  112,  111,  203,\n",
      "         112, 2957])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 517 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   420,   233,   223,  1677,   216,   588,   969,  5031,  1798,\n",
      "          521,   117,   110,   163,   212, 18798,  5322,   469,   110,   163,\n",
      "         3883,   211,  2310,  4196,   153,   212,   290,   244,   614,   225,\n",
      "         1678,   117,   233,   223,   435,  1677,   216,  2550,  5322,   529,\n",
      "          110,   163,   264,   266,  3244,   210,   347,   111,  4196,   426])\n",
      "Original length: 802 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,  2231,   210,   273,   665,   867,   211,  4106,   111,\n",
      "         5166,   117,   362,   112,   207,   588, 15447,  1437,   410,   115,\n",
      "          521,   117,   111, 15447,   112,   249,  3754,   211,  4106,   207,\n",
      "         1929,  2481,   210,   207,   396,   759,   115,   777,   692,   201,\n",
      "          115,   634,   115,   213,   229,   207,  1481,  1131,   595,  3125])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 200, 210, 201, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4181 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  847,  210,  709,  152,  184,  866,  126,  128,  105,  207,  592,\n",
      "         210,  145,  867,  211, 4106,  223,  211, 3371,  117,  117,  117,  207,\n",
      "         384, 3768,  210,  207, 2148,  210,  126,  113,  199,  128,  220, 1840,\n",
      "         117,  117,  117,  211,  264,  145,  388,  306,  229, 1322,  356,  219,\n",
      "         614,  117])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 201, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2974 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 15447,  2421,   216, 23019,   110,   163,  1929,  2481,  2048,\n",
      "          211,   264,   145,  2278,  4333,   388,   552,   207,   460,  2777,\n",
      "          216,   239,  7311,   860,  1333,   223,   227, 28342,   117,   216,\n",
      "          460,  1676,  1298,  1698,  2940,  1350,   218, 23019,   211,   247,\n",
      "          207,  1333,  1629,   215,  8935,   715,   117,   207,  7311,   860])\n",
      "Original length: 1520 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   106,  4974,  4221,  4301,   641,   163,   174,   433,   113,\n",
      "          120,  8703,   158,   117,   167,   117,   199,   174,  7655,   113,\n",
      "          113,   120,   641,   163,   117,   148,   117, 13648,   189,   404,\n",
      "          113,   113,   113,   120,   641,   116,   198,   616,  2987,   117,\n",
      "          111,  4693,   178,   112,   160, 15269,   115, 11344,   213,   665])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 199, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2112 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  152,  184,  973,  126,  128, 1327,  210,  709,  115, 2287,  210,\n",
      "        1045,  237, 2287,  210, 1045, 2410,  211,  145, 1045, 2031,  211,  237,\n",
      "         711,  215,  592,  227, 1948,  218,  115,  212, 1528,  351,  744,  212,\n",
      "         460,  117,  213, 1799,  207, 2287,  210, 1045,  847,  115,  207, 4624,\n",
      "         240,  386])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 200, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2310 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   145,   240,   223,   329,   211,   818,   145, 12446,  1355,\n",
      "          211,  1025,   253,   207,  1877,   210,  1954,   117,   162,   117,\n",
      "         3195,   117,   160,   117,   516,   247,   255,  1624,   117,   966,\n",
      "          492,   123,   629,   417,   123,   687,   872,   123,  2122,   210,\n",
      "         3610,   966,   492,   123,   117,   117,   117,   123,   687,   872])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 201, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2224 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   966,   492,   123,   117,   117,   117,   123,   687,   872,\n",
      "          123, 17002,   217,   687,   347,   123,   336,  9283,   152,   184,\n",
      "         4290,   126,   128,   687,   872,   115, 17002,   217,   687,   347,\n",
      "          145,  4624,   240,   311,   818,   145,   105, 12446,  1355,   105,\n",
      "          213,  1455,   331,   207,  1877,   210,  1954,   117,   162,   117])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2527 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   152,   184,  5045,   126,   128,  1909,  2229,   115, 18467,\n",
      "          190,   847,   373,  1909,  2137,  1743,   245,   219,   571,   115,\n",
      "          233,   311,   219,  1582,   216,   119,   111,   198,   112,   233,\n",
      "          249,   255,   893,   252,   120,   111,   199,   112,   233,   249,\n",
      "          255,  3368,   211,  6482,   709,   212,  1279,   120,   111,   200])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2683 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   237,  2287,   210,  1045,  2299,   307,   343,   145,  1263,\n",
      "         2921,   115,   213,   631,   210,   207,   266,   212,   620,   115,\n",
      "          481,   227,   247,   625,  2318,   207,   376,  1113,   117,  2287,\n",
      "          210,  1045,   223,   207,  1024, 18672,   847,   210,   709,   537,\n",
      "          225,   207,  1887,   210,   230,   709,   236,   241,   117,   966])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3435 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1233,   145,   117, 20573,   210, 16454,   115, 25986, 10700,\n",
      "          109, 20573,   115,   160,   173,   115, 16631,   175,   115,  1896,\n",
      "         9606,   115,   219,   184, 16603,  5750,   210, 16603,  5750,   109,\n",
      "         2932,   189,   115,   160,   117,   147,   117,   115,  4161,   115,\n",
      "          153,   182,   115, 12415,   146,   117, 10015,   210,  7266,  3352])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3453 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113,  160, 2567,  128,  152,  184,  973,  126,  128,  105,\n",
      "         110,  237, 2287,  210, 1045, 2410,  211,  145, 1045, 2031,  211,  237,\n",
      "         711,  215,  592,  227, 1948,  218,  115,  212, 1528,  351,  744,  212,\n",
      "         460,  117,  110,  105, 2709,  166,  117,  687,  115, 1094,  163,  174,\n",
      "         478,  115])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2997 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  111,  199,  112,  207,  257, 7040,  207,  687,  249, 3350,  215,\n",
      "        2329,  211,  325,  222, 1030, 1309,  278,  211,  207,  687,  115, 1724,\n",
      "        1017,  489, 2192, 4162, 1322,  215, 1602, 6010,  195, 1322,  225,  267,\n",
      "         211,  207,  687,  221,  145,  945,  120,  215,  111,  200,  112,  207,\n",
      "         240, 1723])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4316 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  126,  113,  160, 2967,  128,  226,  223, 1545,  225,  207,  716,\n",
      "        1091,  210,  163,  174,  173,  182,  984,  116,  198,  116,  875,  152,\n",
      "         184, 2967,  126,  128,  229,  830,  189,  145,  729,  240,  119,  105,\n",
      "         213,  220, 1230,  347,  861,  238,  207,  376,  818,  115,  207,  240,\n",
      "         245,  500])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4342 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   329,   117,   105,   217,   635,   115,   207,   881,   240,\n",
      "         1118,   211,  2687,   417,   629,   511,   111,   525,  3978,  2311,\n",
      "          111,   146,   112,   112,  2810,  7534,   153,   116,   166,   649,\n",
      "          115,   230,   117, 16736,   186,   117,   230,   117, 24686,   111,\n",
      "         3000,   189,  3272, 11746,   190,   115,  2254,  6643,   185,   535])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3826 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 18467,   190,   115,   207,   881,   240,   110,   163, 12446,\n",
      "         2122,  1355,   245,   219, 11324,   218,   331,   105,   237,  1909,\n",
      "          110,   163,  1743,  3056,   189,   222,   606,   110,   145,  4575,\n",
      "         4099,   126,   113,   113,   113,   422,   128,   212,   223,   444,\n",
      "          211,   207,  3346,   236,  1107,   117,   110,   105, 17039,   715])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4249 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  317,  207,  405,  145, 1896, 9606,  711,  116, 1830,  493,  217,\n",
      "        2810, 7534,  110,  163,  159,  189,  189,  212,  207,  405,  207, 1830,\n",
      "         297,  247,  493,  213,  207, 1345,  210, 2810, 7534,  110,  163,  832,\n",
      "        1975,  818,  117,  584, 1781,  244, 1661,  117,  126,  113,  160, 5042,\n",
      "         128,  207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4517 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 7739, 3436,  117,  236,  342,  117,  111,  888, 2871,  189,  212,\n",
      "        5453, 1396,  112,  117,  126,  113,  160, 7154,  128,  236, 1744, 1151,\n",
      "         115, 2810, 7534, 2022, 8491,  301,  306,  207, 2243,  256,  210,  145,\n",
      "         109,  157, 1010,  166,  117, 2810, 7534, 1610,  115, 5268, 7221,  117,\n",
      "        2153,  117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3338 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1080,   166,   117,   687,   115,  1020,   163,   174,   204,\n",
      "          115,   160,  6336,   115,  6277,   158,   117,   167,   117,   199,\n",
      "          174,  6883,   279,   115,  1020,   163,   174,   204,   115,  6277,\n",
      "          158,   117,   167,   117,   199,   174,  6883,   115,  6767,   111,\n",
      "         3189,  4364,   166,   117, 11375,   110,   163,   521,   115,  7941])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5222 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3447,   189,   115,   521,   117,   115,  7107,   165,   117,\n",
      "          163,   117,  8525,   115,  2623,   163,   117,   756,   117,  5909,\n",
      "         2837,   115,  2531,   156,   117,   149,   174,   117,   199,   174,\n",
      "         7183,   111,  1154,   112,   120, 14346,  4738, 11001,   535,   117,\n",
      "          115,  1392,   117,   166,   117,  1645, 26473,   115,  7994,   165])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4568 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   160, 10114,   128,  2793,   207,  1983,   817,\n",
      "          210,   207,   820,   110,  1909,  2825,   115,   145, 18467,   190,\n",
      "          886,   481,   519,   213,  2618,  6377,  4130,   117,   207,  1313,\n",
      "          240,  2928,   189,   687,   872,   145,   105,   370,  6437,   174,\n",
      "        19630,   757,   105,   816,   210,  1641,   117,   354,   265,  6629])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 422, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2804 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,  9226,   128,  2231,   113,   373,   119,   148,\n",
      "          117,   167,   117,  8886,   383,   115,  6403,  9160,   212,  3290,\n",
      "          115,   881,  1716,   117,  6848,   189,  1381,   469,  1409,   238,\n",
      "          207,   844,   210,  1152,   371,   213,  2009,   210, 11711,   185,\n",
      "        18448,  6848,   189,   115,   521,   117,   207,   483,   240,   126])\n",
      "Original length: 2837 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  4250,   128,  2231,   528,   530,   301,\n",
      "         1282,   212,  5293,   217,  1374,   213,   226,   256,   223,   207,\n",
      "          867,   217,  1152,   371,   210,   588, 11514, 10201,   469,   111,\n",
      "         6047,   221,   105, 11514,   243,   115,   165,   117,   163,   117,\n",
      "          145,   117,   105,   212,   385,   211,   531,   221,   105, 11514])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4842 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  582,  211,  207,  439,  210,  207,  446, 2289,  189,  117,  595,\n",
      "         396,  207,  759,  222,  647,  508,  115, 1020,  115,  212,  207,  184,\n",
      "        2096,  222,  713,  258,  115, 1020,  115, 4921, 4196, 3442,  211,  207,\n",
      "        2148,  210,  405, 1669,  351,  588,  117,  222,  609,  422,  115,  751,\n",
      "         115,  595])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4009 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 12414,  4957,  1610,   117,   115,  5457,   165,   117,   163,\n",
      "          117,   236,  5946,   117,   390,   115,   152,   184,  2138,   126,\n",
      "          128,   105,   110,   145,  3036, 23239,   210,   460,   223,   227,\n",
      "         3985,   211,  1915,   145,   482,   550,   117,   110,   105,  2061,\n",
      "         6943,   166,   117, 12414,  4957,  1610,   117,   115,  9604,   150])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4038 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 25748,  1161, 11328,   189,  1315,   765,   117,   115,  1293,\n",
      "          157,   174,   117,  2153,   117,  7610,   115,  8675,   145,   117,\n",
      "          199,   174, 22583,   115, 18077,   111,  1301,   112,   112,   117,\n",
      "          213,   207,  3107,   256,   115,  1073,   272,  7552,   174,   207,\n",
      "         2738,   394,   210,  1118,   242,   207,  1651,   110, 18320,   235])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4555 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3942,   120,   111,   200,   112,   216,   588,  6513,   174,\n",
      "          213,   405,   317,   595,   212,   231,  3942,   120,   212,   111,\n",
      "          201,   112,   216,   207,  1669,   263,   145,  2003,   363,   222,\n",
      "         1016,   117,   321,  5567,   171,  1517,   115,   521,   117,   166,\n",
      "          117,   249,  9414, 12396,   115,  7351,   165,   117,   163,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4989 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  420,  115, 1791,  221,  595,  110,  163, 7465,  116, 7724, 1522,\n",
      "         325,  388, 1096,  552,  595,  503,  227, 1204,  428,  210,  207, 1754,\n",
      "        1877,  115,  233,  323, 2048,  242,  248,  282,  116, 4232,  217,  207,\n",
      "         376,  853,  117,  152,  184, 5443,  126,  128,  105,  264,  470,  242,\n",
      "         248,  282])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2695 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  4430,   189,   307,   211,  2783,   212, 11451, 11514,   116,\n",
      "         9203,  8954,   115,   233,   223,   705,   116,  2624,   216,   228,\n",
      "          145,   454,   223,   237,  2058,   358,   210,  4648,   145,  7553,\n",
      "          110,   163,   667,  4841,   212,  2651,  3050,   708,   210,   145,\n",
      "         2750,   424,   117,   321,   115,   149,   117,   151,   117,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 337, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 48 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 12381, 20420,   155,   117,  7025,   189,  4853,   193,   354,\n",
      "          265,   483,   865,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Original length: 821 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  4210,   128,   528,   212,   308, 19134,\n",
      "          171,   145,   117,  4595,   175,   890,   174,  2970,   115,   165,\n",
      "          117,   163,   117,   148,   117,   154,   117,   119,   222,   692,\n",
      "          201,   115,   634,   115,  8922, 25785,  5750,  1125,   226,   347,\n",
      "          213,   275,   750,   887,   212,  1457,   221,  3764, 27546,  4306])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3481 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101, 25785,  5750,   223,   145,  4287,   210,   350,   871,   212,\n",
      "         3222,   221,   237, 27370,   212,  4269,  2062,   115,  8412,   301,\n",
      "         1457,   221,  3764, 27546,  4306,   189,   117,  1824,   117,  7233,\n",
      "          117,   160,   198,   117,   894,   698,  2341,   148,   117,  5282,\n",
      "        11323, 13832,   115, 14721,   190, 16236,   577,   115, 14721,   190])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4274 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   203,   128,   487,   222,   392,  1534,\n",
      "          115, 25785,  5750,  1009,   189,  2391,   470,   217,  1322,   117,\n",
      "          201,   126,   113,   113,   204,   128,   894,   642,   428,  1182,\n",
      "         3883,   211,  2310, 25785,  5750,   110,   163,   470,   117,   296,\n",
      "          115,   207,  4269,  3942,   212,  4306,  9363,  3620,   211,  2310])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4147 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  152,  184, 2567,  126,  128,  211, 5818,  237, 5859,  347,  115,\n",
      "         145,  595,  311,  296,  126,  113,  113,  258,  128, 4327, 1566,  211,\n",
      "        1979, 1333,  117,  145,  595,  307,  249, 1566,  253,  300, 2150,  237,\n",
      "        5859, 1563,  117,  321,  151,  117,  155,  117,  145,  117, 5057, 1610,\n",
      "         117,  166])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3844 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1970,  4306,   189,   117,   809,   115,  2007,   503,   227,\n",
      "          935,  2010,   907,   217,   270, 11365,   117, 25785,  5750,   249,\n",
      "         1566,   552,   300,   249,  2423,   832,   216,   207,  3237,   116,\n",
      "        29704,   177,  1286,  3160,   213,  5859,  3159,   117,   147,   117,\n",
      "          388,   426,   119,  3854,   894,  1595,  2533, 25785,  5750,   110])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3970 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  7435,  1275,   310,   126,   207,   128,   721,   210, 13482,\n",
      "          235,   801,   117,   105,  4732,  4827,   184,  2370,   115,  9948,\n",
      "          150,   117,   199,   174,   236,  1348,   117,   126,   113,   113,\n",
      "          416,   128,   206,   809,   115,   211,   247,  1566,   211,  1979,\n",
      "          105,   145,   388,   217,   966,   949,   242,   248,  4456,   111])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4192 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1351,   218,   207,  1128,   215,  1476,   210,   237,   832,\n",
      "         3047,   117,   152,   184,  5045,   126,   128,   105,   314,   145,\n",
      "         4132,   110,  1128,  1563,   115,   110,   126,   145,   595,   128,\n",
      "          356, 10899,   264,   145,   602,   210,   347,   242,  2069,  4456,\n",
      "          111,   146,   112,   117,   105,  6303,   115,   634,   165,   117])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4276 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   165,   117,   163,   117,  6394,   117, 13648,   189,  3643,\n",
      "         5042,   115,   230,   117,   922,  3195,   117, 10379,  2837,   115,\n",
      "          689,   167,   182, 20306, 19099,   115,   236,   113,   200,   111,\n",
      "          163,   117,   148,   117,   158,   117,   169,   117,  5186,   117,\n",
      "          203,   115,   689,   112,   111,  3189,  5793,  2123,   115,   521])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4620 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2067,  5766,  1121,   218, 10341,   301,  3994,   235,   289,\n",
      "         1091,  4197,   220,  1002,   210,   207,  1286,   117,   321,   447,\n",
      "          117,   160,  1076,   111,   105,   595,   246,   227,  1930,   216,\n",
      "          117,   117,   117,   207,  5770,  2759,   749,   213,  9214,  1242,\n",
      "          190,   126,  6457,   128,   212,   520,   252,   212, 27985,   174])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4364 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  3807,   207,  4269,  3942,   212,  4306,  9363,   117,   207,\n",
      "          616,  4363,   110,   650,   211,  3807,   223,   227,   885,  3515,\n",
      "         1153,   211,  2942,   603,   221,   520,  1662,   212,  3701, 11886,\n",
      "          188,   189,   117,  1782,   115,   226,   388,   223,  1629,   117,\n",
      "          426,   117,  1152,   371,   207,  4269,  3942,   212,  4306,  9363])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4700 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   281,   207,  1779,   287,   117,   105,  3458,  1631, 10797,\n",
      "         5859,  1641,   115,   896,   150,   117,  1772,   117,   199,   174,\n",
      "         3643,   115,  4705,   111,   163,   117,   148,   117,   158,   117,\n",
      "          169,   117,   722,   112,   111,  3284, 12186,   178,   188,   115,\n",
      "         7523,   165,   117,   163,   117,   236,  4343,   112,   117,   126])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 478, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4028 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   390,   115,   391,   207,   376,  4972,   300,  7972,   216,\n",
      "          300,  1287,   211,  1262,   384,   347,   293,   300,   246,  1700,\n",
      "         6968,   303,   210,   316,   117,   321,   447,   117,   236,  1288,\n",
      "          117, 25785,  5750,   110,   163,  8863,  2734,   189,   216,   300,\n",
      "          246,  6968,   303,   210,   316,   356, 10899,   126,   113,   113])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 516, 210, 516, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 324 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   867,   211,  2310,   223,  1680,   301,   614,   212,   270,\n",
      "          525,  1185,   867,   217,  1152,   371,   223,   614,   213,   576,\n",
      "          117,   420,   115,   241,   470,   351,   241,   894,   244,  1629,\n",
      "          117,   207,  5318,   210,   207,   240,   223,  2296,   211,  1686,\n",
      "          392,  3883,   212,   226,   256,   117,   340,  1677,   119, 19134])\n",
      "Original length: 4175 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,   113, 19626,   128,   126,   113,  7211,\n",
      "          128, 21560,   184,   115,   154,   117, 11209,   383,  1484,  1463,\n",
      "          595,   110,   163,   352,   217,   145,  1232,   211,  2075,   275,\n",
      "         1071,  1471,   316,   303,   210,   275,  1939,   213,   145,   645,\n",
      "         1640, 29043,   252,  3226, 13096,   117,  3187, 10520,   317,   207])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 202, 210, 204, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4628 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   300,   263,   105,   145,  4792,  4878,  1870,   212,   365,\n",
      "         7270,   211,   355,   212,  2585,   207,  1116,   221,   954,   211,\n",
      "         7659,   216,   207,  4124,   355,  1232,   261,   219,  3350,   222,\n",
      "          213,   237,   489,   812,   117,   105,   595,  3451,   207,  3053,\n",
      "          211,   207, 11209,   383,  1484,   515,   210,  2735,   189,   111])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 203, 210, 204, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4745 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   672,   117,   105,   111,  2937,   213,   856,   117,   112,\n",
      "          207,  3647,   507,   210,   324,   768,   223,   216, 13096,   409,\n",
      "          960,  4849,   211,  1399,  1801,   212,  1116,   355,  3280,   211,\n",
      "          207,   526,   115, 10092,   126,   113,  7292,   128,   373,  1927,\n",
      "          211,   220,  1040,  1499,   216,   323,   944,   211,   325,   222])\n",
      "Original length: 11 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 204, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3290 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  588,  323, 1637,  686,  242,  215,  189,  339,  117, 2367,  222,\n",
      "         207, 1300,  216,  105,  330,  246,  230, 5979,  480,  421,  217, 5444,\n",
      "         207,  388,  115,  105,  310,  207,  729,  240,  503,  227,  785,  216,\n",
      "        1300,  213,  239,  528,  117,  207,  240, 6175,  119,  105,  242,  215,\n",
      "         189, 8712])\n",
      "Original length: 3374 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113,  6631,   128,   126,   113,   113,   199,\n",
      "          128,  2231,   528,   212,   308,  1282,   223,   588, 19867,  7332,\n",
      "          793,   115,   521,   117,   110,   163,   867,   211,  2310,  4196,\n",
      "          628,   212,  5302,   126,  4523,   342,   128,   212,   588, 24196,\n",
      "         9339,   115,   884,   110,   163,   867,   211,  2310,   241,  4196])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4556 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   211,  4327,   237,  1563,   211,   145,  2381,   115,   145,\n",
      "         1121,   210,   142,  1195,   116,   282,   171,   116,   200,   210,\n",
      "          207,   167,   192,  3272,   171,   117,   207,   820,  3618,   216,\n",
      "          335,   247,  3859,  5818,   207,   369,  3159,   217,   606,  3031,\n",
      "          117, 24196,  9339,   678,   145,  1182,   867,   211,  2310,   241])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5087 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2745,   993,  9273,  1181,   115,   696,   249,   233, 12587,\n",
      "          207,  1823,   210,   207,   595,   110,   163,   388,   242,   248,\n",
      "          199,   111,   147,   112,   117,  1625,   115, 19867,  7332,  2042,\n",
      "          216,   728,   253,   207,   820,   247,  3859,   763,   145,   248,\n",
      "          199,   111,   147,   112,   388,   115,   335,  1591,  1566,   211])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 5498 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,  4000,   126,   113,   113,   337,   128,   564,   210,\n",
      "          207,  6480,   317,   207,   820,   212,   207,   588,  6677,   213,\n",
      "          270,  4393,   210,  1067,  1539,  5859,  1563,   213,   207,  1158,\n",
      "          210,   145,   388,   242,   248,   199,   111,   147,   112,   117,\n",
      "        19867,  7332,  2421,   216,  5859,  1563,  1379,  1683,   216,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2772 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1790,   207,  5859,  1563,  1121,   117,   321,   115,   149,\n",
      "          117,   151,   117,   115, 22824,   182,   110,   158, 10761,  5585,\n",
      "         4091,   115,   521,   117,   166,   117, 21535,   190,  6089,   115,\n",
      "         1094,   165,   117,   163,   117,  6394,   117, 13648,   189, 24582,\n",
      "         2567,   115,   230,   117,  1001,  3195,   117,  5422,  2450,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 337, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3987 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1772,   117,   236, 24895,   111,  5453,  1396,   112,   117,\n",
      "          207,   240,   207,   184,  1266,   216,   105,   213,   901,   210,\n",
      "          207,  1699,   592,   210,   248,   199,   111,   147,   112,   115,\n",
      "          226,   240,  3448,   216,   207,  5859,  1563,  1121,   100,  1379,\n",
      "          145,   595, 15807,  2759,   217, 20440,   949,   217,  2581,   210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 368, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4003 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   307,  9434,  9273,  1181,   216,  1687,   395,   207,   430,\n",
      "          116,   640,  4473,  1383,   116,   116,   217,  1363,   115,   372,\n",
      "          207,   430,  1907,   145,   344,   211,   237,   313,   210,   207,\n",
      "          640,   117,   447,   117,   213,  6424, 11518,   115,   207,   240,\n",
      "         1266,   216,   207,  2477,   246,   227,  1225,   221,   207,  5643])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 342, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2939 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,   113,   508,   128,   213,   226,   256,   115,\n",
      "          207,   820,  4327,   216, 19867,  7332,  2473,   221,   207,   313,\n",
      "          217,   213,  3020,  2851,   189,   213, 13617,   235,   212,  3502,\n",
      "          235,   737,   212,  2658,   217,   207,  2851,   117,   213,   239,\n",
      "          887,   221,   207,   313,   210,   213,  3020,  2851,   189,   115])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 404, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3769 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   126,   113,  7079,   128,   167,   117,  3322,   117,   473,\n",
      "          142,  1195,   116,   282,   171,   116,   200,   111,  1631,   634,\n",
      "          112,   117,   258, 19867,  7332,  2042,   126,   113,   113,   415,\n",
      "          128,   216,   242,   226,  1119,   115,  1683,   211,  1563,   210,\n",
      "          145,  2381,   210,   207,   257,  1848,   207,  5822,   215,  1922])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 416, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4797 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   521,   117,   115,  8978,   160,   117,   199,   174,   236,\n",
      "         5631,   112,   117,   217,  1363,   115,   207,  2015,   240,   210,\n",
      "         1409,   545,   216,   145,  2676,   210,  8406,  5006,   189,   212,\n",
      "        19108,   395,   763,   145,   388,   217,  1563,   242,  2015,   110,\n",
      "          163,   459,   171,   218,  3187,   216,   145,  2381,   116,  2676])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 422, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4043 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   207,   231,   588,   115, 24196,  9339,   115,   678,   145,\n",
      "          867,   211,  2310,   241,   210,   207,  4196,   351,   233,   117,\n",
      "          217,   207,   853,  3124,   688,   115,   207,   240,  3119,   216,\n",
      "        24196,  9339,   223,   307,   575,   211,  2957,   210,  2481,   953,\n",
      "          111,  2067,   112,   117,   145,   117,  2957,   487,   222,   207])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 433, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4597 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   497,   211,   207,   167,   192,  3272,   171,   212, 15005,\n",
      "          212,  3701, 21557,   177,   145,   681,   210,  3077,   838,   117,\n",
      "          152,   184,  5531,   126,   128,   211,   264,   145,   388,   242,\n",
      "          207,   167,   192,  3272,   171,   115,   207,   820,   944,   307,\n",
      "         4327,   889,  3388,   105,   207,  3569,   344,   215,  2648,   210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 339, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4007 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  2148,   210,  3515,  4611,   383,   189,   218, 24196,  9339,\n",
      "          211,   213,  3020,  2851,   189,  1591,   207, 10140,   369,   211,\n",
      "          462, 24196,  9339,   225,   105,   207,   382,   620,   217,   229,\n",
      "          126,   233,   128,   261,   247,   211,  2771,   145,  1372,   236,\n",
      "          729,   115,   105, 13570,   115,  4188,   150,   117,   200,   174])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 441, 210, 441, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 28 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 354, 265, 483, 865, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2766 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   528,   126,   113, 20288,   128,  2231,   212,   308,   226,\n",
      "         5271,   687,   347,  3335,   303,   210,   237,   832,   793,  3854,\n",
      "          211,  4034,   207,   907,   210, 11420,  5498,   115,   283, 12517,\n",
      "          189,  2172, 11420,  1990,   212, 11420,  1161,  1953,   115,   229,\n",
      "          244,  1821, 15772,  9023,   407,   213,  1360,   338,   117,   820])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 204, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4576 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   111,   258,   475,   682,   117,  1020,   112,   117,   207,\n",
      "          550,   213,  7830,   145,   867,   228,   221,   226,   223,   227,\n",
      "          331,   207,   595,   261,   126,   113, 13466,   128,  4048,  2774,\n",
      "          115,   310,   331,   300,   215,   741,   223,   575,   211,  1115,\n",
      "          460,   211,   635,   207,   470,   117,   321, 26214,   577,   166])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 205, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4422 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101,  207, 1313,  240,  210, 5879,  249,  227, 1560,  207,  461, 1564,\n",
      "         211,  226,  240,  115,  153,  117,  149,  117,  115,  331,  207,  576,\n",
      "         731, 1249,  271,  217,  213,  142,  541,  116, 2686,  223,  537,  211,\n",
      "        1977,  518,  189,  228,  221,  820,  213,  226,  347,  117,  763,  789,\n",
      "         710,  115])\n",
      "Original length: 12 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 206, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3992 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   215,  1765,   213,   405,   218,   228,  1614,   117,   105,\n",
      "         6667,   117,  3496,   117,   237,   184,   117,   142,   541,   116,\n",
      "         2686,   111,  2937,  1061,   112,   117, 11373,   195,   115,   242,\n",
      "          207,  3647,  1412,   210,   207,  1119,   115,   372,   145,   424,\n",
      "          223,   105,  3372,   117,   117,   117,   213,   405,   105,   218])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 258, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 4274 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,  1015,   115,  2793,   207,  5879,   264,   240,   273,   213,\n",
      "         2838,   115,   207,  3647,  1412,   210,   142,   541,   116,  2686,\n",
      "          115,   212,  2793,   207, 24293,   879,   236,   226,  1925,   210,\n",
      "          207,   417,   115,   207,   240,  1723,   115,   487,   222,   207,\n",
      "          879,   373,   233,   115,   216,   142,   541,   116,  2686,   210])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 282, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 3495 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([  101,   820,   110,   759,  4142,   216,   894,   110,   872,   213,\n",
      "         2080,   207,   907,   210, 11420,  5498,   292,   213,   683,   210,\n",
      "         5879,  5859,   266,   115,  6667,   117,  3496,   117,   237,   184,\n",
      "          117,   142,   541,   116,  1973,   115,   149,   190,  2830,   117,\n",
      "         5879,   264,   266,  3743,   189,   207,  1119,   210,  1779,   278])\n",
      "Original length: 13 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([101, 898, 279, 210, 279, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "Original length: 2423 characters\n",
      "Tokenized length: 512 tokens\n",
      "Tokenized text (first 50 tokens): tensor([ 101, 2415, 1322,  386,  227, 4037,  155,  117,  163,  117,  145,  117,\n",
      "         743,  116, 6628,  111,  199,  112,  120,  233,  311,  462,  145,  350,\n",
      "        2865,  319,  216,  386,  227,  378, 2086,  236,  346,  266,  117,  105,\n",
      "        6879,  166,  117, 6667,  117, 1205,  648,  115, 5137, 6667,  117,  922,\n",
      "        2311,  115])\n"
     ]
    }
   ],
   "source": [
    "# 假設您已經有一個處理過的 DataFrame `final_df` 和 `tokenizer`\n",
    "def check_token_lengths(final_df, tokenizer, max_length=512):\n",
    "    for _, row in df.iterrows():\n",
    "        paragraph = row['Paragraph']\n",
    "        \n",
    "        # Tokenize the paragraph to check the length\n",
    "        inputs = tokenizer(paragraph, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n",
    "        \n",
    "        # Print the length of tokens after tokenization\n",
    "        print(f\"Original length: {len(paragraph)} characters\")\n",
    "        print(f\"Tokenized length: {len(inputs['input_ids'][0])} tokens\")\n",
    "        print(f\"Tokenized text (first 50 tokens): {inputs['input_ids'][0][:50]}\")  # Display first 50 tokens\n",
    "        \n",
    "# 執行檢查\n",
    "check_token_lengths(final_df, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確保文本正確裁剪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens before splitting: 269\n",
      "Number of chunks after splitting: 2\n",
      "Tokens in the first chunk: 256\n"
     ]
    }
   ],
   "source": [
    "def split_text_into_chunks(text, max_length=256):\n",
    "    tokens = tokenizer.encode(text, truncation=False)  # 不進行裁剪，先取得所有tokens\n",
    "    print(f\"Total tokens before splitting: {len(tokens)}\")\n",
    "    chunks = []\n",
    "    \n",
    "    # If the text exceeds max_length, split it\n",
    "    while len(tokens) > max_length:\n",
    "        chunk = tokens[:max_length]\n",
    "        chunks.append(chunk)\n",
    "        tokens = tokens[max_length:]\n",
    "    \n",
    "    # Add any remaining tokens\n",
    "    if len(tokens) > 0:\n",
    "        chunks.append(tokens)\n",
    "    \n",
    "    print(f\"Number of chunks after splitting: {len(chunks)}\")\n",
    "    print(f\"Tokens in the first chunk: {len(chunks[0])}\")\n",
    "    return chunks\n",
    "\n",
    "# Example of splitting a long paragraph\n",
    "example_paragraph = final_df.iloc[0]['Paragraph']  # Example paragraph from the dataset\n",
    "split_chunks = split_text_into_chunks(example_paragraph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 驗證裁剪後的文本內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: opinion [ * 42 ] order wolf, d. j. february 3, 1998 the following analysis is based upon the transcr...\n",
      "Chunk 2: . s. dist. lexis 1155, * * 1...\n"
     ]
    }
   ],
   "source": [
    "def check_split_chunks(chunks, tokenizer):\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        text = tokenizer.decode(chunk, skip_special_tokens=True)  # Decode the chunk back to text\n",
    "        print(f\"Chunk {i+1}: {text[:100]}...\")  # Display first 100 characters of each chunk\n",
    "\n",
    "# Check chunks for a specific paragraph\n",
    "check_split_chunks(split_chunks, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分割訓練集和測試集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Split the data into train and validation sets\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"question\": [entry[\"question\"] for entry in train_data],\n",
    "    \"context\": [entry[\"context\"] for entry in train_data],\n",
    "    \"answer\": [entry[\"answer\"] for entry in train_data],\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    \"question\": [entry[\"question\"] for entry in val_data],\n",
    "    \"context\": [entry[\"context\"] for entry in val_data],\n",
    "    \"answer\": [entry[\"answer\"] for entry in val_data],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'context', 'answer'],\n",
      "    num_rows: 99068\n",
      "})\n",
      "Dataset({\n",
      "    features: ['question', 'context', 'answer'],\n",
      "    num_rows: 24768\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Check final dataset\n",
    "print(train_dataset)\n",
    "print(val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens before splitting: 20\n",
      "Number of chunks after splitting: 1\n",
      "Tokens in the first chunk: 20\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer  # 確保導入BertTokenizer\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "\n",
    "# Function to split text into chunks (max_length 512 tokens)\n",
    "def split_text_into_chunks(text, max_length=256):\n",
    "    tokens = tokenizer.encode(text, truncation=False)  # Don't truncate yet, just get all tokens\n",
    "    chunks = []\n",
    "    \n",
    "    # Split the tokens into chunks of size max_length\n",
    "    while len(tokens) > max_length:\n",
    "        chunk = tokens[:max_length]\n",
    "        chunks.append(chunk)\n",
    "        tokens = tokens[max_length:]\n",
    "    \n",
    "    # Add any remaining tokens\n",
    "    if len(tokens) > 0:\n",
    "        chunks.append(tokens)\n",
    "    \n",
    "    print(f\"Total tokens before splitting: {len(tokens)}\")\n",
    "    print(f\"Number of chunks after splitting: {len(chunks)}\")\n",
    "    print(f\"Tokens in the first chunk: {len(chunks[0])}\")\n",
    "    return chunks\n",
    "\n",
    "# Example usage: Process a specific paragraph from the dataframe\n",
    "example_paragraph = \"This is an example paragraph. It will be tokenized and split into chunks.\"\n",
    "split_chunks = split_text_into_chunks(example_paragraph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 進行tokenization -> 生成 start/end positions -> 將數據移到GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load tokenizer once\n",
    "tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "\n",
    "# Use device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_find_answers(examples, tokenizer):\n",
    "    # Tokenizer 需要傳遞的資料\n",
    "    questions = examples['question']\n",
    "    contexts = examples['context']\n",
    "    \n",
    "    # 用 tokenizer 處理問題和上下文\n",
    "    inputs = tokenizer(questions, contexts, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "\n",
    "    # 處理每個上下文中的範例\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    for i, context in enumerate(examples['context']):\n",
    "        answer = examples['answer'][i]\n",
    "        \n",
    "        # 找到答案在上下文中的位置\n",
    "        start_position = context.find(answer)\n",
    "        end_position = start_position + len(answer)\n",
    "\n",
    "        # 調整位置來符合 tokenized 輸入\n",
    "        start_token_pos = len(tokenizer.encode(context[:start_position], truncation=True, padding='max_length', max_length=512)) - 1\n",
    "        end_token_pos = len(tokenizer.encode(context[:end_position], truncation=True, padding='max_length', max_length=512)) - 1\n",
    "\n",
    "        start_positions.append(start_token_pos)\n",
    "        end_positions.append(end_token_pos)\n",
    "\n",
    "    # 使用 torch.tensor 將位置轉為 PyTorch 張量\n",
    "    inputs['start_positions'] = torch.tensor(start_positions)\n",
    "    inputs['end_positions'] = torch.tensor(end_positions)\n",
    "\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d6498cf99a4c2a9aa50801fe09dc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99068 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af5c626b0e645e2b305645f5377dc63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24768 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load tokenizer once\n",
    "tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "\n",
    "# Use device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def tokenize_and_find_answers(examples, tokenizer):\n",
    "    # Extract questions and contexts from the batch (which are lists)\n",
    "    questions = examples['question']\n",
    "    contexts = examples['context']\n",
    "    \n",
    "    # Tokenize the batch of questions and contexts\n",
    "    inputs = tokenizer(questions, contexts, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "    \n",
    "    # Initialize lists to store start and end positions\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    # For each example in the batch, calculate the start and end positions of the answer\n",
    "    for i in range(len(contexts)):\n",
    "        context = contexts[i]\n",
    "        answer = examples['answer'][i]\n",
    "        \n",
    "        # Find the start and end position of the answer within the context\n",
    "        start_position = context.find(answer)\n",
    "        end_position = start_position + len(answer)\n",
    "        \n",
    "        # Adjust positions for the tokenized input\n",
    "        start_token_pos = len(tokenizer.encode(context[:start_position], truncation=True, padding='max_length', max_length=512)) - 1\n",
    "        end_token_pos = len(tokenizer.encode(context[:end_position], truncation=True, padding='max_length', max_length=512)) - 1\n",
    "        \n",
    "        start_positions.append(start_token_pos)\n",
    "        end_positions.append(end_token_pos)\n",
    "    \n",
    "    # Convert the positions to tensors and add them to the inputs\n",
    "    inputs['start_positions'] = torch.tensor(start_positions).to(device)\n",
    "    inputs['end_positions'] = torch.tensor(end_positions).to(device)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "# 進行批次處理，這次確保處理的是批次資料\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_and_find_answers, \n",
    "    remove_columns=[\"question\", \"context\", \"answer\"], \n",
    "    num_proc=1,  # Parallel processing\n",
    "    batched=True,  # Process in batches\n",
    "    fn_kwargs={'tokenizer': tokenizer}  # Pass tokenizer explicitly\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    tokenize_and_find_answers, \n",
    "    remove_columns=[\"question\", \"context\", \"answer\"], \n",
    "    num_proc=1,  # Parallel processing\n",
    "    batched=True,  # Process in batches\n",
    "    fn_kwargs={'tokenizer': tokenizer}  # Pass tokenizer explicitly\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確認訓練集和測試集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "    num_rows: 99068\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "    num_rows: 24768\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Display the final processed dataset\n",
    "print(train_dataset)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 確認GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設置Trainer並開始訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清除之前train的緩存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"./results\", ignore_errors=True)  # 刪除保存路徑的內容\n",
    "\n",
    "shutil.rmtree(\"./logs\", ignore_errors=True)  # 清除舊日誌\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018023c4ed7a46e5a6752e51add80d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1837, 'grad_norm': 8.810790061950684, 'learning_rate': 1.9468130921619294e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7896, 'grad_norm': 7.659759998321533, 'learning_rate': 1.8929801894918175e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2886, 'grad_norm': 24.44487762451172, 'learning_rate': 1.839254952627046e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2173, 'grad_norm': 0.027751997113227844, 'learning_rate': 1.785529715762274e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'grad_norm': 0.002484299708157778, 'learning_rate': 1.731696813092162e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0013, 'grad_norm': 0.002694260561838746, 'learning_rate': 1.67786391042205e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'grad_norm': 0.0016239694086834788, 'learning_rate': 1.624031007751938e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'grad_norm': 0.0012248739367350936, 'learning_rate': 1.570198105081826e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'grad_norm': 0.0009639933705329895, 'learning_rate': 1.5163652024117141e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.0007894939044490457, 'learning_rate': 1.4625322997416023e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.0006389901973307133, 'learning_rate': 1.4086993970714902e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'grad_norm': 0.21165981888771057, 'learning_rate': 1.3548664944013782e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38bc4e92c1144385a5fc8836a20648d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.17504133010516e-06, 'eval_runtime': 164.808, 'eval_samples_per_second': 150.284, 'eval_steps_per_second': 9.393, 'epoch': 1.0}\n",
      "{'loss': 0.0005, 'grad_norm': 0.0004933038726449013, 'learning_rate': 1.3010335917312662e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.00038034998578950763, 'learning_rate': 1.2472006890611544e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.00028678830130957067, 'learning_rate': 1.1933677863910422e-05, 'epoch': 1.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.00023462538956664503, 'learning_rate': 1.1395348837209304e-05, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.0001954116451088339, 'learning_rate': 1.0857019810508182e-05, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.00015455964603461325, 'learning_rate': 1.0318690783807063e-05, 'epoch': 1.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.00013642210979014635, 'learning_rate': 9.780361757105943e-06, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.00010868001845665276, 'learning_rate': 9.242032730404825e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 9.802278509596363e-05, 'learning_rate': 8.703703703703705e-06, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 7.585759158246219e-05, 'learning_rate': 8.165374677002584e-06, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 5.829895235365257e-05, 'learning_rate': 7.627045650301464e-06, 'epoch': 1.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 5.860239252797328e-05, 'learning_rate': 7.088716623600345e-06, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8749d41189448298a4aa7d0636e7de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4549520983564435e-06, 'eval_runtime': 163.5884, 'eval_samples_per_second': 151.404, 'eval_steps_per_second': 9.463, 'epoch': 2.0}\n",
      "{'loss': 0.0003, 'grad_norm': 9.079089795704931e-05, 'learning_rate': 6.550387596899226e-06, 'epoch': 2.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 7.462609210051596e-05, 'learning_rate': 6.012058570198106e-06, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 5.245958891464397e-05, 'learning_rate': 5.474806201550388e-06, 'epoch': 2.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 4.4496300688479096e-05, 'learning_rate': 4.936477174849269e-06, 'epoch': 2.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 4.494240783969872e-05, 'learning_rate': 4.398148148148149e-06, 'epoch': 2.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 4.5231856347527355e-05, 'learning_rate': 3.8598191214470285e-06, 'epoch': 2.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 3.43977844750043e-05, 'learning_rate': 3.321490094745909e-06, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'grad_norm': 8.605823677498847e-05, 'learning_rate': 2.783161068044789e-06, 'epoch': 2.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 3.17390586133115e-05, 'learning_rate': 2.2448320413436694e-06, 'epoch': 2.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0012, 'grad_norm': 0.00013088688137941062, 'learning_rate': 1.707579672695952e-06, 'epoch': 2.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.0003269449225626886, 'learning_rate': 1.1692506459948321e-06, 'epoch': 2.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.00013910986308474094, 'learning_rate': 6.309216192937123e-07, 'epoch': 2.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.00015015251119621098, 'learning_rate': 9.259259259259259e-08, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d4f4f26bd3419aa70578797bab567b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.066227127921593e-06, 'eval_runtime': 164.0956, 'eval_samples_per_second': 150.936, 'eval_steps_per_second': 9.434, 'epoch': 3.0}\n",
      "{'train_runtime': 7389.1059, 'train_samples_per_second': 40.222, 'train_steps_per_second': 2.514, 'train_loss': 0.06685295676333312, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18576, training_loss=0.06685295676333312, metrics={'train_runtime': 7389.1059, 'train_samples_per_second': 40.222, 'train_steps_per_second': 2.514, 'total_flos': 7.765844128896614e+16, 'train_loss': 0.06685295676333312, 'epoch': 3.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering, Trainer, TrainingArguments\n",
    "\n",
    "# 載入模型\n",
    "model = BertForQuestionAnswering.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "\n",
    "# 設置訓練參數\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# 設置 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    " \n",
    "# 開始訓練\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate the model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebfebddebdb45b38ff43a2e86d3d5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.066227127921593e-06,\n",
       " 'eval_runtime': 166.0984,\n",
       " 'eval_samples_per_second': 149.116,\n",
       " 'eval_steps_per_second': 9.32,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_model\\\\tokenizer_config.json',\n",
       " './fine_tuned_model\\\\special_tokens_map.json',\n",
       " './fine_tuned_model\\\\vocab.txt',\n",
       " './fine_tuned_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存微調後的模型\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 進行推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加載新的Validation Data進行測試並切割判例段落防止Token length太長"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV Files:   0%|          | 0/30 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (854 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing CSV Files: 100%|██████████| 30/30 [20:38<00:00, 41.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "處理完成，已保存為 'Processed_Opinion.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 設置文件夾路徑\n",
    "folder_path = 'Data2_Opinion_valid'\n",
    "\n",
    "# 初始化分詞器\n",
    "tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "\n",
    "def preprocess_and_split_with_overlap(text, max_length=256, stride=128):\n",
    "    \"\"\"\n",
    "    使用滑動窗口切割長文本，保留一定的上下文重疊。\n",
    "    \n",
    "    Args:\n",
    "        text (str): 待切割的文本。\n",
    "        max_length (int): 每個段落的最大 token 長度。\n",
    "        stride (int): 滑動窗口的步長，重疊部分長度 = max_length - stride。\n",
    "        \n",
    "    Returns:\n",
    "        list: 切割後的文本段列表。\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, truncation=False)  # 不截斷，獲取所有 tokens\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_length, len(tokens))\n",
    "        chunk = tokens[start:end]  # 當前窗口的 tokens\n",
    "        chunks.append(chunk)\n",
    "        if end == len(tokens):  # 最後一段退出\n",
    "            break\n",
    "        start += stride  # 滑動窗口開始位置\n",
    "    # 將 tokens 解碼回文本\n",
    "    return [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n",
    "\n",
    "# 初始化列表存放處理後的數據\n",
    "processed_data = []\n",
    "\n",
    "# 讀取文件夾中的 CSV 文件\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# 遍歷 CSV 文件並顯示進度條\n",
    "for csv_file in tqdm(csv_files, desc=\"Processing CSV Files\"):\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 遍歷 DataFrame 中的每行並顯示進度條\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Processing {csv_file}\", leave=False):\n",
    "        case_index = row['Case Index']  # 替換成您的列名\n",
    "        paragraph = row['Paragraph']    # 替換成您的列名\n",
    "        \n",
    "        # 對段落進行處理和滑動窗口切割\n",
    "        chunks = preprocess_and_split_with_overlap(paragraph)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            processed_data.append({\n",
    "                'CSV File': csv_file,  # 添加 CSV 文件名稱\n",
    "                'Case Index': case_index,\n",
    "                'Chunk ID': i,\n",
    "                'Chunk Text': chunk\n",
    "            })\n",
    "\n",
    "# 將處理後的數據轉為 DataFrame 並保存\n",
    "processed_df = pd.DataFrame(processed_data)\n",
    "processed_df.to_csv('Processed_Opinion.csv', index=False)\n",
    "\n",
    "print(\"處理完成，已保存為 'Processed_Opinion.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加載模型、知識圖譜, 定義檢索知識圖譜函數、RAG測試Pipeline、預測函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded knowledge graph with 13 entries.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# 加載微調後的模型\n",
    "model = BertForQuestionAnswering.from_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# 加載知識圖譜\n",
    "with open(\"definition.json\", \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    knowledge_graph = json.load(f)\n",
    "\n",
    "# 建立名稱到定義的映射\n",
    "name_to_definition = {item[\"Name\"]: item[\"Definition\"] for item in knowledge_graph}\n",
    "\n",
    "print(f\"Loaded knowledge graph with {len(knowledge_graph)} entries.\")\n",
    "\n",
    "# 定義檢索知識圖譜的函數\n",
    "\n",
    "def retrieve_from_knowledge_graph(answer, knowledge_graph):\n",
    "    \"\"\"\n",
    "    根據模型的答案在知識圖譜中進行檢索。\n",
    "    \"\"\"\n",
    "    related_definitions = []\n",
    "    for entry in knowledge_graph:\n",
    "        if entry[\"Name\"].lower() in answer.lower():\n",
    "            related_definitions.append(entry)\n",
    "    \n",
    "    return related_definitions if related_definitions else [{\"Name\": \"No Match\", \"Definition\": \"No related definition found.\"}]\n",
    "\n",
    "# 定義 RAG 測試管線\n",
    "\n",
    "def rag_test_pipeline(row, model, tokenizer, knowledge_graph):\n",
    "    \"\"\"\n",
    "    單條測試管線，適配切割後的資料。\n",
    "    \"\"\"\n",
    "    csv_file = row['CSV File']  # 新增：來源文件名稱\n",
    "    case_index = row['Case Index']\n",
    "    paragraph = row['Chunk Text']\n",
    "    \n",
    "    # 定義測試問題\n",
    "    question = f\"What is the judge's opinion regarding entry barriers in case {case_index}?\"\n",
    "    \n",
    "    # 預測答案\n",
    "    inputs = tokenizer.encode_plus(question, paragraph, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    \n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "    \n",
    "    # 檢索知識圖譜\n",
    "    related_definitions = retrieve_from_knowledge_graph(answer, knowledge_graph)\n",
    "    \n",
    "    return {\n",
    "        \"CSV File\": csv_file,  # 新增：來源文件名稱\n",
    "        \"Case Index\": case_index,\n",
    "        \"Question\": question,\n",
    "        \"Predicted Answer\": answer,\n",
    "        \"Knowledge Graph Result\": related_definitions\n",
    "    }\n",
    "\n",
    "# 批量處理的預測函數\n",
    "\n",
    "def batch_predict(model, tokenizer, questions, contexts, batch_size=16):\n",
    "    \"\"\"\n",
    "    批量處理問題和上下文，進行預測\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for i in tqdm(range(0, len(questions), batch_size), desc=\"Predicting\"):\n",
    "        batch_questions = questions[i:i + batch_size]\n",
    "        batch_contexts = contexts[i:i + batch_size]\n",
    "        \n",
    "        # 編碼輸入\n",
    "        inputs = tokenizer(batch_questions, batch_contexts, return_tensors=\"pt\", \n",
    "                           padding=True, truncation=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}  # 移動到 GPU\n",
    "        \n",
    "        # 模型推理\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # 提取答案位置\n",
    "        for j in range(len(batch_questions)):\n",
    "            start = torch.argmax(outputs.start_logits[j]).item()\n",
    "            end = torch.argmax(outputs.end_logits[j]).item() + 1\n",
    "            input_ids = inputs[\"input_ids\"][j].tolist()\n",
    "            prediction = tokenizer.convert_tokens_to_string(\n",
    "                tokenizer.convert_ids_to_tokens(input_ids[start:end])\n",
    "            )\n",
    "            predictions.append(prediction)\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加載處理過的驗證數據並執行測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 275371 samples for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 17211/17211 [49:19<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'RAG_Test_Results_With_CSV_File.json'\n"
     ]
    }
   ],
   "source": [
    "# 加載處理後的數據\n",
    "valid_data = pd.read_csv(\"Processed_Opinion.csv\")\n",
    "\n",
    "# 提取問題和上下文，並加入文件名\n",
    "questions = [f\"What is the judge's opinion regarding entry barriers in case {case}?\" \n",
    "             for case in valid_data[\"Case Index\"]]\n",
    "contexts = valid_data[\"Chunk Text\"].tolist()\n",
    "csv_files = valid_data[\"CSV File\"].tolist()  # 使用 'CSV File' 列\n",
    "\n",
    "print(f\"Loaded {len(questions)} samples for validation.\")\n",
    "\n",
    "# 移動模型到 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 執行批量預測\n",
    "predicted_answers = batch_predict(model, tokenizer, questions, contexts)\n",
    "\n",
    "## 知識圖譜檢索\n",
    "def generate_results_with_csv(valid_data, questions, contexts, csv_files, predicted_answers, knowledge_graph):\n",
    "    \"\"\"\n",
    "    將驗證數據和模型預測結果封裝成包含 Case Index 和 CSV 文件名的結果列表。\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for case_index, csv_file, question, context, prediction in zip(valid_data[\"Case Index\"], csv_files, questions, contexts, predicted_answers):\n",
    "        related_definitions = retrieve_from_knowledge_graph(prediction, knowledge_graph)\n",
    "        results.append({\n",
    "            \"Case Index\": case_index,\n",
    "            \"CSV File\": csv_file,  # 包含 CSV 文件名\n",
    "            \"Question\": question,\n",
    "            \"Context\": context,\n",
    "            \"Predicted Answer\": prediction,\n",
    "            \"Knowledge Graph Result\": related_definitions\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# 調用函數生成 results\n",
    "results = generate_results_with_csv(valid_data, questions, contexts, csv_files, predicted_answers, knowledge_graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將結果保存為 JSON 文件\n",
    "import json\n",
    "with open(\"RAG_Test_Results_With_CSV_File.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Results saved to 'RAG_Test_Results_With_CSV_File.json'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看測試結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV File 1015 (1)_opinion.csv\n",
      "Case Index: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case 1?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "CSV File 1015 (1)_opinion.csv\n",
      "Case Index: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case 1?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "CSV File 1015 (1)_opinion.csv\n",
      "Case Index: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case 1?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "CSV File 1015 (1)_opinion.csv\n",
      "Case Index: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case 1?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "CSV File 1015 (1)_opinion.csv\n",
      "Case Index: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case 1?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 查看部分結果\n",
    "for result in results[:5]:\n",
    "    print(\"CSV File\", result[\"CSV File\"])\n",
    "    print(\"Case Index:\", result[\"Case Index\"])  # 新增 Case Index 的輸出\n",
    "    print(\"Question:\", result[\"Question\"])\n",
    "    print(\"Predicted Answer:\", result[\"Predicted Answer\"])\n",
    "    print(\"Knowledge Graph Result:\", result[\"Knowledge Graph Result\"])\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看Match的數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 matched results.\n",
      "CSV File 1015 (10)_opinion.csv\n",
      "Case Index: 4\n",
      "Question: What is the judge's opinion regarding entry barriers in case 4?\n",
      "Context: power is \" the ability to... increase... profits by reducing output and charging more than a competitive price. \" hovenkamp, supra, antitrust law § 3. 1. monopoly power is a high dosage of market power. id. § 5. 2. although various factors are important in determining whether a firm has monopoly power ( such as entry barriers, economies of scale, concentration ), often the analysis turns on market share - - generally, the higher the market share the easier is the plaintiff's case. id. usually, c...\n",
      "Predicted Answer: [SEP] power is \" the ability to . . . increase . . . profits by reducing output and charging more than a competitive price . \" hovenkamp , supra , antitrust law § 3 . 1 . monopoly power is a high dosage of market power . id . § 5 . 2 . although various factors are important in determining whether a firm has monopoly power ( such as entry barriers , economies of scale , concentration ) , often the analysis turns on market share - - generally , the higher the market share the easier is the plaintiff ' s case . id . usually , courts require a threshold showing of 60 - 70 % before they will infer monopoly power from market share . see william m . landes & richard a . posner , market power in antitrust cases , 94 harv . l . rev . 937 , 951 ( 1981 ) ( discussing the role of elasticity of demand - - responsiveness of quantity demanded to a one percentage change in price - - and elasticity of supply - - responsiveness of quantity supplied to a one percentage change in price ) . depending on other factors , however , a market share as low as 40 % might suffice . id . ( explaining that when demand and supply for a product are highly inelastic , [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Entry Barriers', 'Definition': 'Factors that deter or prevent new competitors from entering a market.'}, {'Name': 'Economies of Scale', 'Definition': 'Cost advantages due to increased production, making it harder for new entrants to compete.'}]\n",
      "==================================================\n",
      "CSV File 1015 (2)_opinion.csv\n",
      "Case Index: 12\n",
      "Question: What is the judge's opinion regarding entry barriers in case 12?\n",
      "Context: ##glis ) ; and ( 3 ) when prices are below average variable cost a prima facia case of predatory pricing is established and the defendant has the burden of proving by a preponderance of the evidence that its prices were justified without regard to any anticipated destructive effect they might have ( inglis ). the inglis segments of the ninth circuit test for predatory pricing have been followed to varying degrees by the sixth, 15 eighth, 16 [ * 24 ] tenth, 17 and eleventh 18 circuits. the transa...\n",
      "Predicted Answer: [SEP] # # glis ) ; and ( 3 ) when prices are below average variable cost a prima facia case of predatory pricing is established and the defendant has the burden of proving by a preponderance of the evidence that its prices were justified without regard to any anticipated destructive effect they might have ( inglis ) . the inglis segments of the ninth circuit test for predatory pricing have been followed to varying degrees by the sixth , 15 eighth , 16 [ * 24 ] tenth , 17 and eleventh 18 circuits . the transamerica segment of the ninth circuit test which allows a plaintiff to show that prices above average total cost are predatory has not been followed by any other circuit . further , all circuits but the eleventh take a very limited view of what may be shown for a party to meet its burden of proof on predatory pricing . the element of proof that the ninth circuit test requires in addition to showing the relationship between price and cost is a showing \" that the anticipated benefits of defendant ' s price depended on its tendency to discipline or eliminate competition and thereby enhance the firm ' s long - term ability [ * 23 ] to reap the benefits of monopoly power . \" inglis , 668 f . 2d at 1035 . subjective [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 1015 (3)_opinion.csv\n",
      "Case Index: 55\n",
      "Question: What is the judge's opinion regarding entry barriers in case 55?\n",
      "Context: music, inc. ) is required, as is wmni, to sign the new agreement or not be renewed. ( february 6, 1992 letter, muzak's hearing exhibit 129 ) accordingly, the court finds that muzak is not discriminating against wmni in violation of the wfdl. the essential and reasonable requirements of the new agreement wmni advances two arguments that the provisions of the mta program in the new agreement are not essential and reasonable. first, because muzak has not forced the other affiliates with unexpired a...\n",
      "Predicted Answer: [SEP] music , inc . ) is required , as is wmni , to sign the new agreement or not be renewed . ( february 6 , 1992 letter , muzak ' s hearing exhibit 129 ) accordingly , the court finds that muzak is not discriminating against wmni in violation of the wfdl . the essential and reasonable requirements of the new agreement wmni advances two arguments that the provisions of the mta program in the new agreement are not essential and reasonable . first , because muzak has not forced the other affiliates with unexpired agreements [ * * 12 ] to sign on , the mta must not be an essential program . ( wmni ' s post - hearing brief , p . 19 - 20 ) muzak and the ipma counter that the mta is a competitive necessity . among the many features of the mta is the ability to provide uniform service , achieve economies of scale , negotiate standardized rates , and a single service 822 f . supp . 1332 , * 1336 ; 1992 u . s . dist . lexis 21710 , * * 9 [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Economies of Scale', 'Definition': 'Cost advantages due to increased production, making it harder for new entrants to compete.'}]\n",
      "==================================================\n",
      "CSV File 1015 (6)_opinion.csv\n",
      "Case Index: 51\n",
      "Question: What is the judge's opinion regarding entry barriers in case 51?\n",
      "Context: the market in health care financing is competitive.... not only because customers can switch readily but also because new suppliers can enter quickly and existing ones can expand their sales quickly. more than 1, 000 firms are licensed to sell health insurance in indiana, and more than 500 sell this insurance currently.... \" entry barriers into the market for health care financing are extremely low. all that is needed to compete in indiana, for example, is sufficient capital to underwrite the po...\n",
      "Predicted Answer: [SEP] the market in health care financing is competitive . . . . not only because customers can switch readily but also because new suppliers can enter quickly and existing ones can expand their sales quickly . more than 1 , 000 firms are licensed to sell health insurance in indiana , and more than 500 sell this insurance currently . . . . \" entry barriers into the market for health care financing are extremely low . all that is needed to compete in indiana , for example , is sufficient capital to underwrite the policies and a license from the indiana insurance commissioner . \" [ * * 29 ] of the 500 firms now selling insurance , many operate nationwide and have ( or can attract ) plenty of capital against which to write policies - if the price is right . . . . id . at 1331 - 32 ( quoting 603 f . supp . 1077 , 1080 ( s . d . ind . 1985 ) ) . the court adopted the district court ' s finding that the defendants did not have the power to restrict output or raise prices in the market because they furnish \" a fungible product that other people can and do supply easily . \" id . at 1331 ; see also national benefit adm ' rs , inc . v . blue cross , 1989 - 2 trade cas . ( cch ) [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Entry Barriers', 'Definition': 'Factors that deter or prevent new competitors from entering a market.'}]\n",
      "==================================================\n",
      "CSV File 1015 (6)_opinion.csv\n",
      "Case Index: 58\n",
      "Question: What is the judge's opinion regarding entry barriers in case 58?\n",
      "Context: predatory pricing scheme. farmland contends that the judge failed to use the proper standard in evaluating the issue of predation and that the evidence was insufficient under the appropriate standard. we agree with farmland that [ * * * 74 ] under the relevant criteria, the evidence was not sufficient to support a finding of \" predation \" as defined by federal courts in the context of antitrust law. since one of the requirements for liability under § 1 of the sherman act ( and n. j. s. a. 56 : 9...\n",
      "Predicted Answer: [SEP] predatory pricing scheme . farmland contends that the judge failed to use the proper standard in evaluating the issue of predation and that the evidence was insufficient under the appropriate standard . we agree with farmland that [ * * * 74 ] under the relevant criteria , the evidence was not sufficient to support a finding of \" predation \" as defined by federal courts in the context of antitrust law . since one of the requirements for liability under § 1 of the sherman act ( and n . j . s . a . 56 : 9 - 3 ) is [ * 193 ] that the object of , and conduct pursuant to , the conspiracy be illegal , this deficiency in ideal ' s proof provides an additional ground for denial of antitrust liability . predatory pricing schemes are generally analyzed under § 2 of the sherman act ( conspiracies to monopolize ) , or § 2 ( a ) of the clayton act , as amended by the robinson - patman act , 15 u . s . c . a . § 13 ( a ) ( price discrimination ) . brooke group v . brown & williamson , supra , 509 u . s . at , 113 s . ct . at 2586 - 87 , 125 l . ed . 2d at 184 - 85 . although new [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 1015 (9)_opinion.csv\n",
      "Case Index: 47\n",
      "Question: What is the judge's opinion regarding entry barriers in case 47?\n",
      "Context: ##ing are a form of antitrust injury because predatory pricing has the requisite anticompetitive effect against competitors. when defendants engage in predatory pricing or other anticompetitive acts in an attempt to gain a monopoly, the competitor who is being driven out of the market is the party with standing. antitrust & trade law >... > private actions > standing > general overview business & corporate law > cooperatives > general overview civil procedure >... > justiciability > standing > g...\n",
      "Predicted Answer: [SEP] # # ing are a form of antitrust injury because predatory pricing has the requisite anticompetitive effect against competitors . when defendants engage in predatory pricing or other anticompetitive acts in an attempt to gain a monopoly , the competitor who is being driven out of the market is the party with standing . antitrust & trade law > . . . > private actions > standing > general overview business & corporate law > cooperatives > general overview civil procedure > . . . > justiciability > standing > general overview hn8 [ ] private actions , standing a competitor of an alleged attempted monopolist has standing where it is either driven out of business or suffers reduced profits because of the alleged anticompetitive acts of the attempted monopolist . 102 f . 3d 1494 , * 1494 ; 1996 u . s . app . lexis 34400 , * * 1 [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 1026 (10)_opinion.csv\n",
      "Case Index: 13\n",
      "Question: What is the judge's opinion regarding entry barriers in case 13?\n",
      "Context: ##d 405, 413 ( 3d cir. 1997 ) ( quoting spectrum sports, inc. v. mcquillan, 506 u. s. 447, 456, 113 s. ct. 884, 122 l. ed. 2d 247 ( 1993 ) ). our supreme court has held that there are \" two prerequisites to recovery \" under a § 2 sherman act claim regarding predatory pricing : ( 1 ) \" plaintiff must prove that the prices complained of are below an appropriate measure of its rival's costs [, ] \" or in other words, \" below - cost prices [, ] \" and ( 2 ) plaintiff must demonstrate that the...\n",
      "Predicted Answer: [SEP] # # d 405 , 413 ( 3d cir . 1997 ) ( quoting spectrum sports , inc . v . mcquillan , 506 u . s . 447 , 456 , 113 s . ct . 884 , 122 l . ed . 2d 247 ( 1993 ) ) . our supreme court has held that there are \" two prerequisites to recovery \" under a § 2 sherman act claim regarding predatory pricing : ( 1 ) \" plaintiff must prove that the prices complained of are below an appropriate measure of its rival ' s costs [ , ] \" or in other words , \" below - cost prices [ , ] \" and ( 2 ) plaintiff must demonstrate that the [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 1026 (6)_opinion.csv\n",
      "Case Index: 94\n",
      "Question: What is the judge's opinion regarding entry barriers in case 94?\n",
      "Context: [ * * 106 ] hn27 [ ] in order to recover damages for a claim under § 2 ( a ) of the robinson - patman act, a plaintiff must satisfy, among other [ * 423 ] things, two elements, commonly known as the \" injury to competition \" or \" competitive injury \" requirement, and \" antitrust injury. \" competitive injury is one element necessary to make out a prima facie case. \" antitrust injury \" is not part of the prima facie case. hn28 [ ] there are two types of \" competitive injuries \" generally alleged u...\n",
      "Predicted Answer: [SEP] [ * * 106 ] hn27 [ ] in order to recover damages for a claim under § 2 ( a ) of the robinson - patman act , a plaintiff must satisfy , among other [ * 423 ] things , two elements , commonly known as the \" injury to competition \" or \" competitive injury \" requirement , and \" antitrust injury . \" competitive injury is one element necessary to make out a prima facie case . \" antitrust injury \" is not part of the prima facie case . hn28 [ ] there are two types of \" competitive injuries \" generally alleged under section 2 ( a ) , primary line \" and \" secondary line . \" primary line injury occurs when there is harm to the seller ' s competition through predatory pricing . secondary line injury occurs when there is a harm to the buyer ' s competition . see , e . g . , brooke group ltd . v . brown & williamson tobacco group ,\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 1026 (8)_opinion.csv\n",
      "Case Index: 9\n",
      "Question: What is the judge's opinion regarding entry barriers in case 9?\n",
      "Context: & predatory practices > predatory pricing antitrust & trade law >... > trade practices & unfair competition > state regulation > scope hn19 [ ] state regulation, claims rsa 358 - a : 2 ( xiv ) ( supp. 2006 ) specifically states that unfair method of competition or unfair or deceptive act or practice shall include, but not be limited to the pricing of goods or services in a manner that tends to create or maintain a monopoly, or otherwise harm competition. thus, the new hampshire legislature has m...\n",
      "Predicted Answer: [SEP] & predatory practices > predatory pricing antitrust & trade law > . . . > trade practices & unfair competition > state regulation > scope hn19 [ ] state regulation , claims rsa 358 - a : 2 ( xiv ) ( supp . 2006 ) specifically states that unfair method of competition or unfair or deceptive act or practice shall include , but not be limited to the pricing of goods or services in a manner that tends to create or maintain a monopoly , or otherwise harm competition . thus , the new hampshire legislature has made no effort to force this type of antitrust activity into an unfair method category for purposes of the new hampshire consumer [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 914 (4)_opinion.csv\n",
      "Case Index: 50\n",
      "Question: What is the judge's opinion regarding entry barriers in case 50?\n",
      "Context: ##ing monopoly power, which includes allegations regarding the market power held by the antitrust defendant. i. product market conexant alleges that 3com has a 50 % share of the \" product market. \" see answer p 81. hn19 [ ] the ninth circuit has indicated that market share is insufficient to show market power and has found that courts should also evaluate the presence of entry barriers and the inability of competitors to expand output. see rebel oil co., inc., v. given the court's finding that t...\n",
      "Predicted Answer: [SEP] # # ing monopoly power , which includes allegations regarding the market power held by the antitrust defendant . i . product market conexant alleges that 3com has a 50 % share of the \" product market . \" see answer p 81 . hn19 [ ] the ninth circuit has indicated that market share is insufficient to show market power and has found that courts should also evaluate the presence of entry barriers and the inability of competitors to expand output . see rebel oil co . , inc . , v . given the court ' s finding that the conduct alleged by conexant does not state any anti - competitive conduct by 3com before the itu , the court does not reach the issue of noerr - pennington immunity . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Entry Barriers', 'Definition': 'Factors that deter or prevent new competitors from entering a market.'}]\n",
      "==================================================\n",
      "CSV File 914 (5)_opinion.csv\n",
      "Case Index: 76\n",
      "Question: What is the judge's opinion regarding entry barriers in case 76?\n",
      "Context: proscribed effects on united states commerce. this principle was stressed by the supreme court in matsushita : respondents also argue that the check prices, the five company rule, and the price fixing in japan are all part of one large conspiracy that includes monopolization of the american market through predatory pricing. the argument is mistaken. however one decides to describe the contours of the asserted conspiracy - - whether there is one conspiracy or several - - respondents must show tha...\n",
      "Predicted Answer: [SEP] proscribed effects on united states commerce . this principle was stressed by the supreme court in matsushita : respondents also argue that the check prices , the five company rule , and the price fixing in japan are all part of one large conspiracy that includes monopolization of the american market through predatory pricing . the argument is mistaken . however one decides to describe the contours of the asserted conspiracy - - whether there is one conspiracy or several - - respondents must show that the conspiracy caused them an injury for which the antitrust laws provide relief . 241 f . 3d 420 , * 427 ; 2001 u . s . app . lexis 1522 , * * 20 [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 914 (6)_opinion.csv\n",
      "Case Index: 7\n",
      "Question: What is the judge's opinion regarding entry barriers in case 7?\n",
      "Context: * * 160 ] absence of any basis for concluding that the salaries paid to the physicians are so \" excessive \" that they reflect predatory conduct under the antitrust laws. first, as noted above, in the context of the overall economic picture, the hiring of the physicians did not produce any losses. there is no evidence wesley will not recover its marginal costs in employing the physicians. similarly, judge lungstrum, citing tenth circuit decisions such as pacific engineering has concluded : \" it i...\n",
      "Predicted Answer: [SEP] * * 160 ] absence of any basis for concluding that the salaries paid to the physicians are so \" excessive \" that they reflect predatory conduct under the antitrust laws . first , as noted above , in the context of the overall economic picture , the hiring of the physicians did not produce any losses . there is no evidence wesley will not recover its marginal costs in employing the physicians . similarly , judge lungstrum , citing tenth circuit decisions such as pacific engineering has concluded : \" it is clear , however , that hn16 [ ] the tenth circuit leans heavily in favor of using marginal or average variable cost in evaluating predatory pricing claims . \" bushnell corp . v . itt corp . , 175 f . r . d . 584 , 588 ( d . kan . 1997 ) . the court concludes that the plaintiff ' s claims of predatory pricing must be tested against american ' s average variable costs . average variable cost , as a measure of predatory pricing , enjoys not only the weight of authority , it is 140 f . supp . 2d 1141 , * 1198 ; 2001 u . s . dist . lexis 5689 , * * 157 [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 914 (6)_opinion.csv\n",
      "Case Index: 7\n",
      "Question: What is the judge's opinion regarding entry barriers in case 7?\n",
      "Context: inc. v. philadelphia newspapers, inc., 854 f. supp. 367, 376 ( e. d. pa. 1994 ) [ * * 209 ] aff'd, 51 f. 3d 1191 ( 3d cir. 1995 ). in that case, the court was also presented with the theory that the alleged predator had engaged in below - cost pricing in one market ( advertising circulars ), and that it sought to recoup its losses through its reputation in another market ( run - of - press advertisements ). the court rightly rejected such a contention. see also rockbit indus. u. s. a., inc. v. b...\n",
      "Predicted Answer: [SEP] inc . v . philadelphia newspapers , inc . , 854 f . supp . 367 , 376 ( e . d . pa . 1994 ) [ * * 209 ] aff ' d , 51 f . 3d 1191 ( 3d cir . 1995 ) . in that case , the court was also presented with the theory that the alleged predator had engaged in below - cost pricing in one market ( advertising circulars ) , and that it sought to recoup its losses through its reputation in another market ( run - of - press advertisements ) . the court rightly rejected such a contention . see also rockbit indus . u . s . a . , inc . v . baker hughes , inc . , 802 f . supp . 1544 , 1552 ( s . d . tex . 1991 ) ( granting summary judgment on predatory pricing claim based on theory of predatory pricing in one market ( secondary drilling bits ) and recoupment in another ( premium drilling bits ) . independently , the court also rejects the government ' s reputational recoupment argument since it is simply unsupported by the facts . 24 140 f . supp . 2d 1141 , * 1214 ; 2001 u . s . dist . lexis 5689 , * * 206 [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 914 (7)_opinion.csv\n",
      "Case Index: 15\n",
      "Question: What is the judge's opinion regarding entry barriers in case 15?\n",
      "Context: 10th cir. 1995 ). this court recently summarized some of the principles relating to claims of attempted monopolization by predatory pricing in united states v. amr [ * 1228 ] corp., 140 f. supp. 2d 1141 ( d. kan. 2001 ). the court noted : in brooke group [ ltd. v. brown & williamson tobacco corp., 509 u. s. 209, 125 l. ed. 2d 168, 113 s. ct. 2578 ( 1993 ), the court clearly recognized the validity, in appropriate actions, of antitrust liability for predatory pricing. at the same time, however, a...\n",
      "Predicted Answer: [SEP] 10th cir . 1995 ) . this court recently summarized some of the principles relating to claims of attempted monopolization by predatory pricing in united states v . amr [ * 1228 ] corp . , 140 f . supp . 2d 1141 ( d . kan . 2001 ) . the court noted : in brooke group [ ltd . v . brown & williamson tobacco corp . , 509 u . s . 209 ,\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 914 (8)_opinion.csv\n",
      "Case Index: 17\n",
      "Question: What is the judge's opinion regarding entry barriers in case 17?\n",
      "Context: throughout his career. these qualifications render him capable under fed. r. evid. 702 to offer opinion evidence on general financial matters of newspaper business, including projected profits. the defendants focus their challenge on shaffer's methodology in reaching his lost - profits opinion. as with [ * * 28 ] the \" predatory pricing \" opinion, the defendants identify alleged flaws which, they argue, lead to inaccurate assessments of damages. this court, however, finds no specific flaw in sha...\n",
      "Predicted Answer: [SEP] throughout his career . these qualifications render him capable under fed . r . evid . 702 to offer opinion evidence on general financial matters of newspaper business , including projected profits . the defendants focus their challenge on shaffer ' s methodology in reaching his lost - profits opinion . as with [ * * 28 ] the \" predatory pricing \" opinion , the defendants identify alleged flaws which , they argue , lead to inaccurate assessments of damages . this court , however , finds no specific flaw in shaffer ' s methodology that renders his [ * 541 ] analysis of lost profits unreliable as a matter of law . shaffer ' s opinion cannot be excluded simply because the defendants ' expert has reached a different conclusion or applied different methods for assessing damages . certainly , the defendants may argue and present evidence showing that theirs is a more accurate statement of damages , but , at this time , the court is not persuaded to exercise its discretion and to exclude the testimony in its entirety . conclusion for the reasons stated above , it is hereby ordered that the defendants ' motion to exclude the expert testimony of james b . shaffer be , and it hereby is , granted in part and denied in part as stated above . date : august 13 ,\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 914 (9)_opinion.csv\n",
      "Case Index: 71\n",
      "Question: What is the judge's opinion regarding entry barriers in case 71?\n",
      "Context: opinion [ * 1111 ] lucero, circuit judge. this case involves the nature of permissible competitive practices in the airline industry under the antitrust laws of this country, centered around the hub - and - spoke system of american airlines. the united states brought this suit against amr corporation, american airlines, inc., and american eagle holding corporation ( \" american \" ), alleging monopolization and attempted monopolization through predatory pricing in violation of § 2 of the sherman a...\n",
      "Predicted Answer: [SEP] opinion [ * 1111 ] lucero , circuit judge . this case involves the nature of permissible competitive practices in the airline industry under the antitrust laws of this country , centered around the hub - and - spoke system of american airlines . the united states brought this suit against amr corporation , american airlines , inc . , and american eagle holding corporation ( \" american \" ) , alleging monopolization and attempted monopolization through predatory pricing in violation of § 2 of the sherman act , 15 u . s . c . § 2 . in essence , the government alleges that american engaged in multiple episodes of price predation in four city - pair airline markets , all connected to american ' s hub at dallas / fort worth international airport ( \" dfw \" ) , with the ultimate purpose of using the reputation for predatory pricing it earned in those four markets to defend a monopoly at its dfw hub . 1 at its root , the government ' s complaint alleges that american : ( 1 ) priced its product on the routes in question below cost ; and ( 2 ) intended to recoup these losses by charging supracompetitive [ * * 3 ] prices either on the four core routes themselves , or on those routes where it [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 914 (9)_opinion.csv\n",
      "Case Index: 80\n",
      "Question: What is the judge's opinion regarding entry barriers in case 80?\n",
      "Context: practice. it is. nor is the issue whether evidence can ever be presented to satisfy the standards for proving a true instance of predatory pricing. such evidence can be marshaled in circumstances where predation has occurred. 14 [ * * * * 25 ] rather, conley's quarrel is over how predatory pricing is defined and the criteria courts must use to assess whether a particular business practice violates the prohibition against predatory pricing. brooke group sets forth a [ * * 148 ] rational and reaso...\n",
      "Predicted Answer: [SEP] practice . it is . nor is the issue whether evidence can ever be presented to satisfy the standards for proving a true instance of predatory pricing . such evidence can be marshaled in circumstances where predation has occurred . 14 [ * * * * 25 ] rather , conley ' s quarrel is over how predatory pricing is defined and the criteria courts must use to assess whether a particular business practice violates the prohibition against predatory pricing . brooke group sets forth a [ * * 148 ] rational and reasoned method for accomplishing this assessment . other articulations of the prerequisites for recoupment [ * * * * 24 ] under [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 載入保存的結果\n",
    "import json\n",
    "\n",
    "with open(\"RAG_Test_Results_With_CSV_File.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# 過濾有匹配的結果\n",
    "matched_results = [\n",
    "    result for result in results if result[\"Knowledge Graph Result\"][0][\"Name\"] != \"No Match\"\n",
    "]\n",
    "\n",
    "# 列出有匹配的結果\n",
    "print(f\"Found {len(matched_results)} matched results.\")\n",
    "for result in matched_results:\n",
    "    print(\"CSV File\", result[\"CSV File\"])   \n",
    "    print(f\"Case Index: {result['Case Index']}\")  # 包含 Case Index\n",
    "    print(f\"Question: {result['Question']}\")\n",
    "    print(f\"Context: {result['Context'][:500]}...\")\n",
    "    print(f\"Predicted Answer: {result['Predicted Answer']}\")\n",
    "    print(f\"Knowledge Graph Result: {result['Knowledge Graph Result']}\")\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched results saved to 'RAGG_Test_Results_With_CSV_File.json'\n"
     ]
    }
   ],
   "source": [
    "# 將匹配的結果保存為新的 JSON 文件\n",
    "output_file = \"RAGG_Test_Results_With_CSV_File.json\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(matched_results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Matched results saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV Files:   0%|          | 0/30 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (920 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing CSV Files: 100%|██████████| 30/30 [17:23<00:00, 34.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "處理完成，已保存為 'Processed_Opinion_Data1.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 設置文件夾路徑\n",
    "folder_path = 'Data1_Opinion'\n",
    "\n",
    "# 初始化分詞器\n",
    "tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "\n",
    "def preprocess_and_split_with_overlap(text, max_length=256, stride=128):\n",
    "    \"\"\"\n",
    "    使用滑動窗口切割長文本，保留一定的上下文重疊。\n",
    "    \n",
    "    Args:\n",
    "        text (str): 待切割的文本。\n",
    "        max_length (int): 每個段落的最大 token 長度。\n",
    "        stride (int): 滑動窗口的步長，重疊部分長度 = max_length - stride。\n",
    "        \n",
    "    Returns:\n",
    "        list: 切割後的文本段列表。\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, truncation=False)  # 不截斷，獲取所有 tokens\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_length, len(tokens))\n",
    "        chunk = tokens[start:end]  # 當前窗口的 tokens\n",
    "        chunks.append(chunk)\n",
    "        if end == len(tokens):  # 最後一段退出\n",
    "            break\n",
    "        start += stride  # 滑動窗口開始位置\n",
    "    # 將 tokens 解碼回文本\n",
    "    return [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n",
    "\n",
    "# 初始化列表存放處理後的數據\n",
    "processed_data = []\n",
    "\n",
    "# 讀取文件夾中的 CSV 文件\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# 遍歷 CSV 文件並顯示進度條\n",
    "for csv_file in tqdm(csv_files, desc=\"Processing CSV Files\"):\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 遍歷 DataFrame 中的每行並顯示進度條\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Processing {csv_file}\", leave=False):\n",
    "        case_index = row['Case Index']  # 替換成您的列名\n",
    "        paragraph = row['Paragraph']    # 替換成您的列名\n",
    "        \n",
    "        # 對段落進行處理和滑動窗口切割\n",
    "        chunks = preprocess_and_split_with_overlap(paragraph)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            processed_data.append({\n",
    "                'CSV File': csv_file,  # 添加 CSV 文件名稱\n",
    "                'Case Index': case_index,\n",
    "                'Chunk ID': i,\n",
    "                'Chunk Text': chunk\n",
    "            })\n",
    "\n",
    "# 將處理後的數據轉為 DataFrame 並保存\n",
    "processed_df = pd.DataFrame(processed_data)\n",
    "processed_df.to_csv('Processed_Opinion_Data1.csv', index=False)\n",
    "\n",
    "print(\"處理完成，已保存為 'Processed_Opinion_Data1.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded knowledge graph with 13 entries.\n",
      "Loaded 234718 samples for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 14670/14670 [42:23<00:00,  5.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# 加載微調後的模型\n",
    "model = BertForQuestionAnswering.from_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# 加載知識圖譜\n",
    "with open(\"definition.json\", \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    knowledge_graph = json.load(f)\n",
    "\n",
    "# 建立名稱到定義的映射\n",
    "name_to_definition = {item[\"Name\"]: item[\"Definition\"] for item in knowledge_graph}\n",
    "\n",
    "print(f\"Loaded knowledge graph with {len(knowledge_graph)} entries.\")\n",
    "\n",
    "# 定義檢索知識圖譜的函數\n",
    "\n",
    "def retrieve_from_knowledge_graph(answer, knowledge_graph):\n",
    "    \"\"\"\n",
    "    根據模型的答案在知識圖譜中進行檢索。\n",
    "    \"\"\"\n",
    "    related_definitions = []\n",
    "    for entry in knowledge_graph:\n",
    "        if entry[\"Name\"].lower() in answer.lower():\n",
    "            related_definitions.append(entry)\n",
    "    \n",
    "    return related_definitions if related_definitions else [{\"Name\": \"No Match\", \"Definition\": \"No related definition found.\"}]\n",
    "\n",
    "# 定義 RAG 測試管線\n",
    "\n",
    "def rag_test_pipeline(row, model, tokenizer, knowledge_graph):\n",
    "    \"\"\"\n",
    "    單條測試管線，適配切割後的資料。\n",
    "    \"\"\"\n",
    "    csv_file = row['CSV File']  # 新增：來源文件名稱\n",
    "    case_index = row['Case Index']\n",
    "    paragraph = row['Chunk Text']\n",
    "    \n",
    "    # 定義測試問題\n",
    "    question = f\"What is the judge's opinion regarding entry barriers in case {case_index}?\"\n",
    "    \n",
    "    # 預測答案\n",
    "    inputs = tokenizer.encode_plus(question, paragraph, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    \n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "    \n",
    "    # 檢索知識圖譜\n",
    "    related_definitions = retrieve_from_knowledge_graph(answer, knowledge_graph)\n",
    "    \n",
    "    return {\n",
    "        \"CSV File\": csv_file,  # 新增：來源文件名稱\n",
    "        \"Case Index\": case_index,\n",
    "        \"Question\": question,\n",
    "        \"Predicted Answer\": answer,\n",
    "        \"Knowledge Graph Result\": related_definitions\n",
    "    }\n",
    "\n",
    "# 批量處理的預測函數\n",
    "\n",
    "def batch_predict(model, tokenizer, questions, contexts, batch_size=16):\n",
    "    \"\"\"\n",
    "    批量處理問題和上下文，進行預測\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for i in tqdm(range(0, len(questions), batch_size), desc=\"Predicting\"):\n",
    "        batch_questions = questions[i:i + batch_size]\n",
    "        batch_contexts = contexts[i:i + batch_size]\n",
    "        \n",
    "        # 編碼輸入\n",
    "        inputs = tokenizer(batch_questions, batch_contexts, return_tensors=\"pt\", \n",
    "                           padding=True, truncation=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}  # 移動到 GPU\n",
    "        \n",
    "        # 模型推理\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # 提取答案位置\n",
    "        for j in range(len(batch_questions)):\n",
    "            start = torch.argmax(outputs.start_logits[j]).item()\n",
    "            end = torch.argmax(outputs.end_logits[j]).item() + 1\n",
    "            input_ids = inputs[\"input_ids\"][j].tolist()\n",
    "            prediction = tokenizer.convert_tokens_to_string(\n",
    "                tokenizer.convert_ids_to_tokens(input_ids[start:end])\n",
    "            )\n",
    "            predictions.append(prediction)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 加載處理後的數據\n",
    "valid_data = pd.read_csv(\"Processed_Opinion_Data1.csv\")\n",
    "\n",
    "# 提取問題和上下文，並加入文件名\n",
    "questions = [f\"What is the judge's opinion regarding entry barriers in case {case}?\" \n",
    "             for case in valid_data[\"Case Index\"]]\n",
    "contexts = valid_data[\"Chunk Text\"].tolist()\n",
    "csv_files = valid_data[\"CSV File\"].tolist()  # 使用 'CSV File' 列\n",
    "\n",
    "print(f\"Loaded {len(questions)} samples for validation.\")\n",
    "\n",
    "# 移動模型到 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 執行批量預測\n",
    "predicted_answers = batch_predict(model, tokenizer, questions, contexts)\n",
    "\n",
    "## 知識圖譜檢索\n",
    "def generate_results_with_csv(valid_data, questions, contexts, csv_files, predicted_answers, knowledge_graph):\n",
    "    \"\"\"\n",
    "    將驗證數據和模型預測結果封裝成包含 Case Index 和 CSV 文件名的結果列表。\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for case_index, csv_file, question, context, prediction in zip(valid_data[\"Case Index\"], csv_files, questions, contexts, predicted_answers):\n",
    "        related_definitions = retrieve_from_knowledge_graph(prediction, knowledge_graph)\n",
    "        results.append({\n",
    "            \"Case Index\": case_index,\n",
    "            \"CSV File\": csv_file,  # 包含 CSV 文件名\n",
    "            \"Question\": question,\n",
    "            \"Context\": context,\n",
    "            \"Predicted Answer\": prediction,\n",
    "            \"Knowledge Graph Result\": related_definitions\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# 調用函數生成 results\n",
    "results = generate_results_with_csv(valid_data, questions, contexts, csv_files, predicted_answers, knowledge_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'RAG_Test_Results_With_CSV_File_Data1.json'\n",
      "CSV File 0101 (1)_opinion.csv\n",
      "Case Index: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case 1?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "CSV File 0101 (1)_opinion.csv\n",
      "Case Index: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case 1?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "CSV File 0101 (1)_opinion.csv\n",
      "Case Index: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case 1?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "CSV File 0101 (1)_opinion.csv\n",
      "Case Index: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case 1?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "CSV File 0101 (1)_opinion.csv\n",
      "Case Index: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case 1?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 將結果保存為 JSON 文件\n",
    "import json\n",
    "with open(\"RAG_Test_Results_With_CSV_File_Data1.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Results saved to 'RAG_Test_Results_With_CSV_File_Data1.json'\")\n",
    "\n",
    "# 查看部分結果\n",
    "for result in results[:5]:\n",
    "    print(\"CSV File\", result[\"CSV File\"])\n",
    "    print(\"Case Index:\", result[\"Case Index\"])  # 新增 Case Index 的輸出\n",
    "    print(\"Question:\", result[\"Question\"])\n",
    "    print(\"Predicted Answer:\", result[\"Predicted Answer\"])\n",
    "    print(\"Knowledge Graph Result:\", result[\"Knowledge Graph Result\"])\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 matched results.\n",
      "CSV File 0101 (1)_opinion.csv\n",
      "Case Index: 92\n",
      "Question: What is the judge's opinion regarding entry barriers in case 92?\n",
      "Context: monopoly does not violate the antitrust laws unless it was acquired or maintained by exclusionary, unfair, or predatory means. \" 29 the trial judge further instructed the jury, however, that hecht bore the burden of proving that the redskins did not have a natural monopoly : 30 natural monopoly. in the economic sense, natural monopoly is monopoly resulting from economies of scale, a relationship between the size of the market and the size of the most efficient firm such that one firm of efficien...\n",
      "Predicted Answer: [SEP] monopoly does not violate the antitrust laws unless it was acquired or maintained by exclusionary , unfair , or predatory means . \" 29 the trial judge further instructed the jury , however , that hecht bore the burden of proving that the redskins did not have a natural monopoly : 30 natural monopoly . in the economic sense , natural monopoly is monopoly resulting from economies of scale , a relationship between the size of the market and the size of the most efficient firm such that one firm of efficient size can produce all or more than the market can take at a remunerative price , and can continually expand its capacity at less cost than that of a new firm entering the business . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Economies of Scale', 'Definition': 'Cost advantages due to increased production, making it harder for new entrants to compete.'}]\n",
      "==================================================\n",
      "CSV File 0101 (6)_opinion.csv\n",
      "Case Index: 12\n",
      "Question: What is the judge's opinion regarding entry barriers in case 12?\n",
      "Context: west kentucky profits during the period 1950 - 1964 plaintiffs claim that west kentucky's profits of over $ 5, 000, 000. 00 in 1950 before taxes and the decline in profits thereafter show that west kentucky was losing money on its tva contracts by reason of its low bids which were made for the predatory purpose of squeezing them out of the market. the evidence indicates that the variations in west kentucky profits resulted from factors unrelated to predatory pricing. the following table shows th...\n",
      "Predicted Answer: [SEP] west kentucky profits during the period 1950 - 1964 plaintiffs claim that west kentucky ' s profits of over $ 5 , 000 , 000 . 00 in 1950 before taxes and the decline in profits thereafter show that west kentucky was losing money on its tva contracts by reason of its low bids which were made for the predatory purpose of squeezing them out of the market . the evidence indicates that the variations in west kentucky profits resulted from factors unrelated to predatory pricing . the following table shows the situation not only in profit , but also in terms of production in tons and sales in tons . it also shows another significant fact in the industry - the decline in national production . tons net income tons produc - tons before national year tion sales income taxes production 1950 6 , 456 , 136 6 , 473 , 135 $ 5 , 645 , 919 516 , 301 , 000 1951 5 , 513 , 890 5 , 466 , 189 4 , 223 , 814 533 , 600 , 000 1952 5 , 738 , 617 5 , 790 , 072 3 , 864 , 129 466 , 800 , 000 1953 5 , 080 , 565 5 , 182 , 814 2 , 721 , 133 457 , 290 , 000 1954 4 , 849 , 854 4 , [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 0213 (2)_opinion.csv\n",
      "Case Index: 49\n",
      "Question: What is the judge's opinion regarding entry barriers in case 49?\n",
      "Context: . ct. 813, 50 l. ed. 2d 792 ( 1977 ), the ninth circuit discussed predatory pricing. an excerpt from this opinion follows : to demonstrate predation, hanson had to show that the prices charged by shell were such that shell was foregoing present profits in order to create a market position in which it could charge enough to obtain supranormal profits and recoup its present losses. this could [ * * 19 ] be shown by evidence that shell was selling its gasoline at below marginal cost or, because mar...\n",
      "Predicted Answer: [SEP] . ct . 813 , 50 l . ed . 2d 792 ( 1977 ) , the ninth circuit discussed predatory pricing . an excerpt from this opinion follows : to demonstrate predation , hanson had to show that the prices charged by shell were such that shell was foregoing present profits in order to create a market position in which it could charge enough to obtain supranormal profits and recoup its present losses . this could [ * * 19 ] be shown by evidence that shell was selling its gasoline at below marginal cost or , because marginal cost is often impossible to ascertain , below average variable cost . 5 < / tmpblockfootnotegrp > [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 0213 (3)_opinion.csv\n",
      "Case Index: 91\n",
      "Question: What is the judge's opinion regarding entry barriers in case 91?\n",
      "Context: , otter tail used its monopoly power in the wholesale power market to prevent the displacement of its ( natural ) monopoly in the local retail power market. a third case of a refusal to deal as part of a vertical integration plan is poster exch., inc. v. nat'l screen serv. corp., supra. national screen had a monopoly in the sale of certain movie posters and other paraphernalia. this monopoly arose because as a matter of independent and legitimate business judgment, movie companies preferred to d...\n",
      "Predicted Answer: [SEP] , otter tail used its monopoly power in the wholesale power market to prevent the displacement of its ( natural ) monopoly in the local retail power market . a third case of a refusal to deal as part of a vertical integration plan is poster exch . , inc . v . nat ' l screen serv . corp . , supra . national screen had a monopoly in the sale of certain movie posters and other paraphernalia . this monopoly arose because as a matter of independent and legitimate business judgment , movie companies preferred to deal only with it to distribute these materials . 40 for years , poster exchange had bought materials [ * * 45 ] from national screen at wholesale prices for resale . the two companies thus competed with one another in the retail [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Vertical Integration', 'Definition': \"Control over supply chain that limits competitors' access to resources or distribution.\"}]\n",
      "==================================================\n",
      "CSV File 0213 (6)_opinion.csv\n",
      "Case Index: 4\n",
      "Question: What is the judge's opinion regarding entry barriers in case 4?\n",
      "Context: 691, 694 - 95 ( 1963 ) ; note, private treble damages antitrust suits : measure of damages for destruction of all or part of a business, 80 harv. l. rev. 1566, 1573 ( 1967 ) ( hereinafter cited as harvard note ). see also terrell v. household goods carriers bureau, 494 f. 2d 16, 21 ( 5th cir. 1974 ). nevertheless, the posture of this case forces an assumption of antitrust violations and an inquiry into the causal connection of such assumed acts to matters of damage. to avoid the directed verdict...\n",
      "Predicted Answer: [SEP] 691 , 694 - 95 ( 1963 ) ; note , private treble damages antitrust suits : measure of damages for destruction of all or part of a business , 80 harv . l . rev . 1566 , 1573 ( 1967 ) ( hereinafter cited as harvard note ) . see also terrell v . household goods carriers bureau , 494 f . 2d 16 , 21 ( 5th cir . 1974 ) . nevertheless , the posture of this case forces an assumption of antitrust violations and an inquiry into the causal connection of such assumed acts to matters of damage . to avoid the directed verdict on the grounds given by the trial court , malcolm need show he introduced substantial evidence of injury caused by the alleged violations and the amount of damages . 13 . a . \" predatory pricing \" to enforce a \" price fixing \" conspiracy malcolm ' s first claim presents a damage theory that is , as he labels it , counter - intuitive . he claims he was injured by a price - fixing conspiracy that included the defendants . indeed , malcolm presented evidence tending to show the existence of a price - fixing conspiracy , although we do not pass on whether this evidence was sufficient to create a jury question with respect to the defendants . a conspiracy to fix retail prices is undoubtedly an antitrust violation . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 0213 (6)_opinion.csv\n",
      "Case Index: 10\n",
      "Question: What is the judge's opinion regarding entry barriers in case 10?\n",
      "Context: flood the american market with cep's at artificially low prices would agree to limit production. second, plaintiffs have adduced no theory to explain how the kind of data dissemination alleged would be helpful to participants in a low price export conspiracy in the open, competitive u. s. cep market in which the conspirators, as new entrants, had no power to affect either output or prices. 355 the record shows that zenith and rca had nearly 50 % of the u. s. market and that there was competition...\n",
      "Predicted Answer: [SEP] flood the american market with cep ' s at artificially low prices would agree to limit production . second , plaintiffs have adduced no theory to explain how the kind of data dissemination alleged would be helpful to participants in a low price export conspiracy in the open , competitive u . s . cep market in which the conspirators , as new entrants , had no power to affect either output or prices . 355 the record shows that zenith and rca had nearly 50 % of the u . s . market and that there was competition among a large number of manufacturers . under these circumstances , the analytical possibilities of a nexus between plaintiffs ' evidence and the concerted predatory pricing conspiracy are nil . third , the scenario portrayed by plaintiffs , even assuming that there were evidence to support it , cannot be a viable part of their \" unitary \" conspiracy theory in the absence of evidence , direct or circumstantial , of concerted pricing activity , and there is none . [ * * 137 ] 5 . plaintiffs ' argument based upon alleged international price discrimination 513 f . supp . 1100 , * 1303 ; 1981 u . s . dist . lexis 17913 , * * 132 [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 0213 (6)_opinion.csv\n",
      "Case Index: 66\n",
      "Question: What is the judge's opinion regarding entry barriers in case 66?\n",
      "Context: . 2d 848, 856 ( 9th cir. 1977 ), cert. denied, 439 u. s. 829, 99 s. ct. 103, 58 l. ed. 2d 122 ( 1978 ) ; areeda and turner, predatory pricing and related practices under section 2 of the sherman act, 88 harv. l. rev. 697, 698 ( 1975 ). the concept of predation under the robinson - patman act does not differ from the sherman act concept of predation. 9 see generally areeda and turner, 88 harv. l. rev. at 727. 659 f. 2d 340, * 347 ; 1981 u. s. app. lexis 18044, * * 17...\n",
      "Predicted Answer: [SEP] . 2d 848 , 856 ( 9th cir . 1977 ) , cert . denied , 439 u . s . 829 , 99 s . ct . 103 , 58 l . ed . 2d 122 ( 1978 ) ; areeda and turner , predatory pricing and related practices under section 2 of the sherman act , 88 harv . l . rev . 697 , 698 ( 1975 ) . the concept of predation under the robinson - patman act does not differ from the sherman act concept of predation . 9 see generally areeda and turner , 88 harv . l . rev . at 727 . 659 f . 2d 340 , * 347 ; 1981 u . s . app . lexis 18044 , * * 17 [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 0213 (8)_opinion.csv\n",
      "Case Index: 92\n",
      "Question: What is the judge's opinion regarding entry barriers in case 92?\n",
      "Context: pricing may exist when a monopolist arbitrarily lowers prices in areas where it faces competition and either raises or does not lower prices in areas where it does not face competition. mci contends that at & t's choice of the \" high [ * * 385 ] density \" routes in the hi - lo tariff was not based on legitimate cost differences but rather was predatory pricing intended to prevent competition by mci and other entrants. the test for determining whether hi - lo was predatory is the same as for telp...\n",
      "Predicted Answer: [SEP] pricing may exist when a monopolist arbitrarily lowers prices in areas where it faces competition and either raises or does not lower prices in areas where it does not face competition . mci contends that at & t ' s choice of the \" high [ * * 385 ] density \" routes in the hi - lo tariff was not based on legitimate cost differences but rather was predatory pricing intended to prevent competition by mci and other entrants . the test for determining whether hi - lo was predatory is the same as for telpak . again , it is a question of whether the price covered what you consider the applicable cost . if it did , you may not infer predatory intent ; if it did not , you may infer predatory intent . 43 . pre - announcement of hi - lo the announcement of a price reduction by a firm with monopoly power a long time before it intends to put the reduction into effect can be a predatory act . this is sometimes called pre - announcement . the reason pre - announcement can be anti - competitive is that the pre - announcement may hang over the market - - that is , may prevent or discourage buyers from switching to a new competitor while they wait for the announced price reduction to go into effect . mci contends that at [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 1217 (1)_opinion.csv\n",
      "Case Index: 30\n",
      "Question: What is the judge's opinion regarding entry barriers in case 30?\n",
      "Context: ash, the smallest of rokeach's rivals in the retail independent market with 15 % of the market, could reasonably be expected to be forced out as a result of the predatory pricing scheme, monopoly pricing would [ * * 39 ] not be sustainable. manischewitz, a national concern with a 45 to 50 % share of the chicago area retail independent market, competes with rokeach in both the retail independent and the retail chain markets, and even if it were [ * 1543 ] forced to leave the independent market be...\n",
      "Predicted Answer: [SEP] ash , the smallest of rokeach ' s rivals in the retail independent market with 15 % of the market , could reasonably be expected to be forced out as a result of the predatory pricing scheme , monopoly pricing would [ * * 39 ] not be sustainable . manischewitz , a national concern with a 45 to 50 % share of the chicago area retail independent market , competes with rokeach in both the retail independent and the retail chain markets , and even if it were [ * 1543 ] forced to leave the independent market because of rokeach ' s below cost prices , it would remain a potential competitor in that market and would grasp the opportunity of supracompetitive prices to regain its market position . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 1217 (10)_opinion.csv\n",
      "Case Index: 12\n",
      "Question: What is the judge's opinion regarding entry barriers in case 12?\n",
      "Context: , 475 u. s. at p. 589 [ 89 l. ed. 2d at p. 554 ]. ) moreover, a scheme involving 21 conspirators \" is incalculably more difficult to execute than an analogous plan executed by a single predator. \" ( id. at p. 590 [ 89 l. ed. 2d at p. 554 ]. ) \" finally, \" the court observed, \" if predatory pricing conspiracies are generally unlikely to occur, they are especially so where, as here, the prospects of attaining monopoly power seem slight. in order to recoup their losses, petitioners must obtain enou...\n",
      "Predicted Answer: [SEP] , 475 u . s . at p . 589 [ 89 l . ed . 2d at p . 554 ] . ) moreover , a scheme involving 21 conspirators \" is incalculably more difficult to execute than an analogous plan executed by a single predator . \" ( id . at p . 590 [ 89 l . ed . 2d at p . 554 ] . ) \" finally , \" the court observed , \" if predatory pricing conspiracies are generally unlikely to occur , they are especially so where , as here , the prospects of attaining monopoly power seem slight . in order to recoup their losses , petitioners must obtain enough market power to set higher than competitive prices , and then must sustain those prices long enough to earn in excess profits what they earlier gave up in below - cost prices . \" ( id . , at pp . 590 - 591 [ 89 l . ed . 2d at p . 555 ] . ) data in the record strongly suggested that the goals of the alleged scheme had not been achieved . the two largest shares of the retail market in television sets was held by american , not japanese [ * * * 58 ] manufacturers . moreover , those shares did not decline appreciably during the two decades after [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 1217 (10)_opinion.csv\n",
      "Case Index: 30\n",
      "Question: What is the judge's opinion regarding entry barriers in case 30?\n",
      "Context: competitive level will lure into the market new competitors able and willing to offer their commercial goods or personal services for less. see metro mobile cts, inc. v. newvector commun., inc., 892 f. 2d 62 ( 9th cir. 1989 ). time after time, we have recognized hn6 [ ] this basic fact of economic life : a high market share, though it may [ * * 12 ] ordinarily raise an inference of monopoly power, will not do so in a market with low entry barriers or other evidence of a defendant's inability to ...\n",
      "Predicted Answer: [SEP] competitive level will lure into the market new competitors able and willing to offer their commercial goods or personal services for less . see metro mobile cts , inc . v . newvector commun . , inc . , 892 f . 2d 62 ( 9th cir . 1989 ) . time after time , we have recognized hn6 [ ] this basic fact of economic life : a high market share , though it may [ * * 12 ] ordinarily raise an inference of monopoly power , will not do so in a market with low entry barriers or other evidence of a defendant ' s inability to control prices or exclude competitors . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Entry Barriers', 'Definition': 'Factors that deter or prevent new competitors from entering a market.'}]\n",
      "==================================================\n",
      "CSV File 1217 (2)_opinion.csv\n",
      "Case Index: 88\n",
      "Question: What is the judge's opinion regarding entry barriers in case 88?\n",
      "Context: jays'first amended complaint contains three counts. count i alleges that frito - lay has engaged in predatory pricing in violation of section 2 of the sherman act. count ii is [ * * 7 ] brought under section 2 of the clayton act and alleges that frito - lay has engaged in illegal price discrimination. count iii contains pendent claims brought under the deceptive trade practices laws of the states served by jays and alleges that frito - lay engaged in anti - competitive conduct. jays claims that ...\n",
      "Predicted Answer: [SEP] jays ' first amended complaint contains three counts . count i alleges that frito - lay has engaged in predatory pricing in violation of section 2 of the sherman act . count ii is [ * * 7 ] brought under section 2 of the clayton act and alleges that frito - lay has engaged in illegal price discrimination . count iii contains pendent claims brought under the deceptive trade practices laws of the states served by jays and alleges that frito - lay engaged in anti - competitive conduct . jays claims that it sustained a loss totalling $ 4 , 311 , 806 . 73 for fiscal years 1975 - 1981 , because of its inability to increase prices to reach a 6 % pre - tax rate of return as a result of frito - lay ' s anti - competitive conduct . ii . in count i jays claims that frito - lay attempted to monopolize the chicago market for potato chips in violation of section 2 of the sherman act . hn2 [ ] 15 u . s . c . § 2 . the elements of an attempt to monopolize are ( 1 ) intent to control prices or destroy competition with respect to a part of commerce ; ( 2 ) predatory or anti - competitive conduct directed at accomplishing the unlawful purpose ; and ( 3 ) a dangerous probability [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 1217 (2)_opinion.csv\n",
      "Case Index: 88\n",
      "Question: What is the judge's opinion regarding entry barriers in case 88?\n",
      "Context: the interests of competition to permit dominant firms to engage in vigorous competition, including price competition. we therefore reject mci's \" profit maximization \" theory and reaffirm this circuit's holding that liability for predatory pricing must be based upon proof [ * * 29 ] of pricing below cost. mci, 708 f. 2d at 1114 ( footnote and citations omitted ). in this case frito - lay was not even the largest supplier of potato chips for the chicago market. its adoption of non - profit maximi...\n",
      "Predicted Answer: [SEP] the interests of competition to permit dominant firms to engage in vigorous competition , including price competition . we therefore reject mci ' s \" profit maximization \" theory and reaffirm this circuit ' s holding that liability for predatory pricing must be based upon proof [ * * 29 ] of pricing below cost . mci , 708 f . 2d at 1114 ( footnote and citations omitted ) . in this case frito - lay was not even the largest supplier of potato chips for the chicago market . its adoption of non - profit maximizing prices in response to jays ' competitive challenge was thus more reasonable and more necessary than if it had been the dominant supplier . c . entry barriers jays describes the barriers to entering the snack food market as \" almost insurmountable . \" as common sense suggests , supermarket sales of potato chips depend to a large extent upon the shelf space allocated to a supplier . a supplier needs adequate shelf space to compete successfully in the cacophony of kinds , colors , shapes and sizes of the various snack food products and their packages . too little shelf space creates difficult supply problems 614 f . supp . 1073 , * 1081 ; 1985 u . s . dist . lexis 17229 , * * 26 [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Entry Barriers', 'Definition': 'Factors that deter or prevent new competitors from entering a market.'}, {'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 1217 (5)_opinion.csv\n",
      "Case Index: 29\n",
      "Question: What is the judge's opinion regarding entry barriers in case 29?\n",
      "Context: claim on the act's requirement of injury to competition and then to its contention that plaintiff's alternative theory [ * * 8 ] based on alleged predatory pricing is also without merit. hn1 [ ] under the robinson - patman act, it is [ * * * 1613 ] unlawful for any person engaged in commerce... to discriminate in price between different purchasers of commodities of like grade and quality... where the effect of such discrimination may be substantially to lessen competition or tend to create a mon...\n",
      "Predicted Answer: [SEP] claim on the act ' s requirement of injury to competition and then to its contention that plaintiff ' s alternative theory [ * * 8 ] based on alleged predatory pricing is also without merit . hn1 [ ] under the robinson - patman act , it is [ * * * 1613 ] unlawful for any person engaged in commerce . . . to discriminate in price between different purchasers of commodities of like grade and quality . . . where the effect of such discrimination may be substantially to lessen competition or tend to create a monopoly in any line of 809 f . 2d 1334 , * 1337 ; 1987 u . s . app . lexis 1175 , * * 4 ; 1 u . s . p . q . 2d ( bna ) 1610 , * * * 1612 [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 1217 (8)_opinion.csv\n",
      "Case Index: 55\n",
      "Question: What is the judge's opinion regarding entry barriers in case 55?\n",
      "Context: the upshot of indiana grocery's high - barriers - to - entry argument, in any event, is that the existence of such barriers may make an otherwise implausible predatory pricing scheme more plausible. hn5 [ ] to be predatory, it is generally agreed that a price must be below the short - run profit maximizing level and must discipline or destroy rivals such that the predator thereafter gains sustained excess ( that is, monopoly ) profits larger than those lost during the rival - bashing period. the...\n",
      "Predicted Answer: [SEP] the upshot of indiana grocery ' s high - barriers - to - entry argument , in any event , is that the existence of such barriers may make an otherwise implausible predatory pricing scheme more plausible . hn5 [ ] to be predatory , it is generally agreed that a price must be below the short - run profit maximizing level and must discipline or destroy rivals such that the predator thereafter gains sustained excess ( that is , monopoly ) profits larger than those lost during the rival - bashing period . the uncertain future gains must greatly exceed the present actual losses to overcome the uncertainty that rivals will be destroyed or disciplined and that monopoly profits can be reaped in the face of future entry . if rivals survive or entry occurs , not only will predation be unsuccessful , but that very prospect reduces the likelihood that a challenged low price is in fact predatory . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 1217 (8)_opinion.csv\n",
      "Case Index: 62\n",
      "Question: What is the judge's opinion regarding entry barriers in case 62?\n",
      "Context: case appear to be an attempt to escape the rule that there is no antitrust injury if a conspiracy actually tends to [ * * 10 ] benefit a plaintiff. the supreme court has stated : to survive petitioners'motion for summary judgment, respondent must establish that there is a genuine issue of material fact as to whether petitioners entered into an illegal conspiracy that caused respondents to suffer a cognizable injury. this showing has two components. first, respondents must show more than a conspi...\n",
      "Predicted Answer: [SEP] case appear to be an attempt to escape the rule that there is no antitrust injury if a conspiracy actually tends to [ * * 10 ] benefit a plaintiff . the supreme court has stated : to survive petitioners ' motion for summary judgment , respondent must establish that there is a genuine issue of material fact as to whether petitioners entered into an illegal conspiracy that caused respondents to suffer a cognizable injury . this showing has two components . first , respondents must show more than a conspiracy in violation of the antitrust laws ; they must show an injury to them resulting from the illegal conduct . . . . except for the alleged conspiracy to monopolize the american market [ * 193 ] through predatory pricing , these alleged conspiracies could not have caused respondents to suffer an \" antitrust injury , \" because they actually tended to benefit respondents . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 載入保存的結果\n",
    "import json\n",
    "\n",
    "with open(\"RAG_Test_Results_With_CSV_File_Data1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# 過濾有匹配的結果\n",
    "matched_results = [\n",
    "    result for result in results if result[\"Knowledge Graph Result\"][0][\"Name\"] != \"No Match\"\n",
    "]\n",
    "\n",
    "# 列出有匹配的結果\n",
    "print(f\"Found {len(matched_results)} matched results.\")\n",
    "for result in matched_results:\n",
    "    print(\"CSV File\", result[\"CSV File\"])   \n",
    "    print(f\"Case Index: {result['Case Index']}\")  # 包含 Case Index\n",
    "    print(f\"Question: {result['Question']}\")\n",
    "    print(f\"Context: {result['Context'][:500]}...\")\n",
    "    print(f\"Predicted Answer: {result['Predicted Answer']}\")\n",
    "    print(f\"Knowledge Graph Result: {result['Knowledge Graph Result']}\")\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV Files:   0%|          | 0/34 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing CSV Files: 100%|██████████| 34/34 [26:30<00:00, 46.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "處理完成，已保存為 'Processed_Opinion_Data3.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 設置文件夾路徑\n",
    "folder_path = 'Data3_Opinion'\n",
    "\n",
    "# 初始化分詞器\n",
    "tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "\n",
    "def preprocess_and_split_with_overlap(text, max_length=256, stride=128):\n",
    "    \"\"\"\n",
    "    使用滑動窗口切割長文本，保留一定的上下文重疊。\n",
    "    \n",
    "    Args:\n",
    "        text (str): 待切割的文本。\n",
    "        max_length (int): 每個段落的最大 token 長度。\n",
    "        stride (int): 滑動窗口的步長，重疊部分長度 = max_length - stride。\n",
    "        \n",
    "    Returns:\n",
    "        list: 切割後的文本段列表。\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, truncation=False)  # 不截斷，獲取所有 tokens\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_length, len(tokens))\n",
    "        chunk = tokens[start:end]  # 當前窗口的 tokens\n",
    "        chunks.append(chunk)\n",
    "        if end == len(tokens):  # 最後一段退出\n",
    "            break\n",
    "        start += stride  # 滑動窗口開始位置\n",
    "    # 將 tokens 解碼回文本\n",
    "    return [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n",
    "\n",
    "# 初始化列表存放處理後的數據\n",
    "processed_data = []\n",
    "\n",
    "# 讀取文件夾中的 CSV 文件\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# 遍歷 CSV 文件並顯示進度條\n",
    "for csv_file in tqdm(csv_files, desc=\"Processing CSV Files\"):\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 遍歷 DataFrame 中的每行並顯示進度條\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Processing {csv_file}\", leave=False):\n",
    "        case_index = row['Case Index']  # 替換成您的列名\n",
    "        paragraph = row['Paragraph']    # 替換成您的列名\n",
    "        \n",
    "        # 對段落進行處理和滑動窗口切割\n",
    "        chunks = preprocess_and_split_with_overlap(paragraph)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            processed_data.append({\n",
    "                'CSV File': csv_file,  # 添加 CSV 文件名稱\n",
    "                'Case Index': case_index,\n",
    "                'Chunk ID': i,\n",
    "                'Chunk Text': chunk\n",
    "            })\n",
    "\n",
    "# 將處理後的數據轉為 DataFrame 並保存\n",
    "processed_df = pd.DataFrame(processed_data)\n",
    "processed_df.to_csv('Processed_Opinion_Data3.csv', index=False)\n",
    "\n",
    "print(\"處理完成，已保存為 'Processed_Opinion_Data3.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded knowledge graph with 13 entries.\n",
      "Loaded 360699 samples for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 22544/22544 [1:04:11<00:00,  5.85it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# 加載微調後的模型\n",
    "model = BertForQuestionAnswering.from_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# 加載知識圖譜\n",
    "with open(\"definition.json\", \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    knowledge_graph = json.load(f)\n",
    "\n",
    "# 建立名稱到定義的映射\n",
    "name_to_definition = {item[\"Name\"]: item[\"Definition\"] for item in knowledge_graph}\n",
    "\n",
    "print(f\"Loaded knowledge graph with {len(knowledge_graph)} entries.\")\n",
    "\n",
    "# 定義檢索知識圖譜的函數\n",
    "\n",
    "def retrieve_from_knowledge_graph(answer, knowledge_graph):\n",
    "    \"\"\"\n",
    "    根據模型的答案在知識圖譜中進行檢索。\n",
    "    \"\"\"\n",
    "    related_definitions = []\n",
    "    for entry in knowledge_graph:\n",
    "        if entry[\"Name\"].lower() in answer.lower():\n",
    "            related_definitions.append(entry)\n",
    "    \n",
    "    return related_definitions if related_definitions else [{\"Name\": \"No Match\", \"Definition\": \"No related definition found.\"}]\n",
    "\n",
    "# 定義 RAG 測試管線\n",
    "\n",
    "def rag_test_pipeline(row, model, tokenizer, knowledge_graph):\n",
    "    \"\"\"\n",
    "    單條測試管線，適配切割後的資料。\n",
    "    \"\"\"\n",
    "    csv_file = row['CSV File']  # 新增：來源文件名稱\n",
    "    case_index = row['Case Index']\n",
    "    paragraph = row['Chunk Text']\n",
    "    \n",
    "    # 定義測試問題\n",
    "    question = f\"What is the judge's opinion regarding entry barriers in case {case_index}?\"\n",
    "    \n",
    "    # 預測答案\n",
    "    inputs = tokenizer.encode_plus(question, paragraph, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    \n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "    \n",
    "    # 檢索知識圖譜\n",
    "    related_definitions = retrieve_from_knowledge_graph(answer, knowledge_graph)\n",
    "    \n",
    "    return {\n",
    "        \"CSV File\": csv_file,  # 新增：來源文件名稱\n",
    "        \"Case Index\": case_index,\n",
    "        \"Question\": question,\n",
    "        \"Predicted Answer\": answer,\n",
    "        \"Knowledge Graph Result\": related_definitions\n",
    "    }\n",
    "\n",
    "# 批量處理的預測函數\n",
    "\n",
    "def batch_predict(model, tokenizer, questions, contexts, batch_size=16):\n",
    "    \"\"\"\n",
    "    批量處理問題和上下文，進行預測\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for i in tqdm(range(0, len(questions), batch_size), desc=\"Predicting\"):\n",
    "        batch_questions = questions[i:i + batch_size]\n",
    "        batch_contexts = contexts[i:i + batch_size]\n",
    "        \n",
    "        # 編碼輸入\n",
    "        inputs = tokenizer(batch_questions, batch_contexts, return_tensors=\"pt\", \n",
    "                           padding=True, truncation=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}  # 移動到 GPU\n",
    "        \n",
    "        # 模型推理\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # 提取答案位置\n",
    "        for j in range(len(batch_questions)):\n",
    "            start = torch.argmax(outputs.start_logits[j]).item()\n",
    "            end = torch.argmax(outputs.end_logits[j]).item() + 1\n",
    "            input_ids = inputs[\"input_ids\"][j].tolist()\n",
    "            prediction = tokenizer.convert_tokens_to_string(\n",
    "                tokenizer.convert_ids_to_tokens(input_ids[start:end])\n",
    "            )\n",
    "            predictions.append(prediction)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 加載處理後的數據\n",
    "valid_data = pd.read_csv(\"Processed_Opinion_Data3.csv\")\n",
    "\n",
    "# 提取問題和上下文，並加入文件名\n",
    "questions = [f\"What is the judge's opinion regarding entry barriers in case {case}?\" \n",
    "             for case in valid_data[\"Case Index\"]]\n",
    "contexts = valid_data[\"Chunk Text\"].tolist()\n",
    "csv_files = valid_data[\"CSV File\"].tolist()  # 使用 'CSV File' 列\n",
    "\n",
    "print(f\"Loaded {len(questions)} samples for validation.\")\n",
    "\n",
    "# 移動模型到 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 執行批量預測\n",
    "predicted_answers = batch_predict(model, tokenizer, questions, contexts)\n",
    "\n",
    "## 知識圖譜檢索\n",
    "def generate_results_with_csv(valid_data, questions, contexts, csv_files, predicted_answers, knowledge_graph):\n",
    "    \"\"\"\n",
    "    將驗證數據和模型預測結果封裝成包含 Case Index 和 CSV 文件名的結果列表。\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for case_index, csv_file, question, context, prediction in zip(valid_data[\"Case Index\"], csv_files, questions, contexts, predicted_answers):\n",
    "        related_definitions = retrieve_from_knowledge_graph(prediction, knowledge_graph)\n",
    "        results.append({\n",
    "            \"Case Index\": case_index,\n",
    "            \"CSV File\": csv_file,  # 包含 CSV 文件名\n",
    "            \"Question\": question,\n",
    "            \"Context\": context,\n",
    "            \"Predicted Answer\": prediction,\n",
    "            \"Knowledge Graph Result\": related_definitions\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# 調用函數生成 results\n",
    "results = generate_results_with_csv(valid_data, questions, contexts, csv_files, predicted_answers, knowledge_graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'RAG_Test_Results_With_CSV_File_Data3.json'\n",
      "CSV File 0423 (1)_opinion.csv\n",
      "Case Index: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case 1?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "CSV File 0423 (1)_opinion.csv\n",
      "Case Index: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case 1?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "CSV File 0423 (1)_opinion.csv\n",
      "Case Index: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case 1?\n",
      "Predicted Answer: ,\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "CSV File 0423 (1)_opinion.csv\n",
      "Case Index: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case 1?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "CSV File 0423 (1)_opinion.csv\n",
      "Case Index: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case 1?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"RAG_Test_Results_With_CSV_File_Data3.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Results saved to 'RAG_Test_Results_With_CSV_File_Data3.json'\")\n",
    "\n",
    "# 查看部分結果\n",
    "for result in results[:5]:\n",
    "    print(\"CSV File\", result[\"CSV File\"])\n",
    "    print(\"Case Index:\", result[\"Case Index\"])  # 新增 Case Index 的輸出\n",
    "    print(\"Question:\", result[\"Question\"])\n",
    "    print(\"Predicted Answer:\", result[\"Predicted Answer\"])\n",
    "    print(\"Knowledge Graph Result:\", result[\"Knowledge Graph Result\"])\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 matched results.\n",
      "CSV File 0423 (4)_opinion.csv\n",
      "Case Index: 93\n",
      "Question: What is the judge's opinion regarding entry barriers in case 93?\n",
      "Context: ##c distribution market in northern california. ( nsw's [ * 20 ] motion for summary judgment re antitrust claims at 9. ) plaintiffs respond that defendant nsw's exclusive contracts to distribute certain hvac products combined with evidence of flat spec'ing practices on hvac projects are sufficient to create a genuine issue of material fact as to whether there are significant barriers to entry. 30 as the ninth circuit has explained, a § 2 plaintiff, establishing monopoly power by circumstantial e...\n",
      "Predicted Answer: [SEP] # # c distribution market in northern california . ( nsw ' s [ * 20 ] motion for summary judgment re antitrust claims at 9 . ) plaintiffs respond that defendant nsw ' s exclusive contracts to distribute certain hvac products combined with evidence of flat spec ' ing practices on hvac projects are sufficient to create a genuine issue of material fact as to whether there are significant barriers to entry . 30 as the ninth circuit has explained , a § 2 plaintiff , establishing monopoly power by circumstantial evidence , must establish more than just market share . even a 100 % monopolist may not exploit its monopoly power in a market without entry barriers . a § 2 plaintiff must show that new competitors face high market barriers to entry and that current competitors lack the ability to expand their output to challenge a monopolist ' s high prices . barriers to entry \" must be capable of constraining the normal operation of the market to the extent that the problem is unlikely to be self - correcting . \" common entry barriers include : patents or other legal licenses , control of essential or superior resources , entrenched buyer preferences , high capital entry costs and economies of scale . image technical servs . , 125 f . 3d 1195 , 1208 ( 9th cir . 1997 [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Entry Barriers', 'Definition': 'Factors that deter or prevent new competitors from entering a market.'}, {'Name': 'Economies of Scale', 'Definition': 'Cost advantages due to increased production, making it harder for new entrants to compete.'}]\n",
      "==================================================\n",
      "CSV File 0423 (9)_opinion.csv\n",
      "Case Index: 74\n",
      "Question: What is the judge's opinion regarding entry barriers in case 74?\n",
      "Context: jersey antitrust act because it fails under § 1 of the sherman antitrust act. by the same reasoning, the court finds that plaintiff's monopoly and attempted monopoly claims, insofar as they are partially based on unilateral predatory pricing, are sufficiently pleaded under the new jersey antitrust act. see transweb, llc v. 3m innovative props. co., no. 10 - 4413, 2011 u. s. dist. lexis 59095, 2011 wl 2181189, at * 20 ( d. n. j. june 1, 2011 ) ( stating, in the context of an [ * 51 ] attempted mo...\n",
      "Predicted Answer: [SEP] jersey antitrust act because it fails under § 1 of the sherman antitrust act . by the same reasoning , the court finds that plaintiff ' s monopoly and attempted monopoly claims , insofar as they are partially based on unilateral predatory pricing , are sufficiently pleaded under the new jersey antitrust act . see transweb , llc v . 3m innovative props . co . ,\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 0707 (5)_opinion.csv\n",
      "Case Index: 72\n",
      "Question: What is the judge's opinion regarding entry barriers in case 72?\n",
      "Context: metros. 201 armstrong argues even assuming a sixty - percent market share, our court of appeals has never recognized a defendant with a sixty - percent market share as a monopolist in an anti - trust case. while armstrong argues we cannot find it a monopolist with only a sixty - percent market share, this is not the law. our court of appeals determined a plaintiff [ * 44 ] establishes predominant market share with \" significantly larger than 55 %. 202 a plaintiff can show monopoly power even wit...\n",
      "Predicted Answer: [SEP] metros . 201 armstrong argues even assuming a sixty - percent market share , our court of appeals has never recognized a defendant with a sixty - percent market share as a monopolist in an anti - trust case . while armstrong argues we cannot find it a monopolist with only a sixty - percent market share , this is not the law . our court of appeals determined a plaintiff [ * 44 ] establishes predominant market share with \" significantly larger than 55 % . 202 a plaintiff can show monopoly power even with a showing of less than fifty - five percent by pointing to other relevant factors like barriers to entry . 203 the ultimate test is whether armstrong has \" the ability to control prices and exclude competition in a given market . \" 204 the parties raise a genuine issue of fact as to armstrong ' s market share . a reasonable jury could credit professor elhauge ' s calculation of market share . rockfon points to several other relevant factors from which a jury could conclude armstrong possesses monopoly power , including the need for a domestic plant , economies of scale , patent protection , and the significant share of \" repair and replace \" jobs , all showing barriers to entry into the united states market . b . a jury must determine whether armstrong used monopoly power to foreclose [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Economies of Scale', 'Definition': 'Cost advantages due to increased production, making it harder for new entrants to compete.'}, {'Name': 'Patent Protection', 'Definition': 'Legal barriers that prevent competitors from using patented technologies or processes.'}]\n",
      "==================================================\n",
      "CSV File 0722 (4)_opinion.csv\n",
      "Case Index: 18\n",
      "Question: What is the judge's opinion regarding entry barriers in case 18?\n",
      "Context: ., at 588 - 589 ; cargill, inc. v. monfort of colo., inc., 479 u. s. 104, 121 - 122, n. 17, 107 s. ct. 484, 93 l. ed. 2d 427 ( 1986 ) ; see also r. bork, the antitrust paradox 145 ( 1978 ). recognizing this economic reality, we established two prerequisites to recovery on claims of predatory pricing. \" first, a plaintiff seeking to establish competitive injury resulting from a rival's low prices must prove that the prices complained of are below an appropriate measure of its rival's costs. \" bro...\n",
      "Predicted Answer: [SEP] . , at 588 - 589 ; cargill , inc . v . monfort of colo . , inc . , 479 u . s . 104 , 121 - 122 , n . 17 , 107 s . ct . 484 , 93 l . ed . 2d 427 ( 1986 ) ; see also r . bork , the antitrust paradox 145 ( 1978 ) . recognizing this economic reality , we established two prerequisites to recovery on claims of predatory pricing . \" first , a plaintiff seeking to establish competitive injury resulting from a rival ' s low prices must prove that the prices complained of are below an appropriate measure of its rival ' s costs . \" brooke group ltd . v . brown & williamson tobacco corp . , 509 u . s . 209 , 222 , 113 s . ct . 2578 , 125 l . ed . 2d 168 ( 1993 ) . second , a plaintiff must demonstrate that \" the competitor had . . . a dangerous probabilit [ y ] of recouping its investment in below - cost prices . \" id . , at 224 . weyerhaeuser co . v . ross - simmons hardwood lumber co . ,\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "CSV File 0831 (2)_opinion.csv\n",
      "Case Index: 5\n",
      "Question: What is the judge's opinion regarding entry barriers in case 5?\n",
      "Context: brand market is proper requires \" a factual inquiry into the'commercial realities'faced by consumers. \" eastman kodak co. v. image tech. servs., 504 u. s. 451, 482, 112 s. ct. 2072, 119 l. ed. 2d 265 ( 1992 ) ( \" eastman kodak \" ) ( quoting, grinnell corp., 384 u. s. at 572 ). \" single - brand markets are, at a minimum, extremely rare \" and courts have rejected such market definitions \" [ e ] ven where brand loyalty is intense. \" apple, inc. v. psystar corp., 586 f. supp. 2d 1190, 1198 ( n. d. c...\n",
      "Predicted Answer: [SEP] brand market is proper requires \" a factual inquiry into the ' commercial realities ' faced by consumers . \" eastman kodak co . v . image tech . servs . , 504 u . s . 451 , 482 , 112 s . ct . 2072 , 119 l . ed . 2d 265 ( 1992 ) ( \" eastman kodak \" ) ( quoting , grinnell corp . , 384 u . s . at 572 ) . \" single - brand markets are , at a minimum , extremely rare \" and courts have rejected such market definitions \" [ e ] ven where brand loyalty is intense . \" apple , inc . v . psystar corp . , 586 f . supp . 2d 1190 , 1198 ( n . d . cal . 2008 ) ( internal quotation marks and citation omitted ) . but see id . ( \" antitrust markets consisting of just a single brand , however , are not per se prohibited . . . . in theory ,\n",
      "Knowledge Graph Result: [{'Name': 'Brand Loyalty', 'Definition': \"Consumer preference for existing brands, reducing new entrants' market share.\"}]\n",
      "==================================================\n",
      "CSV File 0831 (4)_opinion.csv\n",
      "Case Index: 4\n",
      "Question: What is the judge's opinion regarding entry barriers in case 4?\n",
      "Context: added that a preferred - provider designation differs from vertical integration first, [ * * 10 ] because the contract can be renegotiated periodically, and second, because the insurance market contains other payors, some of which will designate the large hospital as a preferred provider and some of which won't. we concluded that competition in the market, rather than judicial decisions, should determine how that process plays out. that is as true of the clinic's claim as it was of methodist hos...\n",
      "Predicted Answer: [SEP] added that a preferred - provider designation differs from vertical integration first , [ * * 10 ] because the contract can be renegotiated periodically , and second , because the insurance market contains other payors , some of which will designate the large hospital as a preferred provider and some of which won ' t . we concluded that competition in the market , rather than judicial decisions , should determine how that process plays out . that is as true of the clinic ' s claim as it was of methodist hospital ' s . instead of calling the arrangement between the blues and the clinic a form of exclusive dealing , the clinic might have argued that the blues ' preferred - provider network is an essential facility to which every medical provider requires access . but it would be hard to invoke the essential - facilities doctrine , for the clinic does not contend that the blues have market power , so its network cannot be essential . and the supreme court greatly curtailed the scope of the essential - facilities doctrine in pacific bell telephone co . v . linkline communications , inc . , 555 u . s . 438 , 129 s . ct . 1109 , 172 l . ed . 2d 836 ( 2009 ) . this makes it understandable that the clinic has not depicted the blues ' network as essential [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Vertical Integration', 'Definition': \"Control over supply chain that limits competitors' access to resources or distribution.\"}]\n",
      "==================================================\n",
      "CSV File 0831 (4)_opinion.csv\n",
      "Case Index: 11\n",
      "Question: What is the judge's opinion regarding entry barriers in case 11?\n",
      "Context: novell, 731 f. 3d at 1072. \" the challenge for an antitrust court lies in... distinguishing between exclusionary acts, which reduce social welfare, and competitive acts, which increase it. \" microsoft, 253 f. 3d at 58 ; novell, 731 f. 3d at 1072. the courts, with time and a gathering body of experience, have been able to \" adapt this general inquiry to particular circumstances, [ * 982 ] developing considerably more specific rules [ * * 49 ] for common forms of alleged misconduct \" like tying, p...\n",
      "Predicted Answer: [SEP] novell , 731 f . 3d at 1072 . \" the challenge for an antitrust court lies in . . . distinguishing between exclusionary acts , which reduce social welfare , and competitive acts , which increase it . \" microsoft , 253 f . 3d at 58 ; novell , 731 f . 3d at 1072 . the courts , with time and a gathering body of experience , have been able to \" adapt this general inquiry to particular circumstances , [ * 982 ] developing considerably more specific rules [ * * 49 ] for common forms of alleged misconduct \" like tying , predatory pricing , or exclusive dealing . novell , 731 f . 3d at 1072 . real - world monopolists may engage in allegedly exclusionary conduct which does not fit within a single paradigm , instead exhibiting characteristics of several common forms of alleged misconduct . hn11 [ ] in these situations , the courts disaggregate the exclusionary conduct into its component parts before applying the relevant law . the supreme court , for example , separated a price - squeeze claim into a duty - to - deal and predatory - pricing claim . pac . bell tel . co . v . linkline commc ' ns , inc . , 555 u . s . 438 , 449 - 52 , 457 [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 載入保存的結果\n",
    "import json\n",
    "\n",
    "with open(\"RAG_Test_Results_With_CSV_File_Data3.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# 過濾有匹配的結果\n",
    "matched_results = [\n",
    "    result for result in results if result[\"Knowledge Graph Result\"][0][\"Name\"] != \"No Match\"\n",
    "]\n",
    "\n",
    "# 列出有匹配的結果\n",
    "print(f\"Found {len(matched_results)} matched results.\")\n",
    "for result in matched_results:\n",
    "    print(\"CSV File\", result[\"CSV File\"])   \n",
    "    print(f\"Case Index: {result['Case Index']}\")  # 包含 Case Index\n",
    "    print(f\"Question: {result['Question']}\")\n",
    "    print(f\"Context: {result['Context'][:500]}...\")\n",
    "    print(f\"Predicted Answer: {result['Predicted Answer']}\")\n",
    "    print(f\"Knowledge Graph Result: {result['Knowledge Graph Result']}\")\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supreme Court"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV Files:   0%|          | 0/19 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1083 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing CSV Files: 100%|██████████| 19/19 [00:14<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "處理完成，已保存為 'Processed_Opinion_Supreme.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 設置文件夾路徑\n",
    "folder_path = 'Opinion_Supreme'\n",
    "\n",
    "# 初始化分詞器\n",
    "tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "\n",
    "def preprocess_and_split_with_overlap(text, max_length=256, stride=128):\n",
    "    \"\"\"\n",
    "    使用滑動窗口切割長文本，保留一定的上下文重疊。\n",
    "    \n",
    "    Args:\n",
    "        text (str): 待切割的文本。\n",
    "        max_length (int): 每個段落的最大 token 長度。\n",
    "        stride (int): 滑動窗口的步長，重疊部分長度 = max_length - stride。\n",
    "        \n",
    "    Returns:\n",
    "        list: 切割後的文本段列表。\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, truncation=False)  # 不截斷，獲取所有 tokens\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_length, len(tokens))\n",
    "        chunk = tokens[start:end]  # 當前窗口的 tokens\n",
    "        chunks.append(chunk)\n",
    "        if end == len(tokens):  # 最後一段退出\n",
    "            break\n",
    "        start += stride  # 滑動窗口開始位置\n",
    "    # 將 tokens 解碼回文本\n",
    "    return [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n",
    "\n",
    "# 初始化列表存放處理後的數據\n",
    "processed_data = []\n",
    "\n",
    "# 讀取文件夾中的 CSV 文件\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# 遍歷 CSV 文件並顯示進度條\n",
    "for csv_file in tqdm(csv_files, desc=\"Processing CSV Files\"):\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 遍歷 DataFrame 中的每行並顯示進度條\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Processing {csv_file}\", leave=False):\n",
    "        case_index = row['Case Index']  # 替換成您的列名\n",
    "        paragraph = row['Paragraph']    # 替換成您的列名\n",
    "        \n",
    "        # 對段落進行處理和滑動窗口切割\n",
    "        chunks = preprocess_and_split_with_overlap(paragraph)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            processed_data.append({\n",
    "                'CSV File': csv_file,  # 添加 CSV 文件名稱\n",
    "                'Case Index': case_index,\n",
    "                'Chunk ID': i,\n",
    "                'Chunk Text': chunk\n",
    "            })\n",
    "\n",
    "# 將處理後的數據轉為 DataFrame 並保存\n",
    "processed_df = pd.DataFrame(processed_data)\n",
    "processed_df.to_csv('Processed_Opinion_Supreme.csv', index=False)\n",
    "\n",
    "print(\"處理完成，已保存為 'Processed_Opinion_Supreme.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded knowledge graph with 13 entries.\n",
      "Loaded 3269 samples for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 205/205 [00:35<00:00,  5.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# 加載微調後的模型\n",
    "model = BertForQuestionAnswering.from_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# 加載知識圖譜\n",
    "with open(\"definition.json\", \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    knowledge_graph = json.load(f)\n",
    "\n",
    "# 建立名稱到定義的映射\n",
    "name_to_definition = {item[\"Name\"]: item[\"Definition\"] for item in knowledge_graph}\n",
    "\n",
    "print(f\"Loaded knowledge graph with {len(knowledge_graph)} entries.\")\n",
    "\n",
    "# 定義檢索知識圖譜的函數\n",
    "\n",
    "def retrieve_from_knowledge_graph(answer, knowledge_graph):\n",
    "    \"\"\"\n",
    "    根據模型的答案在知識圖譜中進行檢索。\n",
    "    \"\"\"\n",
    "    related_definitions = []\n",
    "    for entry in knowledge_graph:\n",
    "        if entry[\"Name\"].lower() in answer.lower():\n",
    "            related_definitions.append(entry)\n",
    "    \n",
    "    return related_definitions if related_definitions else [{\"Name\": \"No Match\", \"Definition\": \"No related definition found.\"}]\n",
    "\n",
    "# 定義 RAG 測試管線\n",
    "\n",
    "def rag_test_pipeline(row, model, tokenizer, knowledge_graph):\n",
    "    \"\"\"\n",
    "    單條測試管線，適配切割後的資料。\n",
    "    \"\"\"\n",
    "    csv_file = row['CSV File']  # 新增：來源文件名稱\n",
    "    case_index = row['Case Index']\n",
    "    paragraph = row['Chunk Text']\n",
    "    \n",
    "    # 定義測試問題\n",
    "    question = f\"What is the judge's opinion regarding entry barriers in case {case_index}?\"\n",
    "    \n",
    "    # 預測答案\n",
    "    inputs = tokenizer.encode_plus(question, paragraph, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    \n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "    \n",
    "    # 檢索知識圖譜\n",
    "    related_definitions = retrieve_from_knowledge_graph(answer, knowledge_graph)\n",
    "    \n",
    "    return {\n",
    "        \"CSV File\": csv_file,  # 新增：來源文件名稱\n",
    "        \"Case Index\": case_index,\n",
    "        \"Question\": question,\n",
    "        \"Predicted Answer\": answer,\n",
    "        \"Knowledge Graph Result\": related_definitions\n",
    "    }\n",
    "\n",
    "# 批量處理的預測函數\n",
    "\n",
    "def batch_predict(model, tokenizer, questions, contexts, batch_size=16):\n",
    "    \"\"\"\n",
    "    批量處理問題和上下文，進行預測\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for i in tqdm(range(0, len(questions), batch_size), desc=\"Predicting\"):\n",
    "        batch_questions = questions[i:i + batch_size]\n",
    "        batch_contexts = contexts[i:i + batch_size]\n",
    "        \n",
    "        # 編碼輸入\n",
    "        inputs = tokenizer(batch_questions, batch_contexts, return_tensors=\"pt\", \n",
    "                           padding=True, truncation=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}  # 移動到 GPU\n",
    "        \n",
    "        # 模型推理\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # 提取答案位置\n",
    "        for j in range(len(batch_questions)):\n",
    "            start = torch.argmax(outputs.start_logits[j]).item()\n",
    "            end = torch.argmax(outputs.end_logits[j]).item() + 1\n",
    "            input_ids = inputs[\"input_ids\"][j].tolist()\n",
    "            prediction = tokenizer.convert_tokens_to_string(\n",
    "                tokenizer.convert_ids_to_tokens(input_ids[start:end])\n",
    "            )\n",
    "            predictions.append(prediction)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 加載處理後的數據\n",
    "valid_data = pd.read_csv(\"Processed_Opinion_Supreme.csv\")\n",
    "\n",
    "# 提取問題和上下文，並加入文件名\n",
    "questions = [f\"What is the judge's opinion regarding entry barriers in case {case}?\" \n",
    "             for case in valid_data[\"Case Index\"]]\n",
    "contexts = valid_data[\"Chunk Text\"].tolist()\n",
    "csv_files = valid_data[\"CSV File\"].tolist()  # 使用 'CSV File' 列\n",
    "\n",
    "print(f\"Loaded {len(questions)} samples for validation.\")\n",
    "\n",
    "# 移動模型到 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 執行批量預測\n",
    "predicted_answers = batch_predict(model, tokenizer, questions, contexts)\n",
    "\n",
    "## 知識圖譜檢索\n",
    "def generate_results_with_csv(valid_data, questions, contexts, csv_files, predicted_answers, knowledge_graph):\n",
    "    \"\"\"\n",
    "    將驗證數據和模型預測結果封裝成包含 Case Index 和 CSV 文件名的結果列表。\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for case_index, csv_file, question, context, prediction in zip(valid_data[\"Case Index\"], csv_files, questions, contexts, predicted_answers):\n",
    "        related_definitions = retrieve_from_knowledge_graph(prediction, knowledge_graph)\n",
    "        results.append({\n",
    "            \"Case Index\": case_index,\n",
    "            \"CSV File\": csv_file,  # 包含 CSV 文件名\n",
    "            \"Question\": question,\n",
    "            \"Context\": context,\n",
    "            \"Predicted Answer\": prediction,\n",
    "            \"Knowledge Graph Result\": related_definitions\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# 調用函數生成 results\n",
    "results = generate_results_with_csv(valid_data, questions, contexts, csv_files, predicted_answers, knowledge_graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'RAG_Test_Results_With_CSV_File_Supreme.json'\n",
      "CSV File Supreme (1)_opinion.csv\n",
      "Case Index: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case 1?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "CSV File Supreme (1)_opinion.csv\n",
      "Case Index: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case 1?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "CSV File Supreme (1)_opinion.csv\n",
      "Case Index: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case 1?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "CSV File Supreme (1)_opinion.csv\n",
      "Case Index: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case 1?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "CSV File Supreme (1)_opinion.csv\n",
      "Case Index: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case 1?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"RAG_Test_Results_With_CSV_File_Supreme.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Results saved to 'RAG_Test_Results_With_CSV_File_Supreme.json'\")\n",
    "\n",
    "# 查看部分結果\n",
    "for result in results[:5]:\n",
    "    print(\"CSV File\", result[\"CSV File\"])\n",
    "    print(\"Case Index:\", result[\"Case Index\"])  # 新增 Case Index 的輸出\n",
    "    print(\"Question:\", result[\"Question\"])\n",
    "    print(\"Predicted Answer:\", result[\"Predicted Answer\"])\n",
    "    print(\"Knowledge Graph Result:\", result[\"Knowledge Graph Result\"])\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 matched results.\n"
     ]
    }
   ],
   "source": [
    "# 載入保存的結果\n",
    "import json\n",
    "\n",
    "with open(\"RAG_Test_Results_With_CSV_File_Supreme.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# 過濾有匹配的結果\n",
    "matched_results = [\n",
    "    result for result in results if result[\"Knowledge Graph Result\"][0][\"Name\"] != \"No Match\"\n",
    "]\n",
    "\n",
    "# 列出有匹配的結果\n",
    "print(f\"Found {len(matched_results)} matched results.\")\n",
    "for result in matched_results:\n",
    "    print(\"CSV File\", result[\"CSV File\"])   \n",
    "    print(f\"Case Index: {result['Case Index']}\")  # 包含 Case Index\n",
    "    print(f\"Question: {result['Question']}\")\n",
    "    print(f\"Context: {result['Context'][:500]}...\")\n",
    "    print(f\"Predicted Answer: {result['Predicted Answer']}\")\n",
    "    print(f\"Knowledge Graph Result: {result['Knowledge Graph Result']}\")\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 涵蓋更多entryBarrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded knowledge graph with 13 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 14670/14670 [42:31<00:00,  5.75it/s]\n",
      "Predicting: 100%|██████████| 17211/17211 [49:40<00:00,  5.77it/s]t]\n",
      "Predicting: 100%|██████████| 22544/22544 [1:05:04<00:00,  5.77it/s]t]\n",
      "Predicting: 100%|██████████| 205/205 [00:36<00:00,  5.68it/s]1s/it]  \n",
      "Processing Files: 100%|██████████| 4/4 [2:38:13<00:00, 2373.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All results saved to 'All_RAG_Results.json'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# 加載微調後的模型\n",
    "model = BertForQuestionAnswering.from_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# 加載知識圖譜\n",
    "with open(\"definition_moreinfo.json\", \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    knowledge_graph = json.load(f)\n",
    "\n",
    "# 建立名稱到定義的映射\n",
    "name_to_definition = {item[\"Name\"]: item[\"Definition\"] for item in knowledge_graph}\n",
    "\n",
    "print(f\"Loaded knowledge graph with {len(knowledge_graph)} entries.\")\n",
    "\n",
    "# 定義檢索知識圖譜的函數\n",
    "def retrieve_from_knowledge_graph(answer, knowledge_graph):\n",
    "    related_definitions = []\n",
    "    for entry in knowledge_graph:\n",
    "        # 獲取名稱和同義詞\n",
    "        synonyms = entry.get(\"Synonyms\", [])\n",
    "        terms_to_match = [entry[\"Name\"]] + synonyms\n",
    "\n",
    "        # 名稱或同義詞匹配\n",
    "        if any(term.lower() in answer.lower() for term in terms_to_match):\n",
    "            related_definitions.append(entry)\n",
    "            continue\n",
    "\n",
    "        # 定義關鍵詞匹配\n",
    "        if entry[\"Definition\"].lower() in answer.lower():\n",
    "            related_definitions.append(entry)\n",
    "\n",
    "    return related_definitions if related_definitions else [{\"Name\": \"No Match\", \"Definition\": \"No related definition found.\"}]\n",
    "\n",
    "# 定義批量處理函數，新增檢索結合\n",
    "def batch_predict_with_retrieval(model, tokenizer, questions, contexts, knowledge_graph, batch_size=16):\n",
    "    predictions = []\n",
    "    for i in tqdm(range(0, len(questions), batch_size), desc=\"Predicting\"):\n",
    "        batch_questions = []\n",
    "        batch_contexts = contexts[i:i + batch_size]\n",
    "\n",
    "        # 檢索知識並組合問題\n",
    "        for question, context in zip(questions[i:i + batch_size], batch_contexts):\n",
    "            # 模擬初步回答以檢索知識\n",
    "            retrieved_knowledge = retrieve_from_knowledge_graph(context, knowledge_graph)\n",
    "            retrieved_text = \" \".join([item[\"Definition\"] for item in retrieved_knowledge if item[\"Name\"] != \"No Match\"])\n",
    "            full_question = f\"{retrieved_text} {question}\"\n",
    "            batch_questions.append(full_question)\n",
    "\n",
    "        # 編碼輸入\n",
    "        inputs = tokenizer(batch_questions, batch_contexts, return_tensors=\"pt\", \n",
    "                           padding=True, truncation=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}  # 移動到 GPU\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # 解碼回答\n",
    "        for j in range(len(batch_questions)):\n",
    "            start = torch.argmax(outputs.start_logits[j]).item()\n",
    "            end = torch.argmax(outputs.end_logits[j]).item() + 1\n",
    "            input_ids = inputs[\"input_ids\"][j].tolist()\n",
    "            prediction = tokenizer.convert_tokens_to_string(\n",
    "                tokenizer.convert_ids_to_tokens(input_ids[start:end])\n",
    "            )\n",
    "            predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "# 遞迴處理所有處理過的 CSV 文件\n",
    "folder_path = \"Processed_Data_Opinion\"\n",
    "all_results = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for file in tqdm(os.listdir(folder_path), desc=\"Processing Files\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        valid_data = pd.read_csv(file_path)\n",
    "\n",
    "        questions = [f\"What is the judge's opinion regarding entry barriers in case {case}?\" \n",
    "                     for case in valid_data[\"Case Index\"]]\n",
    "        contexts = valid_data[\"Chunk Text\"].tolist()\n",
    "        csv_files = [file] * len(questions)  # 文件名對應每個樣本\n",
    "\n",
    "        predicted_answers = batch_predict_with_retrieval(model, tokenizer, questions, contexts, knowledge_graph)\n",
    "\n",
    "        for case_index, csv_file, question, context, prediction in zip(valid_data[\"Case Index\"], csv_files, questions, contexts, predicted_answers):\n",
    "            related_definitions = retrieve_from_knowledge_graph(prediction, knowledge_graph)\n",
    "            all_results.append({\n",
    "                \"Case Index\": case_index,\n",
    "                \"CSV File\": csv_file,\n",
    "                \"Question\": question,\n",
    "                \"Context\": context,\n",
    "                \"Predicted Answer\": prediction,\n",
    "                \"Knowledge Graph Result\": related_definitions\n",
    "            })\n",
    "\n",
    "# 保存所有結果\n",
    "with open(\"All_RAG_Results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"All results saved to 'All_RAG_Results.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61 matched results.\n",
      "\n",
      "Displaying the first 5 matched results:\n",
      "==================================================\n",
      "CSV File: Processed_Opinion_Data1.csv\n",
      "Case Index: 25\n",
      "Question: What is the judge's opinion regarding entry barriers in case 25?\n",
      "Context: s is $ 6, 000, 000. appellees offered testimony that the merger would enable certain economies of scale, specifically, that it would enable the formation of a more elaborate foreign department than either bank is presently able to maintain. but this attempted justification, which was not mentioned b...\n",
      "Predicted Answer: [SEP] s is $ 6 , 000 , 000 . appellees offered testimony that the merger would enable certain economies of scale , specifically , that it would enable the formation of a more elaborate foreign department than either bank is presently able to maintain . but this attempted justification , which was not mentioned by the district court in its opinion and has not been developed with any fullness before this court , we consider abandoned . the exclusion of banks from the ftc ' s jurisdiction appears to have been motivated by the fact that banks were already subject to extensive federal administrative controls . see t . c . hurst & son v . federal trade comm ' n , 268 f . 874 , 877 ( d . c . e . d . va . 1920 ) . 374 u . s . 321 , * 334 ; 83 s . ct . 1715 , * * 1726 ; 10 l . ed . 2d 915 , * * * 928 ; 1963 u . s . lexis 2413 , * * * * 23 [SEP]\n",
      "Knowledge Graph Result:\n",
      "  - Name: Economies of Scale\n",
      "    Definition: Cost advantages due to increased production, making it harder for new entrants to compete.\n",
      "==================================================\n",
      "==================================================\n",
      "CSV File: Processed_Opinion_Data1.csv\n",
      "Case Index: 30\n",
      "Question: What is the judge's opinion regarding entry barriers in case 30?\n",
      "Context: difference at all between what plaintiff alleged previously and what it alleges now. only the form of the complaint and amount of requested relief have changed. as indicated above, in the control proceedings leading to the approval of the acquisition the commission considered the deposit receipts ar...\n",
      "Predicted Answer: [SEP] difference at all between what plaintiff alleged previously and what it alleges now . only the form of the complaint and amount of requested relief have changed . as indicated above , in the control proceedings leading to the approval of the acquisition the commission considered the deposit receipts arrangement and concluded that it was not a partial consummation of the written contract , a determination that \" necessarily included a finding that there was no agreement , formal or informal , to purchase the receipts . \" 287 f . supp . at 383 . it found that the deposit receipts arrangement erected no additional barrier to competition . it found that the i . c . c . was not misled by false testimony . it found that transcon did not acquire pre - approval control over queen . these are the \" precise ingredients \" [ * * 44 ] of investors ' complaint here . true , neither the commission nor the courts specifically stated that defendants lacked an anticompetitive purpose when they agreed upon the acquisition , or that the defendants did not make false and misleading statements to the [SEP]\n",
      "Knowledge Graph Result:\n",
      "  - Name: Entry Barriers\n",
      "    Definition: Factors that deter or prevent new competitors from entering a market.\n",
      "==================================================\n",
      "==================================================\n",
      "CSV File: Processed_Opinion_Data1.csv\n",
      "Case Index: 47\n",
      "Question: What is the judge's opinion regarding entry barriers in case 47?\n",
      "Context: ( 10th cir. 1981 ), cert. dismissed, 456 u. s. 1001, 102 s. ct. 2287, 73 l. ed. 2d 1296 ( 1982 ) ; home box office, inc. v. fcc, 185 u. s. app. d. c. 142, 567 f. 2d 9, 43 - 51 ( d. c. cir. ), cert. denied, 434 u. s. 829, 98 s. ct. 111, 54 l. ed. 2d 89 ( 1977 ) ; midwest video corp. v. fcc, 571 f. 2d...\n",
      "Predicted Answer: [SEP] ( 10th cir . 1981 ) , cert . dismissed , 456 u . s . 1001 , 102 s . ct . 2287 , 73 l . ed . 2d 1296 ( 1982 ) ; home box office , inc . v . fcc , 185 u . s . app . d . c . 142 , 567 f . 2d 9 , 43 - 51 ( d . c . cir . ) , cert . denied , 434 u . s . 829 , 98 s . ct . 111 , 54 l . ed . 2d 89 ( 1977 ) ; midwest video corp . v . fcc , 571 f . 2d 1025 , 1052 - 57 ( 8th cir . 1978 ) ( dictum ) , aff ' d on other grounds , 440 u . s . 689 , 99 s . ct . 1435 , 59 l . ed . 2d 692 ( 1979 ) . the problem lies in determining just what government regulations are permissible . the supreme court has repeatedly emphasized that hn9 [ ] \" each medium of expression , of course , must be assessed for first amendment purposes by standards suited to it , \" southeastern promotions , ltd . v . conrad ,\n",
      "Knowledge Graph Result:\n",
      "  - Name: Government Regulations\n",
      "    Definition: Legal or administrative restrictions that limit market entry.\n",
      "==================================================\n",
      "==================================================\n",
      "CSV File: Processed_Opinion_Data1.csv\n",
      "Case Index: 75\n",
      "Question: What is the judge's opinion regarding entry barriers in case 75?\n",
      "Context: - elasticity of demand in this industry, its ability to control prices, its ability to exclude competition, the presence of barriers to entry, or the existence of monopoly profits. see generally, ii p. areeda & d. turner, antitrust law pp507 - 16 ( 1978 ) ; united states v. e. i. du pont de nemours ...\n",
      "Predicted Answer: [SEP] - elasticity of demand in this industry , its ability to control prices , its ability to exclude competition , the presence of barriers to entry , or the existence of monopoly profits . see generally , ii p . areeda & d . turner , antitrust law pp507 - 16 ( 1978 ) ; united states v . e . i . du pont de nemours & co . , 351 u . s . 377 , 394 - 404 ( 1956 ) ; american tobacco co . v . united states , 328 u . s . 781 , 808 - 14 ( 1946 ) ; grinnell , 384 u . s . at 571 - 76 ; united states v . aluminum co . ( alcoa ) , 148 f . 2d 416 ( 2nd cir . 1945 ) . plaintiff stressed that snowshoe was perhaps the premier ski resort in the mid - atlantic region , due to the particular natural characteristics associated with the mountain . ( brigham testimony ; ladd [ * 7 ] testimony ) . there was also 1984 u . s . dist . lexis 17592 , * 2 [SEP]\n",
      "Knowledge Graph Result:\n",
      "  - Name: Entry Barriers\n",
      "    Definition: Factors that deter or prevent new competitors from entering a market.\n",
      "==================================================\n",
      "==================================================\n",
      "CSV File: Processed_Opinion_Data1.csv\n",
      "Case Index: 89\n",
      "Question: What is the judge's opinion regarding entry barriers in case 89?\n",
      "Context: s. 1030, 101 s. ct. 1740, 68 l. ed. 2d 226 ( 1982 ) ( about 50 %, coupled with evidence of low number of actual competitors and high barriers to market entry ) ; heatransfer corp. v. volkswagenwerk a. g., 553 f. 2d 964, 981 ( 5th cir. 1977 ), cert. denied, 434 u. s. 1087, 98 s. ct. 1282, 55 l. ed. 2...\n",
      "Predicted Answer: [SEP] s . 1030 , 101 s . ct . 1740 , 68 l . ed . 2d 226 ( 1982 ) ( about 50 % , coupled with evidence of low number of actual competitors and high barriers to market entry ) ; heatransfer corp . v . volkswagenwerk a . g . , 553 f . 2d 964 , 981 ( 5th cir . 1977 ) , cert . denied , 434 u . s . 1087 , 98 s . ct . 1282 , 55 l . ed . 2d 792 ( 1978 ) ( 71 % - 76 % ) ; woods exploration & producing co . v . aluminum co . of america , 438 f . 2d 1286 , 1307 ( 5th cir . 1971 ) ; cert . denied , 404 u . s . 1047 , 92 s . ct . 701 , 30 l . ed . 2d 736 ( 1973 ) ( 90 % ) ; north texas producers ass ' n v . metzger dairies , inc . ,\n",
      "Knowledge Graph Result:\n",
      "  - Name: Entry Barriers\n",
      "    Definition: Factors that deter or prevent new competitors from entering a market.\n",
      "==================================================\n",
      "Matched results saved to 'Matched_ALL_DATA.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 載入所有結果\n",
    "with open(\"All_RAG_Results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# 過濾有匹配的結果\n",
    "matched_results = [\n",
    "    result for result in results if any(res[\"Name\"] != \"No Match\" for res in result[\"Knowledge Graph Result\"])\n",
    "]\n",
    "\n",
    "# 列出匹配的結果數量\n",
    "print(f\"Found {len(matched_results)} matched results.\")\n",
    "\n",
    "# 保存匹配結果到新文件\n",
    "output_file = \"Matched_ALL_DATA.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(matched_results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 顯示部分匹配結果\n",
    "if matched_results:\n",
    "    print(\"\\nDisplaying the first 5 matched results:\")\n",
    "    for result in matched_results[:5]:  # 只顯示前5個匹配結果\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"CSV File: {result['CSV File']}\")\n",
    "        print(f\"Case Index: {result['Case Index']}\")\n",
    "        print(f\"Question: {result['Question']}\")\n",
    "        print(f\"Context: {result['Context'][:300]}...\")  # 限制輸出長度\n",
    "        print(f\"Predicted Answer: {result['Predicted Answer']}\")\n",
    "        print(\"Knowledge Graph Result:\")\n",
    "        for kg_result in result[\"Knowledge Graph Result\"]:\n",
    "            print(f\"  - Name: {kg_result['Name']}\")\n",
    "            print(f\"    Definition: {kg_result['Definition']}\")\n",
    "        print(\"=\" * 50)\n",
    "else:\n",
    "    print(\"No matched results found.\")\n",
    "\n",
    "print(f\"Matched results saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FN, HN, CoreTerms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing /Users/tangjiahong/Dropbox/textmining1/Data_Clean/CSV/merge_CSV/merge_FN.csv:   0%|          | 212/129441 [00:00<03:06, 694.38it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (681 > 512). Running this sequence through the model will result in indexing errors\n",
      "                                                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "處理完成，已保存為 'Processed_FN_Data.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 設置 CSV 文件路徑\n",
    "csv_file = r'/Users/tangjiahong/Dropbox/textmining1/Data_Clean/CSV/merge_CSV/merge_FN.csv'\n",
    "\n",
    "# 初始化分詞器\n",
    "tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "\n",
    "def preprocess_and_split_with_overlap(text, max_length=256, stride=128):\n",
    "    \"\"\"\n",
    "    使用滑動窗口切割長文本，保留一定的上下文重疊。\n",
    "    \n",
    "    Args:\n",
    "        text (str): 待切割的文本。\n",
    "        max_length (int): 每個段落的最大 token 長度。\n",
    "        stride (int): 滑動窗口的步長，重疊部分長度 = max_length - stride。\n",
    "        \n",
    "    Returns:\n",
    "        list: 切割後的文本段列表。\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, truncation=False)  # 不截斷，獲取所有 tokens\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_length, len(tokens))\n",
    "        chunk = tokens[start:end]  # 當前窗口的 tokens\n",
    "        chunks.append(chunk)\n",
    "        if end == len(tokens):  # 最後一段退出\n",
    "            break\n",
    "        start += stride  # 滑動窗口開始位置\n",
    "    # 將 tokens 解碼回文本\n",
    "    return [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n",
    "\n",
    "# 初始化列表存放處理後的數據\n",
    "processed_data = []\n",
    "\n",
    "# 讀取 CSV 文件\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# 遍歷 DataFrame 中的每行並顯示進度條\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Processing {csv_file}\", leave=False):\n",
    "    pdf_name = row['PDF']  # PDF 文件名稱\n",
    "    page_number = row['Page']  # 頁碼\n",
    "    label = row['Label']  # 標籤\n",
    "    paragraph = row['Text']  # 需要處理的文本內容\n",
    "    \n",
    "    # 對段落進行處理和滑動窗口切割\n",
    "    chunks = preprocess_and_split_with_overlap(paragraph)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        processed_data.append({\n",
    "            'PDF': pdf_name,\n",
    "            'Page': page_number,\n",
    "            'Label': label,\n",
    "            'Text': chunk\n",
    "        })\n",
    "\n",
    "# 將處理後的數據轉為 DataFrame 並保存\n",
    "processed_df = pd.DataFrame(processed_data)\n",
    "processed_df.to_csv('Processed_FN_Data.csv', index=False)\n",
    "\n",
    "print(\"處理完成，已保存為 'Processed_FN_Data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded knowledge graph with 13 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19972\\2680819668.py:99: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  valid_data[\"Label\"] = valid_data[\"Label\"].fillna(method=\"ffill\")  # 用前一個非 NaN 值填補 Label\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19972\\2680819668.py:100: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  valid_data[\"Text\"] = valid_data[\"Text\"].fillna(method=\"ffill\")  # 用前一個非 NaN 值填補 Text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 145859 samples for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 9117/9117 [22:06<00:00,  6.88it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# 加載微調後的模型\n",
    "model = BertForQuestionAnswering.from_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# 加載知識圖譜\n",
    "with open(r\"C:\\Users\\User\\Dropbox\\textmining1\\KG\\definition_moreinfo.json\", \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    knowledge_graph = json.load(f)\n",
    "\n",
    "# 建立名稱到定義的映射\n",
    "name_to_definition = {item[\"Name\"]: item[\"Definition\"] for item in knowledge_graph}\n",
    "\n",
    "print(f\"Loaded knowledge graph with {len(knowledge_graph)} entries.\")\n",
    "\n",
    "# 定義檢索知識圖譜的函數\n",
    "def retrieve_from_knowledge_graph(answer, knowledge_graph):\n",
    "    \"\"\"\n",
    "    根據模型的答案在知識圖譜中進行檢索。\n",
    "    \"\"\"\n",
    "    related_definitions = [entry for entry in knowledge_graph if entry[\"Name\"].lower() in answer.lower()]\n",
    "    return related_definitions if related_definitions else [{\"Name\": \"No Match\", \"Definition\": \"No related definition found.\"}]\n",
    "\n",
    "# 定義 RAG 測試管線\n",
    "def rag_test_pipeline(row, model, tokenizer, knowledge_graph):\n",
    "    \"\"\"\n",
    "    單條測試管線，適配切割後的資料。\n",
    "    \"\"\"\n",
    "    pdf_file = row['PDF']  # PDF 文件名稱\n",
    "    page_number = row['Page']  # 頁碼\n",
    "    label = row['Label']  # 標籤\n",
    "    paragraph = row['Text']\n",
    "    \n",
    "    # 定義測試問題\n",
    "    question = f\"What is the judge's opinion regarding entry barriers in case from {pdf_file}, page {page_number}?\"\n",
    "    \n",
    "    # 預測答案\n",
    "    inputs = tokenizer.encode_plus(question, paragraph, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    \n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "    \n",
    "    # 檢索知識圖譜\n",
    "    related_definitions = retrieve_from_knowledge_graph(answer, knowledge_graph)\n",
    "    \n",
    "    return {\n",
    "        \"PDF\": pdf_file,\n",
    "        \"Page\": page_number,\n",
    "        \"Label\": label,\n",
    "        \"Question\": question,\n",
    "        \"Predicted Answer\": answer,\n",
    "        \"Knowledge Graph Result\": related_definitions\n",
    "    }\n",
    "\n",
    "# 批量處理的預測函數\n",
    "def batch_predict(model, tokenizer, questions, contexts, batch_size=16):\n",
    "    \"\"\"\n",
    "    批量處理問題和上下文，進行預測\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for i in tqdm(range(0, len(questions), batch_size), desc=\"Predicting\"):\n",
    "        batch_questions = questions[i:i + batch_size]\n",
    "        batch_contexts = contexts[i:i + batch_size]\n",
    "        \n",
    "        inputs = tokenizer(batch_questions, batch_contexts, return_tensors=\"pt\", \n",
    "                           padding=True, truncation=True, max_length=512).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        for j in range(len(batch_questions)):\n",
    "            start = torch.argmax(outputs.start_logits[j]).item()\n",
    "            end = torch.argmax(outputs.end_logits[j]).item() + 1\n",
    "            input_ids = inputs[\"input_ids\"][j].tolist()\n",
    "            prediction = tokenizer.convert_tokens_to_string(\n",
    "                tokenizer.convert_ids_to_tokens(input_ids[start:end])\n",
    "            )\n",
    "            predictions.append(prediction)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 加載處理後的數據\n",
    "valid_data = pd.read_csv(\"Processed_FN_Data.csv\")\n",
    "\n",
    "# **填補 NaN 值**\n",
    "valid_data[\"Label\"] = valid_data[\"Label\"].fillna(method=\"ffill\")  # 用前一個非 NaN 值填補 Label\n",
    "valid_data[\"Text\"] = valid_data[\"Text\"].fillna(method=\"ffill\")  # 用前一個非 NaN 值填補 Text\n",
    "\n",
    "# **確保所有 Text 欄位都是字串**\n",
    "valid_data[\"Text\"] = valid_data[\"Text\"].astype(str)\n",
    "\n",
    "# 提取問題和上下文\n",
    "questions = [\n",
    "    f\"What is the judge's opinion regarding entry barriers in case from {pdf}, page {page}?\"\n",
    "    for pdf, page in zip(valid_data[\"PDF\"], valid_data[\"Page\"])\n",
    "]\n",
    "contexts = valid_data[\"Text\"].tolist()\n",
    "\n",
    "print(f\"Loaded {len(questions)} samples for validation.\")\n",
    "\n",
    "# 執行批量預測\n",
    "predicted_answers = batch_predict(model, tokenizer, questions, contexts)\n",
    "\n",
    "# 生成結果\n",
    "def generate_results(valid_data, questions, contexts, predicted_answers, knowledge_graph):\n",
    "    \"\"\"\n",
    "    將驗證數據和模型預測結果封裝成包含 PDF 和頁碼的結果列表。\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for pdf, page, label, question, context, prediction in zip(\n",
    "        valid_data[\"PDF\"], valid_data[\"Page\"], valid_data[\"Label\"], questions, contexts, predicted_answers):\n",
    "        \n",
    "        related_definitions = retrieve_from_knowledge_graph(prediction, knowledge_graph)\n",
    "        results.append({\n",
    "            \"PDF\": pdf,\n",
    "            \"Page\": page,\n",
    "            \"Label\": label,\n",
    "            \"Question\": question,\n",
    "            \"Context\": context,\n",
    "            \"Predicted Answer\": prediction,\n",
    "            \"Knowledge Graph Result\": related_definitions\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# 調用函數生成 results\n",
    "results = generate_results(valid_data, questions, contexts, predicted_answers, knowledge_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'RAG_Test_Results.json'\n",
      "PDF: a1\n",
      "Page: 33\n",
      "Label: 1\n",
      "Question: What is the judge's opinion regarding entry barriers in case from a1, page 33?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "PDF: a1\n",
      "Page: 33\n",
      "Label: 2\n",
      "Question: What is the judge's opinion regarding entry barriers in case from a1, page 33?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "PDF: a1\n",
      "Page: 34\n",
      "Label: 2\n",
      "Question: What is the judge's opinion regarding entry barriers in case from a1, page 34?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "PDF: a1\n",
      "Page: 35\n",
      "Label: 3\n",
      "Question: What is the judge's opinion regarding entry barriers in case from a1, page 35?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "PDF: a1\n",
      "Page: 36\n",
      "Label: 4\n",
      "Question: What is the judge's opinion regarding entry barriers in case from a1, page 36?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 將結果保存為 JSON 文件\n",
    "import json\n",
    "# 將結果保存為 JSON 文件\n",
    "with open(\"RAG_Test_Results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Results saved to 'RAG_Test_Results.json'\")\n",
    "\n",
    "# 查看部分結果\n",
    "for result in results[:5]:\n",
    "    print(\"PDF:\", result[\"PDF\"])\n",
    "    print(\"Page:\", result[\"Page\"])\n",
    "    print(\"Label:\", result[\"Label\"])\n",
    "    print(\"Question:\", result[\"Question\"])\n",
    "    print(\"Predicted Answer:\", result[\"Predicted Answer\"])\n",
    "    print(\"Knowledge Graph Result:\", result[\"Knowledge Graph Result\"])\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 matched results.\n",
      "PDF: b1\n",
      "Page: 233\n",
      "Label: 38\n",
      "Question: What is the judge's opinion regarding entry barriers in case from b1, page 233?\n",
      "Context: and, a fortiori, as the district court concluded, spcc has failed to prove that at & t engaged in predatory pricing under the areeda - turner test. ...\n",
      "Predicted Answer: [SEP] and , a fortiori , as the district court concluded , spcc has failed to prove that at & t engaged in predatory pricing under the areeda - turner test . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: b5\n",
      "Page: 979\n",
      "Label: 24\n",
      "Question: What is the judge's opinion regarding entry barriers in case from b5, page 979?\n",
      "Context: . the court has also discussed whether there was below marginal cost pricing of any product, aside from the question of whether that product had sufficient market importance to even support a claim of predatory pricing, and aside from the question of whether the rebates should be disregarded. were the only issue before the court whether, ignoring the rebates, the fly - drives were sold at below marginal or average variable cost, the court would find that there are disputed questions of fact on t ...\n",
      "Predicted Answer: [SEP] . the court has also discussed whether there was below marginal cost pricing of any product , aside from the question of whether that product had sufficient market importance to even support a claim of predatory pricing , and aside from the question of whether the rebates should be disregarded . were the only issue before the court whether , ignoring the rebates , the fly - drives were sold at below marginal or average variable cost , the court would find that there are disputed questions of fact on this issue . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: b7\n",
      "Page: 825\n",
      "Label: 13\n",
      "Question: What is the judge's opinion regarding entry barriers in case from b7, page 825?\n",
      "Context: two commentators recently proposed a means of selecting rules directed against predatory conduct on a basis similar to this one. jaskow & klevorick, a framework for analyzing predatory pricing policy, 89 yale l. j. 213 ( 1979 ). ...\n",
      "Predicted Answer: [SEP] two commentators recently proposed a means of selecting rules directed against predatory conduct on a basis similar to this one . jaskow & klevorick , a framework for analyzing predatory pricing policy , 89 yale l . j . 213 ( 1979 ) . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: b9\n",
      "Page: 1252\n",
      "Label: 3\n",
      "Question: What is the judge's opinion regarding entry barriers in case from b9, page 1252?\n",
      "Context: it has been suggested that one of the factors to consider in determining the consequences of vertical integration is the degree of competition between the two levels. note, refusals to deal by vertically integrated monopolists, 87 harv. l. rev. 1720, 1726 ( 1974 ). that is, the higher the degree of competition between the two levels, the lesser the benefit to be anticipated by vertical integration. given the minimum degree of competition, or potential competition, between the star and the indepe ...\n",
      "Predicted Answer: [SEP] it has been suggested that one of the factors to consider in determining the consequences of vertical integration is the degree of competition between the two levels . note , refusals to deal by vertically integrated monopolists , 87 harv . l . rev . 1720 , 1726 ( 1974 ) . that is , the higher the degree of competition between the two levels , the lesser the benefit to be anticipated by vertical integration . given the minimum degree of competition , or potential competition , between the star and the independent carriers , one might expect little by way of adverse effect of forward integration . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Vertical Integration', 'Definition': \"Control over supply chain that limits competitors' access to resources or distribution.\"}]\n",
      "==================================================\n",
      "PDF: c1\n",
      "Page: 26\n",
      "Label: 8\n",
      "Question: What is the judge's opinion regarding entry barriers in case from c1, page 26?\n",
      "Context: belcher attempts to distinguish atlantic richfield on the ground that it concerned a claim of a vertical maximum price fixing conspiracy, whereas the instant case concerns an alleged horizontal maximum price fixing scheme. this distinction is irrelevant to the question of antitrust injury. atlantic richfield clearly states that \" in the context of pricing practices, only predatory pricing has the requisite anticompetitive effect. \" 495 u. s. at, 110 s. ct. at 1892, 109 l. ed. 2d at 347. ...\n",
      "Predicted Answer: [SEP] belcher attempts to distinguish atlantic richfield on the ground that it concerned a claim of a vertical maximum price fixing conspiracy , whereas the instant case concerns an alleged horizontal maximum price fixing scheme . this distinction is irrelevant to the question of antitrust injury . atlantic richfield clearly states that \" in the context of pricing practices , only predatory pricing has the requisite anticompetitive effect . \" 495 u . s . at , 110 s . ct . at 1892 , 109 l . ed . 2d at 347 . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: c9\n",
      "Page: 376\n",
      "Label: 17\n",
      "Question: What is the judge's opinion regarding entry barriers in case from c9, page 376?\n",
      "Context: e. g., williamson, predatory pricing : a strategic and welfare analysis, 87 yale l. j. 284 ( 1977 ). areeda and turner's redefinition of average variable cost also blunts this criticism. see supra n. 16. ...\n",
      "Predicted Answer: [SEP] e . g . , williamson , predatory pricing : a strategic and welfare analysis , 87 yale l . j . 284 ( 1977 ) . areeda and turner ' s redefinition of average variable cost also blunts this criticism . see supra n . 16 . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: e1\n",
      "Page: 75\n",
      "Label: 137\n",
      "Question: What is the judge's opinion regarding entry barriers in case from e1, page 75?\n",
      "Context: as professors areeda and hovenkamp explain : the essential facility doctrine concerns vertical integration - - in particular, the duty of a vertically integrated monopolist to share some input in a vertically related market, which we call market # 1, with someone operating in an upstream or downstream market, which we shall call market # 2. if the facility is truly \" essential, \" then the # 1 monopoly facility also establishes a # 2 monopoly. iiia areeda & hovenkamp, supra, p 771a, at 172 ( rev. ...\n",
      "Predicted Answer: [SEP] as professors areeda and hovenkamp explain : the essential facility doctrine concerns vertical integration - - in particular , the duty of a vertically integrated monopolist to share some input in a vertically related market , which we call market # 1 , with someone operating in an upstream or downstream market , which we shall call market # 2 . if the facility is truly \" essential , \" then the # 1 monopoly facility also establishes a # 2 monopoly . iiia areeda & hovenkamp , supra , p 771a , at 172 ( rev . ed . 1996 ) . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Vertical Integration', 'Definition': \"Control over supply chain that limits competitors' access to resources or distribution.\"}]\n",
      "==================================================\n",
      "PDF: e3\n",
      "Page: 174\n",
      "Label: 26\n",
      "Question: What is the judge's opinion regarding entry barriers in case from e3, page 174?\n",
      "Context: morgan v. ponder, 892 f. 2d 1355, 1359 - 60 ( 8th cir. 1989 ) ( \" objective cost analysis is the crucial component in a prima facia case of predatory pricing ; \" prices above average total cost are per se legal ; \" as our cases have made clear, evidence of statements [ of subjective intent ] will not relieve a plaintiff of the burden of proving predation through a separate showing of predatory conduct. \" ). ...\n",
      "Predicted Answer: [SEP] morgan v . ponder , 892 f . 2d 1355 , 1359 - 60 ( 8th cir . 1989 ) ( \" objective cost analysis is the crucial component in a prima facia case of predatory pricing ; \" prices above average total cost are per se legal ; \" as our cases have made clear , evidence of statements [ of subjective intent ] will not relieve a plaintiff of the burden of proving predation through a separate showing of predatory conduct . \" ) . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: e4\n",
      "Page: 1371\n",
      "Label: 17\n",
      "Question: What is the judge's opinion regarding entry barriers in case from e4, page 1371?\n",
      "Context: the indiana grocery court indicated that the rule would not apply when predatory pricing was present. 684 f. supp. at 583. predatory pricing is \" pricing below some appropriate measure of cost. \" matsushita, 475 u. s. at 584 - 85 n. 8. in the present case, there is no allegation that the maximum price which the defendants allegedly sought to have the plaintiff charge was a predatory price. indeed, it was the price charged by allopathic practitioners for emgs. ...\n",
      "Predicted Answer: [SEP] the indiana grocery court indicated that the rule would not apply when predatory pricing was present . 684 f . supp . at 583 . predatory pricing is \" pricing below some appropriate measure of cost . \" matsushita , 475 u . s . at 584 - 85 n . 8 . in the present case , there is no allegation that the maximum price which the defendants allegedly sought to have the plaintiff charge was a predatory price . indeed , it was the price charged by allopathic practitioners for emgs . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: e5\n",
      "Page: 1098\n",
      "Label: 4\n",
      "Question: What is the judge's opinion regarding entry barriers in case from e5, page 1098?\n",
      "Context: the main sources of entry barriers are : ( 1 ) legal license ; ( 2 ) control over an essential or superior resource ; ( 3 ) entrenched buyer preferences for established brands or company reputations ; and ( 4 ) capital market evaluations imposing higher capital costs on new entrants. economies of scale may also be considered an entry barrier in some situations. 2 areeda & turner, antitrust law par. 409b at 299 - 300 ( 1978 ). l. a. land points to no evidence which fits in any of these categories ...\n",
      "Predicted Answer: [SEP] the main sources of entry barriers are : ( 1 ) legal license ; ( 2 ) control over an essential or superior resource ; ( 3 ) entrenched buyer preferences for established brands or company reputations ; and ( 4 ) capital market evaluations imposing higher capital costs on new entrants . economies of scale may also be considered an entry barrier in some situations . 2 areeda & turner , antitrust law par . 409b at 299 - 300 ( 1978 ) . l . a . land points to no evidence which fits in any of these categories . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Entry Barriers', 'Definition': 'Factors that deter or prevent new competitors from entering a market.', 'Synonyms': ['Barriers', 'Barrier', 'Barrier to Entry', 'Entry Costs', 'Entry Restrictions']}, {'Name': 'Economies of Scale', 'Definition': 'Cost advantages due to increased production, making it harder for new entrants to compete.'}]\n",
      "==================================================\n",
      "PDF: e7\n",
      "Page: 1134\n",
      "Label: 10\n",
      "Question: What is the judge's opinion regarding entry barriers in case from e7, page 1134?\n",
      "Context: if the other substantive elements of an attempt to monopolize claim are satisfied. id. at 168. we are reluctant to apply such bright - line rules regarding market share in deciding whether a defendant has market power to restrict output or raise prices. courts should be \" wary of the numbers game of market percentage \" when considering attempt - to - monopolize claims. dimmitt agri industries, 679 f. 2d at 533. as professors areeda and turner admit, their refined rules may be \" illusory \" guides ...\n",
      "Predicted Answer: [SEP] if the other substantive elements of an attempt to monopolize claim are satisfied . id . at 168 . we are reluctant to apply such bright - line rules regarding market share in deciding whether a defendant has market power to restrict output or raise prices . courts should be \" wary of the numbers game of market percentage \" when considering attempt - to - monopolize claims . dimmitt agri industries , 679 f . 2d at 533 . as professors areeda and turner admit , their refined rules may be \" illusory \" guides to deciding market power . areeda & turner par . 835c , at 350 . the far wiser approach , which this circuit has observed if not explicitly adopted , is illustrated by ball memorial hosp . , 784 f . 2d at 1335 ( 7th cir . ) and ryko mfg . , 823 f . 2d at 1232 ( 8th cir . ) , where hn22 [ ] the issue of market power was decided by carefully analyzing certain telltale factors in the relevant market : market share , entry barriers and the capacity of existing competitors to expand output . we see no reason to depart from this mode of analysis . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Entry Barriers', 'Definition': 'Factors that deter or prevent new competitors from entering a market.', 'Synonyms': ['Barriers', 'Barrier', 'Barrier to Entry', 'Entry Costs', 'Entry Restrictions']}]\n",
      "==================================================\n",
      "PDF: e8\n",
      "Page: 390\n",
      "Label: 4\n",
      "Question: What is the judge's opinion regarding entry barriers in case from e8, page 390?\n",
      "Context: \" operational synergies \" is a term used in the newspaper industry to refer to the economies of scale that can be achieved by combining functions or departments, including accounting, administration, press rooms, and composing departments. ...\n",
      "Predicted Answer: [SEP] \" operational synergies \" is a term used in the newspaper industry to refer to the economies of scale that can be achieved by combining functions or departments , including accounting , administration , press rooms , and composing departments . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Economies of Scale', 'Definition': 'Cost advantages due to increased production, making it harder for new entrants to compete.'}]\n",
      "==================================================\n",
      "PDF: e10\n",
      "Page: 1225\n",
      "Label: 8\n",
      "Question: What is the judge's opinion regarding entry barriers in case from e10, page 1225?\n",
      "Context: in its complaint, plaintiff also included allegations regarding vertical integration. plaintiff informed the court at oral argument it does not wish to pursue this claim. ...\n",
      "Predicted Answer: [SEP] in its complaint , plaintiff also included allegations regarding vertical integration . plaintiff informed the court at oral argument it does not wish to pursue this claim . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Vertical Integration', 'Definition': \"Control over supply chain that limits competitors' access to resources or distribution.\"}]\n",
      "==================================================\n",
      "PDF: f3\n",
      "Page: 286\n",
      "Label: 15\n",
      "Question: What is the judge's opinion regarding entry barriers in case from f3, page 286?\n",
      "Context: merger gave a firm the structural power to engage in predatory pricing. a damages action challenging such a merger would fail as long as predation were merely possible or even likely. as a result, the statute of limitation would not run on such a damage claim. however, once unlawful predation began and the plaintiff could show the merger facilitated the predation, then the statute of limitation would not bar a challenge to the merger itself... \" ). whether the distinctive use is referred to as a ...\n",
      "Predicted Answer: [SEP] merger gave a firm the structural power to engage in predatory pricing . a damages action challenging such a merger would fail as long as predation were merely possible or even likely . as a result , the statute of limitation would not run on such a damage claim . however , once unlawful predation began and the plaintiff could show the merger facilitated the predation , then the statute of limitation would not bar a challenge to the merger itself . . . \" ) . whether the distinctive use is referred to as a continuing violation or different holding and use , the idea is the same . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: g3\n",
      "Page: 921\n",
      "Label: 16\n",
      "Question: What is the judge's opinion regarding entry barriers in case from g3, page 921?\n",
      "Context: in matsushita, american producers alleged their japanese competitors ( the defendants ) maintained a monopoly over the japanese domestic television market and were using the profits derived in japan to fund a predatory pricing scheme in america. the supreme court refused to find an antitrust violation. the court stated : nor does the possibility that [ the defendants ] have obtained supracompetitive profits in the japanese market change this calculation. whether or not [ the defendants ] have th ...\n",
      "Predicted Answer: [SEP] in matsushita , american producers alleged their japanese competitors ( the defendants ) maintained a monopoly over the japanese domestic television market and were using the profits derived in japan to fund a predatory pricing scheme in america . the supreme court refused to find an antitrust violation . the court stated : nor does the possibility that [ the defendants ] have obtained supracompetitive profits in the japanese market change this calculation . whether or not [ the defendants ] have the means to sustain substantial losses in this country over a long period of time , they have no motive to sustain such losses absent some strong likelihood that the alleged conspiracy in this country will eventually pay off . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: g3\n",
      "Page: 1598\n",
      "Label: 10\n",
      "Question: What is the judge's opinion regarding entry barriers in case from g3, page 1598?\n",
      "Context: matsushita seemed to raise the possibility that there might be a special summary judgment standard for antitrust cases in general, or predatory pricing cases in particular. however, the supreme court later made clear in eastman kodak co. v. image technical serv's., ind., 504 u. s. 451, 468 - 69, 112 s. ct. 2072, 2083, 119 l. ed. 2d 265 ( 1992 ), that there was no special standard. see also advo v. philadelphia newspapers, inc., 51 f. 3d 1191, 1195 ( 3d cir. 1995 ). ...\n",
      "Predicted Answer: [SEP] matsushita seemed to raise the possibility that there might be a special summary judgment standard for antitrust cases in general , or predatory pricing cases in particular . however , the supreme court later made clear in eastman kodak co . v . image technical serv ' s . , ind . , 504 u . s . 451 , 468 - 69 , 112 s . ct . 2072 , 2083 , 119 l . ed . 2d 265 ( 1992 ) , that there was no special standard . see also advo v . philadelphia newspapers , inc . , 51 f . 3d 1191 , 1195 ( 3d cir . 1995 ) . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: g9\n",
      "Page: 1232\n",
      "Label: 12\n",
      "Question: What is the judge's opinion regarding entry barriers in case from g9, page 1232?\n",
      "Context: past losses. brooke group, ltd. v. brown & williamson tobacco corp., 509 u. s. 209, 222 - 24, 125 l. ed. 2d 168, 113 s. ct. 2578 ( 1993 ) ; richter concrete corp. v. hilltop concrete corp., 691 f. 2d 818, 823 ( 6th cir. 1982 ). where the defendant's prices are below the \" average variable costs \" of producing the product, a prima facie case of predatory pricing is established and the burden shifts to the defendant to show that the pricing was justified. d. e. rogers assoc., inc., 718 f. 2d 1431, ...\n",
      "Predicted Answer: [SEP] past losses . brooke group , ltd . v . brown & williamson tobacco corp . , 509 u . s . 209 , 222 - 24 , 125 l . ed . 2d 168 , 113 s . ct . 2578 ( 1993 ) ; richter concrete corp . v . hilltop concrete corp . , 691 f . 2d 818 , 823 ( 6th cir . 1982 ) . where the defendant ' s prices are below the \" average variable costs \" of producing the product , a prima facie case of predatory pricing is established and the burden shifts to the defendant to show that the pricing was justified . d . e . rogers assoc . , inc . , 718 f . 2d 1431 , 1436 ( 6th cir . 1983 ) . where the pricing is above \" average variable costs \" but below the defendant ' s \" total costs \" it is presumed that the pricing scheme is not predatory and the plaintiff may not prevail absent detailed and convincing economic [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: h2\n",
      "Page: 818\n",
      "Label: 2\n",
      "Question: What is the judge's opinion regarding entry barriers in case from h2, page 818?\n",
      "Context: in matsushita, [ * * 41 ] the supreme court concluded that the alleged conspirators would have had little motive to enter into the alleged conspiracy, which was economically implausible because it involved an inherently speculative and rarely successful predatory pricing scheme, intended to charged lower - than - competitive prices to drive out competitors ; it involved not merely a single company, but 21 companies ; each conspirator would have had a strong incentive to cheat during the conspira ...\n",
      "Predicted Answer: [SEP] in matsushita , [ * * 41 ] the supreme court concluded that the alleged conspirators would have had little motive to enter into the alleged conspiracy , which was economically implausible because it involved an inherently speculative and rarely successful predatory pricing scheme , intended to charged lower - than - competitive prices to drive out competitors ; it involved not merely a single company , but 21 companies ; each conspirator would have had a strong incentive to cheat during the conspiracy , and cheating would have been fatal to the conspiracy ; and even after 20 years , the conspiracy had not yet succeeded , as the conspirators had not yet gained significant market share , even though such a conspiracy requires a long period of success to recoup losses sustained during the period of predatory pricing . see matsushita , 475 u . s . at 589 - 93 . the present case certainly does not involve such an unlikely conspiracy . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: h3\n",
      "Page: 786\n",
      "Label: 12\n",
      "Question: What is the judge's opinion regarding entry barriers in case from h3, page 786?\n",
      "Context: there is no allegation of predatory pricing here. ...\n",
      "Predicted Answer: [SEP] there is no allegation of predatory pricing here . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: i6\n",
      "Page: 1460\n",
      "Label: 190\n",
      "Question: What is the judge's opinion regarding entry barriers in case from i6, page 1460?\n",
      "Context: id. at ja6776 ( elhauge report ) ( economies of scale ). ...\n",
      "Predicted Answer: [SEP] id . at ja6776 ( elhauge report ) ( economies of scale ) . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Economies of Scale', 'Definition': 'Cost advantages due to increased production, making it harder for new entrants to compete.'}]\n",
      "==================================================\n",
      "PDF: i7\n",
      "Page: 940\n",
      "Label: 60\n",
      "Question: What is the judge's opinion regarding entry barriers in case from i7, page 940?\n",
      "Context: bolton, brodley and riordan \" predatory pricing : strategic theory and legal policy \" ( 2000 ) 88 geo law journal 2239 at 2244. ...\n",
      "Predicted Answer: [SEP] bolton , brodley and riordan \" predatory pricing : strategic theory and legal policy \" ( 2000 ) 88 geo law journal 2239 at 2244 . [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "Found 21 matched results.\n"
     ]
    }
   ],
   "source": [
    "# 載入保存的結果\n",
    "import json\n",
    "\n",
    "# 加載結果\n",
    "with open(\"RAG_FN_Results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# 過濾有匹配的結果\n",
    "matched_results = [\n",
    "    result for result in results if result[\"Knowledge Graph Result\"][0][\"Name\"] != \"No Match\"\n",
    "]\n",
    "\n",
    "# 列出有匹配的結果\n",
    "print(f\"Found {len(matched_results)} matched results.\")\n",
    "for result in matched_results:\n",
    "    print(\"PDF:\", result[\"PDF\"])\n",
    "    print(\"Page:\", result[\"Page\"])\n",
    "    print(\"Label:\", result[\"Label\"])\n",
    "    print(\"Question:\", result[\"Question\"])\n",
    "    print(\"Context:\", result[\"Context\"][:500], \"...\")\n",
    "    print(\"Predicted Answer:\", result[\"Predicted Answer\"])\n",
    "    print(\"Knowledge Graph Result:\", result[\"Knowledge Graph Result\"])\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# 列出匹配的結果數量\n",
    "print(f\"Found {len(matched_results)} matched results.\")\n",
    "\n",
    "# 保存匹配結果到新文件\n",
    "output_file = \"Matched_FN_DATA.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(matched_results, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "處理完成，已保存為 'Processed_HN_Data.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 設置 CSV 文件路徑\n",
    "csv_file = r'/Users/tangjiahong/Dropbox/textmining1/Data_Clean/CSV/merge_CSV/merged_HN.csv'\n",
    "\n",
    "# 初始化分詞器\n",
    "tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "\n",
    "def preprocess_and_split_with_overlap(text, max_length=256, stride=128):\n",
    "    \"\"\"\n",
    "    使用滑動窗口切割長文本，保留一定的上下文重疊。\n",
    "    \n",
    "    Args:\n",
    "        text (str): 待切割的文本。\n",
    "        max_length (int): 每個段落的最大 token 長度。\n",
    "        stride (int): 滑動窗口的步長，重疊部分長度 = max_length - stride。\n",
    "        \n",
    "    Returns:\n",
    "        list: 切割後的文本段列表。\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, truncation=False)  # 不截斷，獲取所有 tokens\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_length, len(tokens))\n",
    "        chunk = tokens[start:end]  # 當前窗口的 tokens\n",
    "        chunks.append(chunk)\n",
    "        if end == len(tokens):  # 最後一段退出\n",
    "            break\n",
    "        start += stride  # 滑動窗口開始位置\n",
    "    # 將 tokens 解碼回文本\n",
    "    return [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n",
    "\n",
    "# 初始化列表存放處理後的數據\n",
    "processed_data = []\n",
    "\n",
    "# 讀取 CSV 文件\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# 遍歷 DataFrame 中的每行並顯示進度條\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Processing {csv_file}\", leave=False):\n",
    "    pdf_name = row['PDF']  # PDF 文件名稱\n",
    "    category = row['Category']  # 類別\n",
    "    content = row['Content']  # 需要處理的文本內容\n",
    "    \n",
    "    # 對內容進行處理和滑動窗口切割\n",
    "    chunks = preprocess_and_split_with_overlap(content)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        processed_data.append({\n",
    "            'PDF': pdf_name,\n",
    "            'Category': category,\n",
    "            'Content': chunk\n",
    "        })\n",
    "\n",
    "# 將處理後的數據轉為 DataFrame 並保存\n",
    "processed_df = pd.DataFrame(processed_data)\n",
    "processed_df.to_csv('Processed_HN_Data.csv', index=False)\n",
    "\n",
    "print(\"處理完成，已保存為 'Processed_HN_Data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded knowledge graph with 13 entries.\n",
      "Loaded 254703 samples for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 15919/15919 [08:42<00:00, 30.45it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# 加載微調後的模型\n",
    "model = BertForQuestionAnswering.from_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# 加載知識圖譜\n",
    "with open(r\"C:\\Users\\User\\Dropbox\\textmining1\\KG\\definition_moreinfo.json\", \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    knowledge_graph = json.load(f)\n",
    "\n",
    "# 建立名稱到定義的映射\n",
    "name_to_definition = {item[\"Name\"]: item[\"Definition\"] for item in knowledge_graph}\n",
    "\n",
    "print(f\"Loaded knowledge graph with {len(knowledge_graph)} entries.\")\n",
    "\n",
    "# 定義檢索知識圖譜的函數\n",
    "def retrieve_from_knowledge_graph(answer, knowledge_graph):\n",
    "    \"\"\"\n",
    "    根據模型的答案在知識圖譜中進行檢索。\n",
    "    \"\"\"\n",
    "    related_definitions = [entry for entry in knowledge_graph if entry[\"Name\"].lower() in answer.lower()]\n",
    "    return related_definitions if related_definitions else [{\"Name\": \"No Match\", \"Definition\": \"No related definition found.\"}]\n",
    "\n",
    "# 定義 RAG 測試管線\n",
    "def rag_test_pipeline(row, model, tokenizer, knowledge_graph):\n",
    "    \"\"\"\n",
    "    單條測試管線，適配切割後的資料。\n",
    "    \"\"\"\n",
    "    pdf_file = row['PDF']  # PDF 文件名稱\n",
    "    category = row['Category']  # 類別\n",
    "    content = row['Content']\n",
    "    \n",
    "    # 定義測試問題\n",
    "    question = f\"What is the judge's opinion regarding entry barriers in case from {pdf_file} in category {category}?\"\n",
    "    \n",
    "    # 預測答案\n",
    "    inputs = tokenizer.encode_plus(question, content, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    \n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "    \n",
    "    # 檢索知識圖譜\n",
    "    related_definitions = retrieve_from_knowledge_graph(answer, knowledge_graph)\n",
    "    \n",
    "    return {\n",
    "        \"PDF\": pdf_file,\n",
    "        \"Category\": category,\n",
    "        \"Question\": question,\n",
    "        \"Predicted Answer\": answer,\n",
    "        \"Knowledge Graph Result\": related_definitions\n",
    "    }\n",
    "\n",
    "# 批量處理的預測函數\n",
    "def batch_predict(model, tokenizer, questions, contexts, batch_size=16):\n",
    "    \"\"\"\n",
    "    批量處理問題和上下文，進行預測\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for i in tqdm(range(0, len(questions), batch_size), desc=\"Predicting\"):\n",
    "        batch_questions = questions[i:i + batch_size]\n",
    "        batch_contexts = contexts[i:i + batch_size]\n",
    "        \n",
    "        inputs = tokenizer(batch_questions, batch_contexts, return_tensors=\"pt\", \n",
    "                           padding=True, truncation=True, max_length=512).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        for j in range(len(batch_questions)):\n",
    "            start = torch.argmax(outputs.start_logits[j]).item()\n",
    "            end = torch.argmax(outputs.end_logits[j]).item() + 1\n",
    "            input_ids = inputs[\"input_ids\"][j].tolist()\n",
    "            prediction = tokenizer.convert_tokens_to_string(\n",
    "                tokenizer.convert_ids_to_tokens(input_ids[start:end])\n",
    "            )\n",
    "            predictions.append(prediction)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 加載處理後的數據\n",
    "valid_data = pd.read_csv(\"Processed_HN_Data.csv\")\n",
    "\n",
    "# 提取問題和上下文\n",
    "questions = [f\"What is the judge's opinion regarding entry barriers in case from {pdf} in category {category}?\"\n",
    "             for pdf, category in zip(valid_data[\"PDF\"], valid_data[\"Category\"])]\n",
    "contexts = valid_data[\"Content\"].tolist()\n",
    "\n",
    "print(f\"Loaded {len(questions)} samples for validation.\")\n",
    "\n",
    "# 執行批量預測\n",
    "predicted_answers = batch_predict(model, tokenizer, questions, contexts)\n",
    "\n",
    "# 生成結果\n",
    "def generate_results(valid_data, questions, contexts, predicted_answers, knowledge_graph):\n",
    "    \"\"\"\n",
    "    將驗證數據和模型預測結果封裝成包含 PDF 和類別的結果列表。\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for pdf, category, question, context, prediction in zip(\n",
    "        valid_data[\"PDF\"], valid_data[\"Category\"], questions, contexts, predicted_answers):\n",
    "        \n",
    "        related_definitions = retrieve_from_knowledge_graph(prediction, knowledge_graph)\n",
    "        results.append({\n",
    "            \"PDF\": pdf,\n",
    "            \"Category\": category,\n",
    "            \"Question\": question,\n",
    "            \"Context\": context,\n",
    "            \"Predicted Answer\": prediction,\n",
    "            \"Knowledge Graph Result\": related_definitions\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# 調用函數生成 results\n",
    "results = generate_results(valid_data, questions, contexts, predicted_answers, knowledge_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'RAG_Test_Results.json'\n",
      "Found 21 matched results.\n",
      "PDF: c1\n",
      "Category: HN10\n",
      "Question: What is the judge's opinion regarding entry barriers in case from c1 in category HN10?\n",
      "Context: antitrust & trade law >... > actual monopolization > anticompetitive & predatory practices > predatory pricing ...\n",
      "Predicted Answer: [SEP] antitrust & trade law > . . . > actual monopolization > anticompetitive & predatory practices > predatory pricing [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: c6\n",
      "Category: Headnotes\n",
      "Question: What is the judge's opinion regarding entry barriers in case from c6 in category Headnotes?\n",
      "Context: predatory pricing - - > headnote : ...\n",
      "Predicted Answer: [SEP] predatory pricing - - > headnote : [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: c6\n",
      "Category: HN7\n",
      "Question: What is the judge's opinion regarding entry barriers in case from c6 in category HN7?\n",
      "Context: antitrust & trade law >... > actual monopolization > anticompetitive & predatory practices > predatory pricing ...\n",
      "Predicted Answer: [SEP] antitrust & trade law > . . . > actual monopolization > anticompetitive & predatory practices > predatory pricing [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: d6\n",
      "Category: Headnotes\n",
      "Question: What is the judge's opinion regarding entry barriers in case from d6 in category Headnotes?\n",
      "Context: pleadings § 6 > predatory pricing conspiracy - - irrelevant ...\n",
      "Predicted Answer: [SEP] pleadings § 6 > predatory pricing conspiracy - - irrelevant [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: d17\n",
      "Category: LEdHN[28]\n",
      "Question: What is the judge's opinion regarding entry barriers in case from d17 in category LEdHN[28]?\n",
      "Context: trade practices § 9 > entry barriers - - prohibition - - ...\n",
      "Predicted Answer: [SEP] trade practices § 9 > entry barriers - - prohibition - - [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Entry Barriers', 'Definition': 'Factors that deter or prevent new competitors from entering a market.', 'Synonyms': ['Barriers', 'Barrier', 'Barrier to Entry', 'Entry Costs', 'Entry Restrictions']}]\n",
      "==================================================\n",
      "PDF: e7\n",
      "Category: HN2\n",
      "Question: What is the judge's opinion regarding entry barriers in case from e7 in category HN2?\n",
      "Context: antitrust & trade law >... > actual monopolization > anticompetitive & predatory practices > predatory pricing ...\n",
      "Predicted Answer: [SEP] antitrust & trade law > . . . > actual monopolization > anticompetitive & predatory practices > predatory pricing [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: e8\n",
      "Category: HN4\n",
      "Question: What is the judge's opinion regarding entry barriers in case from e8 in category HN4?\n",
      "Context: antitrust & trade law >... > actual monopolization > anticompetitive & predatory practices > predatory pricing ...\n",
      "Predicted Answer: [SEP] antitrust & trade law > . . . > actual monopolization > anticompetitive & predatory practices > predatory pricing [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: e8\n",
      "Category: HN4\n",
      "Question: What is the judge's opinion regarding entry barriers in case from e8 in category HN4?\n",
      "Context: antitrust & trade law >... > actual monopolization > anticompetitive & predatory practices > predatory pricing ...\n",
      "Predicted Answer: [SEP] antitrust & trade law > . . . > actual monopolization > anticompetitive & predatory practices > predatory pricing [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: f4\n",
      "Category: HN3\n",
      "Question: What is the judge's opinion regarding entry barriers in case from f4 in category HN3?\n",
      "Context: antitrust & trade law >... > actual monopolization > anticompetitive & predatory practices > predatory pricing ...\n",
      "Predicted Answer: [SEP] antitrust & trade law > . . . > actual monopolization > anticompetitive & predatory practices > predatory pricing [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: f9\n",
      "Category: HN24\n",
      "Question: What is the judge's opinion regarding entry barriers in case from f9 in category HN24?\n",
      "Context: antitrust & trade law >... > actual monopolization > anticompetitive & predatory practices > predatory pricing ...\n",
      "Predicted Answer: [SEP] antitrust & trade law > . . . > actual monopolization > anticompetitive & predatory practices > predatory pricing [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: f10\n",
      "Category: HN4\n",
      "Question: What is the judge's opinion regarding entry barriers in case from f10 in category HN4?\n",
      "Context: antitrust & trade law >... > actual monopolization > anticompetitive & predatory practices > predatory pricing ...\n",
      "Predicted Answer: [SEP] antitrust & trade law > . . . > actual monopolization > anticompetitive & predatory practices > predatory pricing [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: f10\n",
      "Category: HN4\n",
      "Question: What is the judge's opinion regarding entry barriers in case from f10 in category HN4?\n",
      "Context: antitrust & trade law >... > actual monopolization > anticompetitive & predatory practices > predatory pricing ...\n",
      "Predicted Answer: [SEP] antitrust & trade law > . . . > actual monopolization > anticompetitive & predatory practices > predatory pricing [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: g3\n",
      "Category: HN6\n",
      "Question: What is the judge's opinion regarding entry barriers in case from g3 in category HN6?\n",
      "Context: antitrust & trade law >... > actual monopolization > anticompetitive & predatory practices > predatory pricing ...\n",
      "Predicted Answer: [SEP] antitrust & trade law > . . . > actual monopolization > anticompetitive & predatory practices > predatory pricing [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: g10\n",
      "Category: LexisNexis® Headnotes\n",
      "Question: What is the judge's opinion regarding entry barriers in case from g10 in category LexisNexis® Headnotes?\n",
      "Context: antitrust & trade law >... > actual monopolization > anticompetitive & predatory practices > predatory pricing ...\n",
      "Predicted Answer: [SEP] antitrust & trade law > . . . > actual monopolization > anticompetitive & predatory practices > predatory pricing [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: h1\n",
      "Category: HN20\n",
      "Question: What is the judge's opinion regarding entry barriers in case from h1 in category HN20?\n",
      "Context: antitrust & trade law >... > actual monopolization > anticompetitive & predatory practices > predatory pricing ...\n",
      "Predicted Answer: [SEP] antitrust & trade law > . . . > actual monopolization > anticompetitive & predatory practices > predatory pricing [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: h9\n",
      "Category: HN19\n",
      "Question: What is the judge's opinion regarding entry barriers in case from h9 in category HN19?\n",
      "Context: antitrust & trade law >... > actual monopolization > anticompetitive & predatory practices > predatory pricing ...\n",
      "Predicted Answer: [SEP] antitrust & trade law > . . . > actual monopolization > anticompetitive & predatory practices > predatory pricing [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: i1\n",
      "Category: HN49\n",
      "Question: What is the judge's opinion regarding entry barriers in case from i1 in category HN49?\n",
      "Context: antitrust & trade law >... > actual monopolization > anticompetitive & predatory practices > predatory pricing ...\n",
      "Predicted Answer: [SEP] antitrust & trade law > . . . > actual monopolization > anticompetitive & predatory practices > predatory pricing [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: i2\n",
      "Category: LexisNexis® Headnotes\n",
      "Question: What is the judge's opinion regarding entry barriers in case from i2 in category LexisNexis® Headnotes?\n",
      "Context: antitrust & trade law >... > actual monopolization > anticompetitive & predatory practices > predatory pricing ...\n",
      "Predicted Answer: [SEP] antitrust & trade law > . . . > actual monopolization > anticompetitive & predatory practices > predatory pricing [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: i10\n",
      "Category: HN17\n",
      "Question: What is the judge's opinion regarding entry barriers in case from i10 in category HN17?\n",
      "Context: antitrust & trade law >... > actual monopolization > anticompetitive & predatory practices > predatory pricing ...\n",
      "Predicted Answer: [SEP] antitrust & trade law > . . . > actual monopolization > anticompetitive & predatory practices > predatory pricing [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: j8\n",
      "Category: HN16\n",
      "Question: What is the judge's opinion regarding entry barriers in case from j8 in category HN16?\n",
      "Context: antitrust & trade law >... > actual monopolization > anticompetitive & predatory practices > predatory pricing ...\n",
      "Predicted Answer: [SEP] antitrust & trade law > . . . > actual monopolization > anticompetitive & predatory practices > predatory pricing [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n",
      "PDF: j9\n",
      "Category: HN22\n",
      "Question: What is the judge's opinion regarding entry barriers in case from j9 in category HN22?\n",
      "Context: antitrust & trade law >... > actual monopolization > anticompetitive & predatory practices > predatory pricing ...\n",
      "Predicted Answer: [SEP] antitrust & trade law > . . . > actual monopolization > anticompetitive & predatory practices > predatory pricing [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Predatory Pricing', 'Definition': 'Deliberate underpricing to drive out competitors or prevent new entry.'}]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 將結果保存為 JSON 文件\n",
    "import json\n",
    "\n",
    "# 將結果保存為 JSON 文件\n",
    "with open(\"RAG_Test_Results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Results saved to 'RAG_Test_Results.json'\")\n",
    "\n",
    "# 加載結果\n",
    "with open(\"RAG_Test_Results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# 過濾有匹配的結果\n",
    "matched_results = [\n",
    "    result for result in results if result[\"Knowledge Graph Result\"][0][\"Name\"] != \"No Match\"\n",
    "]\n",
    "\n",
    "# 列出有匹配的結果\n",
    "print(f\"Found {len(matched_results)} matched results.\")\n",
    "for result in matched_results:\n",
    "    print(\"PDF:\", result[\"PDF\"])\n",
    "    print(\"Category:\", result[\"Category\"])\n",
    "    print(\"Question:\", result[\"Question\"])\n",
    "    print(\"Context:\", result[\"Context\"][:500], \"...\")\n",
    "    print(\"Predicted Answer:\", result[\"Predicted Answer\"])\n",
    "    print(\"Knowledge Graph Result:\", result[\"Knowledge Graph Result\"])\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "# 保存匹配結果到新文件\n",
    "output_file = \"Matched_HN_DATA.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(matched_results, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoreTerms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing /Users/tangjiahong/Dropbox/textmining1/Data_Clean/CSV/merge_CSV/merge_CoreTerms.csv: 100%|██████████| 9259/9259 [00:10<00:00, 907.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "處理完成，已保存為 'Processed_CoreTerms_Data.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 設置 CSV 文件路徑\n",
    "csv_file = r'/Users/tangjiahong/Dropbox/textmining1/Data_Clean/CSV/merge_CSV/merge_CoreTerms.csv'\n",
    "\n",
    "# 初始化分詞器\n",
    "tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "\n",
    "def preprocess_and_split_with_overlap(text, max_length=256, stride=128):\n",
    "    \"\"\"\n",
    "    使用滑動窗口切割長文本，保留一定的上下文重疊。\n",
    "    \n",
    "    Args:\n",
    "        text (str): 待切割的文本。\n",
    "        max_length (int): 每個段落的最大 token 長度。\n",
    "        stride (int): 滑動窗口的步長，重疊部分長度 = max_length - stride。\n",
    "        \n",
    "    Returns:\n",
    "        list: 切割後的文本段列表。\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, truncation=False)  # 不截斷，獲取所有 tokens\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_length, len(tokens))\n",
    "        chunk = tokens[start:end]  # 當前窗口的 tokens\n",
    "        chunks.append(chunk)\n",
    "        if end == len(tokens):  # 最後一段退出\n",
    "            break\n",
    "        start += stride  # 滑動窗口開始位置\n",
    "    # 將 tokens 解碼回文本\n",
    "    return [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n",
    "\n",
    "# 初始化列表存放處理後的數據\n",
    "processed_data = []\n",
    "\n",
    "# 讀取 CSV 文件\n",
    "df = pd.read_csv(csv_file, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# 遍歷 DataFrame 中的每行並顯示進度條\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Processing {csv_file}\"):\n",
    "    pdf_name = row['PDF']        # PDF 文件名稱\n",
    "    page_number = row['Page ']   # 注意列名中的空格（根據CSV內容調整）\n",
    "    core_terms = row['Core Terms']  # 需要處理的文本內容\n",
    "    \n",
    "    # 對內容進行處理和滑動窗口切割（修正變量名）\n",
    "    chunks = preprocess_and_split_with_overlap(core_terms)\n",
    "    \n",
    "    # 保存分塊後的數據（保存chunk而非原始內容）\n",
    "    for chunk in chunks:\n",
    "        processed_data.append({\n",
    "            'PDF': pdf_name,\n",
    "            'Page': page_number,\n",
    "            'Core Terms Chunk': chunk  # 存儲分塊後的內容\n",
    "        })\n",
    "\n",
    "# 將處理後的數據轉為 DataFrame 並保存\n",
    "processed_df = pd.DataFrame(processed_data)\n",
    "processed_df.to_csv('Processed_CoreTerms_Data.csv', index=False)\n",
    "\n",
    "print(\"處理完成，已保存為 'Processed_CoreTerms_Data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded knowledge graph with 13 entries.\n",
      "Loaded 9259 samples for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 579/579 [00:39<00:00, 14.57it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 130\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# 調用函數生成 results\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_answers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknowledge_graph\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 115\u001b[0m, in \u001b[0;36mgenerate_results\u001b[1;34m(valid_data, questions, contexts, predicted_answers, knowledge_graph)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m將驗證數據和模型預測結果封裝成包含 PDF 和頁碼的結果列表。\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    113\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pdf, page, label, question, context, prediction \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m--> 115\u001b[0m     valid_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPDF\u001b[39m\u001b[38;5;124m\"\u001b[39m], valid_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPage\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[43mvalid_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, questions, contexts, predicted_answers):\n\u001b[0;32m    117\u001b[0m     related_definitions \u001b[38;5;241m=\u001b[39m retrieve_from_knowledge_graph(prediction, knowledge_graph)\n\u001b[0;32m    118\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPDF\u001b[39m\u001b[38;5;124m\"\u001b[39m: pdf,\n\u001b[0;32m    120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPage\u001b[39m\u001b[38;5;124m\"\u001b[39m: page,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKnowledge Graph Result\u001b[39m\u001b[38;5;124m\"\u001b[39m: related_definitions\n\u001b[0;32m    126\u001b[0m     })\n",
      "File \u001b[1;32mc:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\User\\Dropbox\\textmining1\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Label'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# 加載微調後的模型\n",
    "model = BertForQuestionAnswering.from_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# 加載知識圖譜\n",
    "with open(r\"C:\\Users\\User\\Dropbox\\textmining1\\KG\\definition_moreinfo.json\", \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    knowledge_graph = json.load(f)\n",
    "\n",
    "# 建立名稱到定義的映射\n",
    "name_to_definition = {item[\"Name\"]: item[\"Definition\"] for item in knowledge_graph}\n",
    "\n",
    "print(f\"Loaded knowledge graph with {len(knowledge_graph)} entries.\")\n",
    "\n",
    "# 定義檢索知識圖譜的函數\n",
    "def retrieve_from_knowledge_graph(answer, knowledge_graph):\n",
    "    \"\"\"\n",
    "    根據模型的答案在知識圖譜中進行檢索。\n",
    "    \"\"\"\n",
    "    related_definitions = [entry for entry in knowledge_graph if entry[\"Name\"].lower() in answer.lower()]\n",
    "    return related_definitions if related_definitions else [{\"Name\": \"No Match\", \"Definition\": \"No related definition found.\"}]\n",
    "\n",
    "# 定義 RAG 測試管線\n",
    "def rag_test_pipeline(row, model, tokenizer, knowledge_graph):\n",
    "    \"\"\"\n",
    "    單條測試管線，適配切割後的資料。\n",
    "    \"\"\"\n",
    "    pdf_file = row['PDF']  # PDF 文件名稱\n",
    "    page_number = row['Page']  # 頁碼\n",
    "    label = row['Label']  # 標籤\n",
    "    paragraph = row['Text']\n",
    "    \n",
    "    # 定義測試問題\n",
    "    question = f\"What is the judge's opinion regarding entry barriers in case from {pdf_file}, page {page_number}?\"\n",
    "    \n",
    "    # 預測答案\n",
    "    inputs = tokenizer.encode_plus(question, paragraph, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    \n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "    \n",
    "    # 檢索知識圖譜\n",
    "    related_definitions = retrieve_from_knowledge_graph(answer, knowledge_graph)\n",
    "    \n",
    "    return {\n",
    "        \"PDF\": pdf_file,\n",
    "        \"Page\": page_number,\n",
    "        \"Label\": label,\n",
    "        \"Question\": question,\n",
    "        \"Predicted Answer\": answer,\n",
    "        \"Knowledge Graph Result\": related_definitions\n",
    "    }\n",
    "\n",
    "# 批量處理的預測函數\n",
    "def batch_predict(model, tokenizer, questions, contexts, batch_size=16):\n",
    "    \"\"\"\n",
    "    批量處理問題和上下文，進行預測\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for i in tqdm(range(0, len(questions), batch_size), desc=\"Predicting\"):\n",
    "        batch_questions = questions[i:i + batch_size]\n",
    "        batch_contexts = contexts[i:i + batch_size]\n",
    "        \n",
    "        inputs = tokenizer(batch_questions, batch_contexts, return_tensors=\"pt\", \n",
    "                           padding=True, truncation=True, max_length=512).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        for j in range(len(batch_questions)):\n",
    "            start = torch.argmax(outputs.start_logits[j]).item()\n",
    "            end = torch.argmax(outputs.end_logits[j]).item() + 1\n",
    "            input_ids = inputs[\"input_ids\"][j].tolist()\n",
    "            prediction = tokenizer.convert_tokens_to_string(\n",
    "                tokenizer.convert_ids_to_tokens(input_ids[start:end])\n",
    "            )\n",
    "            predictions.append(prediction)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 加載處理後的數據\n",
    "valid_data = pd.read_csv(\"Processed_CoreTerms_Data.csv\")\n",
    "\n",
    "# 提取問題和上下文\n",
    "questions = [f\"What is the judge's opinion regarding entry barriers in case from {pdf}, page {page}?\"\n",
    "             for pdf, page in zip(valid_data[\"PDF\"], valid_data[\"Page\"])]\n",
    "contexts = valid_data[\"Core Terms Chunk\"].tolist()\n",
    "\n",
    "print(f\"Loaded {len(questions)} samples for validation.\")\n",
    "\n",
    "# 執行批量預測\n",
    "predicted_answers = batch_predict(model, tokenizer, questions, contexts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成結果\n",
    "def generate_results(valid_data, questions, contexts, predicted_answers, knowledge_graph):\n",
    "    \"\"\"\n",
    "    將驗證數據和模型預測結果封裝成包含 PDF 和頁碼的結果列表。\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for pdf, page, question, context, prediction in zip(\n",
    "        valid_data[\"PDF\"], valid_data[\"Page\"], questions, contexts, predicted_answers):\n",
    "        \n",
    "        related_definitions = retrieve_from_knowledge_graph(prediction, knowledge_graph)\n",
    "        results.append({\n",
    "            \"PDF\": pdf,\n",
    "            \"Page\": page,\n",
    "            \"Question\": question,\n",
    "            \"Context\": context,\n",
    "            \"Predicted Answer\": prediction,\n",
    "            \"Knowledge Graph Result\": related_definitions\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# 調用函數生成 results\n",
    "results = generate_results(valid_data, questions, contexts, predicted_answers, knowledge_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'RAG_Test_Results.json'\n",
      "PDF: a1  \n",
      "Page: 21\n",
      "Question: What is the judge's opinion regarding entry barriers in case from a1  , page 21?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "PDF: a1  \n",
      "Page: 65\n",
      "Question: What is the judge's opinion regarding entry barriers in case from a1  , page 65?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "PDF: a1  \n",
      "Page: 73\n",
      "Question: What is the judge's opinion regarding entry barriers in case from a1  , page 73?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "PDF: a1  \n",
      "Page: 81\n",
      "Question: What is the judge's opinion regarding entry barriers in case from a1  , page 81?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n",
      "PDF: a1  \n",
      "Page: 85\n",
      "Question: What is the judge's opinion regarding entry barriers in case from a1  , page 85?\n",
      "Predicted Answer: [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'No Match', 'Definition': 'No related definition found.'}]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 將結果保存為 JSON 文件\n",
    "import json\n",
    "# 將結果保存為 JSON 文件\n",
    "with open(\"RAG_Test_Results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Results saved to 'RAG_Test_Results.json'\")\n",
    "\n",
    "# 查看部分結果\n",
    "for result in results[:5]:\n",
    "    print(\"PDF:\", result[\"PDF\"])\n",
    "    print(\"Page:\", result[\"Page\"])\n",
    "    print(\"Question:\", result[\"Question\"])\n",
    "    print(\"Predicted Answer:\", result[\"Predicted Answer\"])\n",
    "    print(\"Knowledge Graph Result:\", result[\"Knowledge Graph Result\"])\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 matched results.\n",
      "PDF: c5  \n",
      "Page: 308\n",
      "Question: What is the judge's opinion regarding entry barriers in case from c5  , page 308?\n",
      "Context: newspaper, prices, independent dealer, monopoly power, plaintiffs ', home delivery, monopolize, vertical integration, circulation, coercion, monopoly, conspiracy, antitrust, dealers, sales, sherman act, customers, delivery, retail, defendants ', wholesalers, adherence, attempt to monopolize, relevant market, specific intent, probability, summary judgment, publisher ...\n",
      "Predicted Answer: [SEP] newspaper , prices , independent dealer , monopoly power , plaintiffs ' , home delivery , monopolize , vertical integration , circulation , coercion , monopoly , conspiracy , antitrust , dealers , sales , sherman act , customers , delivery , retail , defendants ' , wholesalers , adherence , attempt to monopolize , relevant market , specific intent , probability , summary judgment , publisher [SEP]\n",
      "Knowledge Graph Result: [{'Name': 'Vertical Integration', 'Definition': \"Control over supply chain that limits competitors' access to resources or distribution.\"}]\n",
      "==================================================\n",
      "Found 1 matched results.\n"
     ]
    }
   ],
   "source": [
    "# 載入保存的結果\n",
    "import json\n",
    "\n",
    "# 加載結果\n",
    "with open(\"RAG_Test_Results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# 過濾有匹配的結果\n",
    "matched_results = [\n",
    "    result for result in results if result[\"Knowledge Graph Result\"][0][\"Name\"] != \"No Match\"\n",
    "]\n",
    "\n",
    "# 列出有匹配的結果\n",
    "print(f\"Found {len(matched_results)} matched results.\")\n",
    "for result in matched_results:\n",
    "    print(\"PDF:\", result[\"PDF\"])\n",
    "    print(\"Page:\", result[\"Page\"])\n",
    "    print(\"Question:\", result[\"Question\"])\n",
    "    print(\"Context:\", result[\"Context\"][:500], \"...\")\n",
    "    print(\"Predicted Answer:\", result[\"Predicted Answer\"])\n",
    "    print(\"Knowledge Graph Result:\", result[\"Knowledge Graph Result\"])\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# 列出匹配的結果數量\n",
    "print(f\"Found {len(matched_results)} matched results.\")\n",
    "\n",
    "# 保存匹配結果到新文件\n",
    "output_file = \"Matched_CoreTerms_DATA.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(matched_results, f, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
