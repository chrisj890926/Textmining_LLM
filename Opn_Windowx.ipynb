{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 抓到Footnotes <br>\n",
        "+ 目前會抓到帶有 \" * \" 的FN \n",
        "\n",
        "+ Counsel: 96個 **p.484出現在FN裡面**\n",
        "+ Judges: 94個 (P471沒有)\n",
        "+ Opinion by: 95個\n",
        "\n",
        "+ 有些會抓到Opinion內容"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kndgiv96am3L",
        "outputId": "cc20dd8f-97e8-4230-af97-02a8085a94d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "完成：C:\\Users\\User\\Dropbox\\textmining1\\opinion\\opinion_100_test.csv\n",
            "Counsel 數量: 95\n",
            "Judges 數量: 92\n",
            "Opinion by 數量: 95\n"
          ]
        }
      ],
      "source": [
        "# Explain\n",
        "import fitz\n",
        "import re\n",
        "import csv\n",
        "\n",
        "pdf_file_path = r'C:\\Users\\User\\Dropbox\\textmining1\\PDF\\Files (100).PDF'\n",
        "csv_file_path = r'C:\\Users\\User\\Dropbox\\textmining1\\opinion\\opinion_100_test.csv'\n",
        "\n",
        "\n",
        "pattern = r'(Counsel:.*?|Judges:.*?(?=Opinion by:)|Opinion by:.*?)((?=Counsel:)|(?=Judges:)|(?=Opinion by:)|(?=Opinion)|$)'\n",
        "\n",
        "# \" (Counsel:.*?|Judges:.*?(?=Opinion by:)|Opinion by:.*?) \"  :\n",
        "# capture content of \" Counsel: \", \" Judges: \" 到 \"Opinion by:\" or \"Opinion\"\n",
        "\n",
        "# \" ((?=Counsel:)|(?=Judges:)|(?=Opinion by:)|(?=Opinion)|$) \" :\n",
        "# capture the type of the match (\"Counsel:\", \"Judges:\", \"Opinion by:\", \"Opinion\", or the end of the string).\n",
        "\n",
        "\n",
        "def find_page_num(text, match_start, page_delimiters):\n",
        "\n",
        "#找出匹配文本再PDF的頁碼. Text(整個PDF文本內容) . match_start(當前匹配文本起始位置)\n",
        "# page_delimiters(包含每頁結尾在text中的列表) 將每頁文本加入all_text並記錄長度, 每個元素代表文本結束的位置\n",
        "    for i, delimiter in enumerate(page_delimiters): # i為當前頁碼索引 從0開始\n",
        "        if match_start < delimiter: #檢查match_start是否小於delimiter  , 小於代表匹配文本位於這一頁或之前的頁面\n",
        "            return i #頁碼通常從1開始, 所以後續都會對這個索引 + 1\n",
        "    return len(page_delimiters) # 表示匹配文本位於文檔最後一頁\n",
        "\n",
        "# 加入計數器\n",
        "counsel_count = 0\n",
        "judges_count = 0\n",
        "opinion_by_count = 0\n",
        "\n",
        "with fitz.open(pdf_file_path) as doc:\n",
        "    all_text = \"\" #儲存整個文本內容\n",
        "    page_delimiters = [] #儲存每頁文本結束的列表\n",
        "\n",
        "    for page_num in range(len(doc)): #閱讀文本每一頁 . len(doc):總頁數\n",
        "        page = doc.load_page(page_num) #載入當前頁\n",
        "        page_text = page.get_text(\"text\")  # 獲取當前頁的文本\n",
        "        all_text += page.get_text(\"text\") + \"\\f\" #從當前頁提取文本並添加到all_text中 # 使用分頁符號 \"\\f\" 作為分頁標記\n",
        "        page_delimiters.append(len(all_text))  # 在page_delimiters列表添加累積文本長度,也就是當前頁結束位置, 紀錄每頁結束的位置.\n",
        "        # 在每段文本前加上頁碼標記\n",
        "        formatted_page_text = f'頁碼 {page_num + 1}:\\n{page_text}'\n",
        "    # 移除包含 \"Page \\d+ of \\d+\" 的字樣\n",
        "    all_text = re.sub(r'Page \\d+ of \\d+', '', all_text)\n",
        "\n",
        "# re.sub 查找和替換 符合正則表達式的字串, ''表示將匹配內容刪除\n",
        "# \\d+ 數字 . Page和of 為單詞 . 表 刪除 ex Page 23 of 456 將其中all_text中刪除\n",
        "\n",
        "    # 移除其他不重要的字樣\n",
        "    all_text = re.sub(r'\\d+ Cal\\..*?\\*{1,4}\\d+; (?:\\d{4} )?Cal\\..*?\\*{1,4}\\d+', '', all_text)\n",
        "\n",
        "# Cal\\. = Cal. \" .*? \" 非貪婪匹配任所有字符直到遇到後面的模式 \"\\*{1,4}\" 匹配1~4個'*' ;匹配分號\n",
        "# (?:\\d{4} ) 匹配四位數字後跟一個空格 , Cal\\..*?\\*{1,4}\\d+ 同上 匹配Cal. 後面的文字,星號,數字\n",
        "\n",
        "    with open(csv_file_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
        "      #確保文件使用utf-8編碼, newline=''為了避免在不同操作系統寫入時產生新行問題\n",
        "        csv_writer = csv.writer(csv_file) # 創建CSV寫入器,將數據輸入\n",
        "        csv_writer.writerow(['頁碼', '內容類型', '內容']) #將標題寫入\n",
        "\n",
        "        matches = re.finditer(pattern, all_text, re.DOTALL) # find all occourance of pattern in 'text'\n",
        "        for match in matches: #loop initiate matches from re.finditer\n",
        "            match_content = match.group(1).strip() # 提取第一個匹配組內容,去除首尾空格\n",
        "            match_type = match.group(2).strip()  # 提取第二個匹配組內容,去除首尾空格\n",
        "            match_page = find_page_num(all_text, match.start(), page_delimiters)\n",
        "            # 調用find_page_num函數確定匹配內容所在頁碼,將這些資訊寫入CSV\n",
        "\n",
        "            if 'Counsel:' in match_content:\n",
        "                csv_writer.writerow([match_page + 1, 'Counsel', match_content])\n",
        "                counsel_count += 1\n",
        "            elif 'Judges:' in match_content:\n",
        "                csv_writer.writerow([match_page + 1, 'Judges', match_content])\n",
        "                judges_count += 1\n",
        "            elif 'Opinion by:' in match_content:\n",
        "                csv_writer.writerow([match_page + 1, 'Opinion by', match_content])\n",
        "                opinion_by_count += 1\n",
        "\n",
        "print(f\"完成：{csv_file_path}\")\n",
        "print(f\"Counsel 數量: {counsel_count}\")\n",
        "print(f\"Judges 數量: {judges_count}\")\n",
        "print(f\"Opinion by 數量: {opinion_by_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 最終版Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "完成：C:\\Users\\User\\Dropbox\\textmining1\\opinion\\opinion_100_test.csv\n",
            "Counsel 數量: 95\n",
            "Judges 數量: 94\n",
            "Opinion by 數量: 95\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import csv\n",
        "\n",
        "pdf_file_path = r\"C:\\Users\\User\\Dropbox\\textmining1\\PDF\\Files (100).PDF\"\n",
        "csv_file_path = r\"C:\\Users\\User\\Dropbox\\textmining1\\opinion\\opinion_100_test.csv\"\n",
        "\n",
        "# 更新後的正則表達式，確保遇到下一個標記或\"Opinion\"時停止\n",
        "pattern = r'(Counsel:.*?)(?=Counsel:|Judges:|Opinion by:|Opinion|$)|(Judges:.*?)(?=Counsel:|Judges:|Opinion by:|Opinion|$)|(Opinion by:.*?)(?=Counsel:|Judges:|Opinion by:|Opinion|$)'\n",
        "\n",
        "def find_page_num(text, match_start, page_delimiters):\n",
        "    for i, delimiter in enumerate(page_delimiters):\n",
        "        if match_start < delimiter:\n",
        "            return i\n",
        "    return len(page_delimiters)\n",
        "\n",
        "counsel_count = 0\n",
        "judges_count = 0\n",
        "opinion_by_count = 0\n",
        "\n",
        "with fitz.open(pdf_file_path) as doc:\n",
        "    all_text = \"\"\n",
        "    page_delimiters = []\n",
        "\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)\n",
        "        page_text = page.get_text(\"text\")\n",
        "        all_text += page_text + \"\\f\"\n",
        "        page_delimiters.append(len(all_text))\n",
        "\n",
        "    # 移除頁碼和案例引用\n",
        "    all_text = re.sub(r'Page \\d+ of \\d+', '', all_text)\n",
        "    all_text = re.sub(r'\\d+ Cal\\..*?\\*{1,4}\\d+; (?:\\d{4} )?Cal\\..*?\\*{1,4}\\d+', '', all_text)\n",
        "\n",
        "    with open(csv_file_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "        csv_writer.writerow(['頁碼', '內容類型', '內容'])\n",
        "\n",
        "        matches = re.finditer(pattern, all_text, re.DOTALL)\n",
        "        for match in matches:\n",
        "            for group_num in range(1, 4):\n",
        "                if match.group(group_num):\n",
        "                    match_content = match.group(group_num).strip()\n",
        "                    match_page = find_page_num(all_text, match.start(group_num), page_delimiters)\n",
        "                    content_type = 'Counsel' if group_num == 1 else 'Judges' if group_num == 2 else 'Opinion by'\n",
        "                    csv_writer.writerow([match_page + 1, content_type, match_content])\n",
        "                    if content_type == 'Counsel':\n",
        "                        counsel_count += 1\n",
        "                    elif content_type == 'Judges':\n",
        "                        judges_count += 1\n",
        "                    elif content_type == 'Opinion by':\n",
        "                        opinion_by_count += 1\n",
        "                    break\n",
        "\n",
        "print(f\"完成：{csv_file_path}\")\n",
        "print(f\"Counsel 數量: {counsel_count}\")\n",
        "print(f\"Judges 數量: {judges_count}\")\n",
        "print(f\"Opinion by 數量: {opinion_by_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 遍歷資料夾及子資料夾PDF\n",
        "+ 匯入一個CSV 但 有PDF檔案名稱"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "完成：C:\\Users\\User\\Dropbox\\textmining1\\opinion\\opinion_summary.csv\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# 指定需要遍歷的根資料夾路徑，這裡將其設置為Data 1資料夾的路徑\n",
        "root_folder_path = r\"C:\\Users\\User\\Dropbox\\textmining1\\Data 1\"\n",
        "# 指定CSV文件路徑\n",
        "csv_file_path = r\"C:\\Users\\User\\Dropbox\\textmining1\\opinion\\opinion_summary.csv\"\n",
        "\n",
        "# 正則表達式\n",
        "pattern = r'(Counsel:.*?)(?=Counsel:|Judges:|Opinion by:|Opinion|$)|(Judges:.*?)(?=Counsel:|Judges:|Opinion by:|Opinion|$)|(Opinion by:.*?)(?=Counsel:|Judges:|Opinion by:|Opinion|$)'\n",
        "\n",
        "def find_page_num(text, match_start, page_delimiters):\n",
        "    for i, delimiter in enumerate(page_delimiters):\n",
        "        if match_start < delimiter:\n",
        "            return i\n",
        "    return len(page_delimiters)\n",
        "\n",
        "def process_pdf_files(folder_path, csv_writer):\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(\".pdf\"):  # 支援小寫或大寫的PDF檔案副檔名\n",
        "                pdf_file_path = os.path.join(root, file)\n",
        "                process_single_pdf(pdf_file_path, csv_writer)\n",
        "\n",
        "def process_single_pdf(pdf_file_path, csv_writer):\n",
        "    with fitz.open(pdf_file_path) as doc:\n",
        "        all_text = \"\"\n",
        "        page_delimiters = []\n",
        "\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            page_text = page.get_text(\"text\")\n",
        "            all_text += page_text + \"\\f\"\n",
        "            page_delimiters.append(len(all_text))\n",
        "\n",
        "        all_text = re.sub(r'Page \\d+ of \\d+', '', all_text)\n",
        "        all_text = re.sub(r'\\d+ Cal\\..*?\\*{1,4}\\d+; (?:\\d{4} )?Cal\\..*?\\*{1,4}\\d+', '', all_text)\n",
        "\n",
        "        matches = re.finditer(pattern, all_text, re.DOTALL)\n",
        "        for match in matches:\n",
        "            for group_num in range(1, 4):\n",
        "                if match.group(group_num):\n",
        "                    match_content = match.group(group_num).strip()\n",
        "                    match_page = find_page_num(all_text, match.start(group_num), page_delimiters)\n",
        "                    content_type = 'Counsel' if group_num == 1 else 'Judges' if group_num == 2 else 'Opinion by'\n",
        "                    csv_writer.writerow([os.path.basename(pdf_file_path), match_page + 1, content_type, match_content])\n",
        "\n",
        "with open(csv_file_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    csv_writer.writerow(['文件名稱', '頁碼', '內容類型', '內容'])\n",
        "    process_pdf_files(root_folder_path, csv_writer)\n",
        "\n",
        "print(f\"完成：{csv_file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 一個PDF一個CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import csv\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 更新後的正則表達式，確保遇到下一個標記或\"Opinion\"時停止\n",
        "pattern = r'(Counsel:.*?)(?=Counsel:|Judges:|Opinion by:|Opinion|$)|(Judges:.*?)(?=Counsel:|Judges:|Opinion by:|Opinion|$)|(Opinion by:.*?)(?=Counsel:|Judges:|Opinion by:|Opinion|$)'\n",
        "\n",
        "def find_page_num(text, match_start, page_delimiters):\n",
        "    for i, delimiter in enumerate(page_delimiters):\n",
        "        if match_start < delimiter:\n",
        "            return i\n",
        "    return len(page_delimiters)\n",
        "\n",
        "def process_single_pdf(pdf_file_path, csv_file_path):\n",
        "    with fitz.open(pdf_file_path) as doc:\n",
        "        all_text = \"\"\n",
        "        page_delimiters = []\n",
        "\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            page_text = page.get_text(\"text\")\n",
        "            all_text += page_text + \"\\f\"\n",
        "            page_delimiters.append(len(all_text))\n",
        "\n",
        "        # 移除頁碼和案例引用\n",
        "        all_text = re.sub(r'Page \\d+ of \\d+', '', all_text)\n",
        "        all_text = re.sub(r'\\d+ Cal\\..*?\\*{1,4}\\d+; (?:\\d{4} )?Cal\\..*?\\*{1,4}\\d+', '', all_text)\n",
        "\n",
        "        with open(csv_file_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
        "            csv_writer = csv.writer(csv_file)\n",
        "            csv_writer.writerow(['頁碼', '內容類型', '內容'])\n",
        "\n",
        "            matches = re.finditer(pattern, all_text, re.DOTALL)\n",
        "            for match in matches:\n",
        "                for group_num in range(1, 4):\n",
        "                    if match.group(group_num):\n",
        "                        match_content = match.group(group_num).strip()\n",
        "                        match_page = find_page_num(all_text, match.start(group_num), page_delimiters)\n",
        "                        content_type = 'Counsel' if group_num == 1 else 'Judges' if group_num == 2 else 'Opinion by'\n",
        "                        csv_writer.writerow([match_page + 1, content_type, match_content])\n",
        "                        break\n",
        "\n",
        "def process_pdf_files(folder_path, output_folder_path):\n",
        "    # 确保输出文件夹存在\n",
        "    Path(output_folder_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # 初始化子文件夹标签索引\n",
        "    label_index = 0  # 从0开始，对应于'a'\n",
        "\n",
        "    # 定义子文件夹的标识符列表\n",
        "    subfolder_labels = list('abcdefghijklmnopqrstuvwxyz')\n",
        "    \n",
        "    # 明确指定顶层文件夹的处理顺序，如果默认排序不适用\n",
        "    top_folders = [r'/Users/tangjiahong/Dropbox/textmining1/Data 1', r'/Users/tangjiahong/Dropbox/textmining1/Data 2', r'/Users/tangjiahong/Dropbox/textmining1/Data 3']  # 根据需要调整\n",
        "\n",
        "    for top_folder in top_folders:\n",
        "        top_folder_path = os.path.join(folder_path, top_folder)\n",
        "\n",
        "        # 获取子文件夹并按预期的顺序排序\n",
        "        subdirs = sorted([d for d in os.listdir(top_folder_path) if os.path.isdir(os.path.join(top_folder_path, d))])\n",
        "        \n",
        "        for subdir in subdirs:\n",
        "            subdir_path = os.path.join(top_folder_path, subdir)\n",
        "\n",
        "            # 获取PDF文件并按字母顺序排序\n",
        "            pdf_files = sorted([f for f in os.listdir(subdir_path) if f.lower().endswith('.pdf')])\n",
        "            \n",
        "            for file_index, file in enumerate(pdf_files, start=1):\n",
        "                pdf_file_path = os.path.join(subdir_path, file)\n",
        "\n",
        "                # 使用子文件夹标签和文件编号生成CSV文件名\n",
        "                letter = subfolder_labels[label_index % len(subfolder_labels)]\n",
        "                pdf_label = f\"{letter}{file_index}_opinion.csv\"\n",
        "                csv_file_path = os.path.join(output_folder_path, pdf_label)\n",
        "                process_single_pdf(pdf_file_path, csv_file_path)\n",
        "                \n",
        "            # 处理完一个子文件夹后，更新标签索引\n",
        "            label_index += 1\n",
        "\n",
        "root_folder_path = \"/Users/tangjiahong/Dropbox/textmining1\"\n",
        "output_folder_path = \"/Users/tangjiahong/Dropbox/textmining1/Opinion\"\n",
        "\n",
        "process_pdf_files(root_folder_path, output_folder_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 生成一個整合CSV , 照英文字母排序"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import os\n",
        "from pathlib import Path\n",
        "import re\n",
        "import sys\n",
        "\n",
        "def natural_sort_key(s):\n",
        "    \"\"\"\n",
        "    提取用于自然排序的键（包括数字排序）。\n",
        "    \"\"\"\n",
        "    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n",
        "\n",
        "def merge_csv_files(input_folder, output_csv_path):\n",
        "    # 确保输出文件夹存在\n",
        "    Path(output_csv_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # 尝试增加字段大小限制\n",
        "    csv.field_size_limit(sys.maxsize)\n",
        "\n",
        "    with open(output_csv_path, 'w', newline='', encoding='utf-8') as output_file:\n",
        "        output_writer = csv.writer(output_file)\n",
        "        headers_written = False\n",
        "\n",
        "        # 使用自定义的自然排序键来排序文件名\n",
        "        for input_file in sorted(os.listdir(input_folder), key=natural_sort_key):\n",
        "            if input_file.endswith('.csv'):\n",
        "                pdf_label = input_file.split('_')[0]  # 假设文件名格式为 'd1_FN.csv'\n",
        "                with open(Path(input_folder) / input_file, newline='', encoding='utf-8') as csv_file:\n",
        "                    csv_reader = csv.reader(csv_file)\n",
        "                    headers = next(csv_reader)\n",
        "\n",
        "                    if not headers_written:\n",
        "                        output_writer.writerow(['PDF'] + headers)\n",
        "                        headers_written = True\n",
        "\n",
        "                    for row in csv_reader:\n",
        "                        output_writer.writerow([pdf_label] + row)\n",
        "\n",
        "input_folder = r'/Users/tangjiahong/Dropbox/textmining1/Opinion'\n",
        "output_csv_path = r'/Users/tangjiahong/Dropbox/textmining1/merge_CSV/merge_Opinion.csv'\n",
        "\n",
        "merge_csv_files(input_folder, output_csv_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "+ 0722(6).pdf p.1090 Counsel ~ p.1203 多達 100多頁"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
